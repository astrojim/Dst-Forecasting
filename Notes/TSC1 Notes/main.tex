\documentclass[a4paper,11pt]{article}

\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage[margin=0.5in]{geometry}
\usepackage{caption}
\usepackage{subcaption}


\begin{document}

{\Huge TSC1 Notes}

\hfill\rule{150mm}{.1pt}

\hfill{\small \today}

\section{Simple Linear Example}
Consider a simple driver-response system:
\begin{eqnarray*}
x_t &=& \sin(t)\\
y_t &=& x_{t-1} + \eta_t\\
&=& \sin(t-1) + \eta_t\\
&=& \sin(t)\cos(1)-\cos(t)\sin(1)+ \eta_t
\end{eqnarray*}
with $\eta_t\sim \mathcal{N}(0,1)$.  Define
$$
\delta x_t \equiv \frac{dx}{dt} \approx \frac{\Delta x}{\Delta t} = x_t-x_{t-1}
$$
and
$$
\delta y_t \equiv \frac{dy}{dt} \approx \frac{\Delta y}{\Delta t} = y_t-y_{t-1}\;\;.
$$
It follows that
\begin{eqnarray*}
\delta x_t &=& \sin(t)-\sin(t-1)\\
&=& \sin(t)-\left(\sin(t)\cos(1)-\cos(t)\sin(1)\right)\\
&=& \sin(t)\left(1-\cos(1)\right)+\sin(1)\cos(t)\\
&\equiv& \kappa_1\sin(t)+\kappa_2\cos(t)
\end{eqnarray*}
with $\kappa_1 = 1-\cos(1)$ and $\kappa_2 = \sin(1)$, and
\begin{eqnarray*}
\delta y_t &=& x_{t-1} + \eta_t - x_{t-2} - \eta_{t-1}\\
&=& \sin(t-1) - \sin(t-2) + \eta_t - \eta_{t-1}\\
&=& \left(\sin(t)\cos(1)-\cos(t)\sin(1)\right) - \left(\sin(t)\cos(2)-\cos(t)\sin(2)\right)+\eta^\prime_t \\
&=& \sin(t)\left(\cos(1)-\cos(2)\right)+\cos(t)\left(\sin(2)-\sin(1)\right)+\eta^\prime_t\\
&\equiv& k_1\sin(t)+k_2\cos(t)+\eta^\prime_t
\end{eqnarray*}
with $\eta^\prime_t = \eta_t - \eta_{t-1} \sim \mathcal{N}(0,2)$\footnote{The difference of two normal distributions with means $\mu_1$ and $\mu_2$ and variances $\sigma^2_1$ and $\sigma^2_2$ is another normal distribution with mean $\mu_1-\mu_2$ and variance $\sigma^2_1+\sigma^2_2$.}, $k_1 = \left(\cos(1)-\cos(2)\right)$ and $k_2 = \left(\sin(2)-\sin(1)\right)$.

The main idea of Local Impulse Response (LIR) causality inference is to use a subsetting procedure on some (or all) of the four time series $x_t$, $y_t$, $\delta x_t$, and $\delta y_t$ to determine the causality of the system.

Consider a few extreme points in the driver cycle, e.g.\ $t=n\pi$ with $n=0,1,2,3,4,\ldots$.  The driver values are
$$
x_{n\pi} = \sin(n\pi) = 0\;\;,
$$
the response values are
$$
y_{n\pi} = \sin(n\pi)\cos(1)-\cos(n\pi)\sin(1)+ \eta_{n\pi} = \eta_{n\pi} + \left(-1\right)^n\sin(1)\;\;,
$$
the local change in the driver values are
$$
\delta x_{n\pi} = \kappa_1\sin(n\pi)+\kappa_2\cos(n\pi) = \left(-1\right)^n \kappa_2\;\;,
$$
and the local change in the response values are
$$
\delta y_{n\pi} = k_1\sin(n\pi)+k_2\cos(n\pi)+\eta^\prime_{n\pi} = \eta^\prime_{n\pi} + \left(-1\right)^n k_2\;\;.
$$

Consider $t=n\pi/2$ with $n=1,2,3,4,\ldots$.  The driver values are
$$
x_{n\frac{\pi}{2}} = \sin\left(n\frac{\pi}{2}\right) = (-1)^n\;\;,
$$
the response values are
$$
y_{n\frac{\pi}{2}} = \sin\left(n\frac{\pi}{2}\right)\cos(1)-\cos\left(n\frac{\pi}{2}\right)\sin(1)+ \eta_{n\frac{\pi}{2}} = \eta_{n\frac{\pi}{2}} + (-1)^n\cos(1)\;\;,
$$
the local change in thd driver values are
$$
\delta x_{n\frac{\pi}{2}} = \kappa_1\sin\left(n\frac{\pi}{2}\right)+\kappa_2\cos\left(n\frac{\pi}{2}\right) = \left(-1\right)^n \kappa_1\;\;,
$$
and the local change in the response values are
$$
\delta y_{n\frac{\pi}{2}} = k_1\sin\left(n\frac{\pi}{2}\right)+k_2\cos\left(n\frac{\pi}{2}\right)+\eta^\prime_{n\frac{\pi}{2}} = \eta^\prime_{n\frac{\pi}{2}} + \left(-1\right)^n k_1\;\;.
$$

\section{LIR Approach to Probabilistic Causality}
Probabilistic causality is centered on the definition that a cause $C$ is said to {\em cause} (or {\em drive}) an effect $E$ if
$$
P\left(E|C\right) > P\left(E|\bar{C}\right)\;\;,
$$
i.e.\ $C$ causes $E$ if the probability of $E$ given $C$ is higher than the probability of $E$ given not $C$.  The LIR causal inference method involves using e.g.\ $\{x_t\}$, $\{y_t\}$, $\{\delta x_t\}$, and $\{\delta y_t\}$ to determine the direction of causal influence in a system of two times series $\{x_t\}$ and $\{y_t\}$.  It follows that applying LIR causal inference to probabilistic causality involves evaluating the above inequality given e.g.\ $C = \{x_t\}$, $\{y_t\}$, $\{\delta x_t\}$, or $\{\delta y_t\}$ and $E = \{x_t\}$, $\{y_t\}$, $\{\delta x_t\}$, or $\{\delta y_t\}$ given $E\neq C$ and for different temporal offsets.

The conditional probabilities are estimated using histograms of the time series data as follows:
$$
P\left(E|C\right) \approx \frac{1}{L} H\left(E|C\right) = \frac{H\left(E\cap C\right)}{H(C)}
$$
where $H(A)$ is an $m$-binned histogram of $A$, $L$ is the library length of the $E$ and $C$ time series (which are assumed to be the same length).  Similarly,
$$
P\left(E|\bar{C}\right) \approx \frac{1}{L} H\left(E|\bar{C}\right) = \frac{H\left(E\cap \bar{C}\right)}{H(\bar{C})}\;\;.
$$

Define the {\em causal penchant} 
$$
\rho_{EC} = P\left(E|C\right) - P\left(E|\bar{C}\right) \approx \frac{H\left(E\cap C\right)}{H(C)}-\frac{H\left(E\cap \bar{C}\right)}{H(\bar{C})}\;\;.
$$
If $C$ causes $E$, then $\rho_{EC} > 0$.  Otherwise, i.e.\ $\rho_{EC} \le 0$, the causal influence of the system is {\em undefined}(?).  Causal influence between a pair of time series is accomplished by comparing penchants.

Consider two short time series $\mathbf{x}=\{x_t\} = \{0,1,2,1,0\}$ and $\mathbf{y}=y_t=x_{t-1}$ (i.e.\ $\{y_t\} = \{0,0,1,2,1\}$.  The time steps are indexed by $t=0,1,2,3,4$.  The individual joint probabilities can be found using the frequencies of occurrence as
\begin{eqnarray}
P(y_t=0 \cap x_t = 0) &=& \frac{1}{5}\\
P(y_t=1 \cap x_t = 0) &=& \frac{1}{5}\\
P(y_t=2 \cap x_t = 0) &=& 0\\
P(y_t=0 \cap x_t = 1) &=& \frac{1}{5}\\
P(y_t=1 \cap x_t = 1) &=& 0\\
P(y_t=2 \cap x_t = 1) &=& \frac{1}{5}\\
P(y_t=0 \cap x_t = 2) &=& 0\\
P(y_t=1 \cap x_t = 2) &=& \frac{1}{5}\\
P(y_t=2 \cap x_t = 2) &=& 0
\end{eqnarray}
The individual probabilities are found similarly:
\begin{eqnarray}
P(x_t = 0) &=& \frac{2}{5}\\
P(x_t = 1) &=& \frac{2}{5}\\
P(x_t = 2) &=& \frac{1}{5}\\
P(y_t = 0) &=& \frac{2}{5}\\
P(y_t = 1) &=& \frac{2}{5}\\
P(y_t = 2) &=& \frac{1}{5}
\end{eqnarray}
The joint probabilities are symmetric (i.e.\ $P(x_t\cap y_t) = P(y_t\cap x_t)$).  Thus, the two above sets of probabilities are sufficient to find all the conditionals.  
\begin{center}
\begin{tabular}{c|c|c}
$P(y_t=0 | x_t = 0) = \frac{P(y_t=0 \cap x_t = 0)}{P(x_t = 0)} = \frac{1}{2}$ &
$P(y_t=0 | x_t = 1) = \frac{P(y_t=0 \cap x_t = 1)}{P(x_t = 1)} = \frac{1}{2}$ &
$P(y_t=0 | x_t = 2) = \frac{P(y_t=0 \cap x_t = 2)}{P(x_t = 2)} = 0$ \\

$P(y_t=1 | x_t = 0) = \frac{P(y_t=1 \cap x_t = 0)}{P(x_t = 0)} = \frac{1}{2}$ &
$P(y_t=1 | x_t = 1) = \frac{P(y_t=1 \cap x_t = 1)}{P(x_t = 1)} = 0$ &
$P(y_t=1 | x_t = 2) = \frac{P(y_t=1 \cap x_t = 2)}{P(x_t = 2)} = 1$ \\

$P(y_t=2 | x_t = 0) = \frac{P(y_t=2 \cap x_t = 0)}{P(x_t = 0)} = 0$ &
$P(y_t=2 | x_t = 1) = \frac{P(y_t=2 \cap x_t = 1)}{P(x_t = 1)} = \frac{1}{2}$ &
$P(y_t=2 | x_t = 2) = \frac{P(y_t=2 \cap x_t = 2)}{P(x_t = 2)} = 0$ \\
\hline \\
$P(x_t=0 | y_t = 0) = \frac{P(y_t=0 \cap x_t = 0)}{P(y_t = 0)} = \frac{1}{2}$ &
$P(x_t=0 | y_t = 1) = \frac{P(y_t=1 \cap x_t = 0)}{P(y_t = 1)} = \frac{1}{2}$ &
$P(x_t=0 | y_t = 2) = \frac{P(y_t=2 \cap x_t = 0)}{P(y_t = 2)} = 0$ \\

$P(x_t=1 | y_t = 0) = \frac{P(y_t=0 \cap x_t = 1)}{P(y_t = 0)} = \frac{1}{2}$ &
$P(x_t=1 | y_t = 1) = \frac{P(y_t=1 \cap x_t = 1)}{P(y_t = 1)} = 0$ &
$P(x_t=1 | y_t = 2) = \frac{P(y_t=2 \cap x_t = 1)}{P(y_t = 2)} = 1$ \\

$P(x_t=2 | y_t = 0) = \frac{P(y_t=0 \cap x_t = 2)}{P(y_t = 0)} = 0$ &
$P(x_t=2 | y_t = 1) = \frac{P(y_t=1 \cap x_t = 2)}{P(y_t = 1)} = \frac{1}{2}$ &
$P(x_t=2 | y_t = 2) = \frac{P(y_t=2 \cap x_t = 2)}{P(y_t = 2)} = 0$ \\
\end{tabular}
\end{center}

The next step is to investigate the difference series: $\mathbf{\delta x} = \delta x_t = x_t - x_{t-1} = \{0,1,1,-1,-1\}$ and $\mathbf{\delta y} =\delta y_t = y_t - y_{t-1} = \{0,0,1,1,-1\}$.  There are now four time series, $\mathbf{x}$, $\mathbf{y}$, $\mathbf{\delta x}$, and $\mathbf{\delta y}$, with six permutations to investigate, $(\mathbf{x},\mathbf{y})$, $(\mathbf{x},\mathbf{\delta y})$, $(\mathbf{\delta x},\mathbf{y})$, $(\mathbf{\delta x},\mathbf{\delta y})$, $(\mathbf{x},\mathbf{\delta x})$, and $(\mathbf{y},\mathbf{\delta y})$.  It has already been determined that no conclusions can be drawn about $(\mathbf{x},\mathbf{y})$.  The following table is for reference in the counting exercises below:
\begin{center}
\begin{tabular}{c|c|c|c|c}
t & $\mathbf{x}$ & $\mathbf{y}$ & $\mathbf{\delta x}$ & $\mathbf{\delta y}$ \\
\hline 
0 & 0 & 0 & 0 & 0\\
1 & 1 & 0 & 1 & 0\\
2 & 2 & 1 & 1 & 1\\
3 & 1 & 2 & -1 & 1\\
4 & 0 & 1 & -1 & -1
\end{tabular}
\end{center}

Consider the three permutations: $(\mathbf{x},\mathbf{\delta x})$, $(\mathbf{\delta x},\mathbf{y})$, and $(\mathbf{\delta x},\mathbf{\delta y})$.  The joint probabilities are as follows:
\begin{center}
\begin{tabular}{c|c|c}
$P(x_t=0 \cap \delta x_t = 0) = \frac{1}{5}$&$P(y_t=0 \cap \delta x_t = 0) = \frac{1}{5}$&$P(\delta y_t=0 \cap \delta x_t = 0) = \frac{1}{5}$\\
$P(x_t=1 \cap \delta x_t = 0) = 0$&$P(y_t=1 \cap \delta x_t = 0) = 0$&$P(\delta y_t=1 \cap \delta x_t = 0) = 0$\\
$P(x_t=2 \cap \delta x_t = 0) = 0$&$P(y_t=2 \cap \delta x_t = 0) = 0$&$P(\delta y_t=-1 \cap \delta x_t = 0) = 0$\\
$P(x_t=0 \cap \delta x_t = 1) = 0$&$P(y_t=0 \cap \delta x_t = 1) = \frac{1}{5}$&$P(\delta y_t=0 \cap \delta x_t = 1) = \frac{1}{5}$\\
$P(x_t=1 \cap \delta x_t = 1) = \frac{1}{5}$&$P(y_t=1 \cap \delta x_t = 1) = \frac{1}{5}$&$P(\delta y_t=1 \cap \delta x_t = 1) = \frac{1}{5}$\\
$P(x_t=2 \cap \delta x_t = 1) = \frac{1}{5}$&$P(y_t=2 \cap \delta x_t = 1) = 0$&$P(\delta y_t=-1 \cap \delta x_t = 1) =0 $\\
$P(x_t=0 \cap \delta x_t = -1) = \frac{1}{5}$&$P(y_t=0 \cap \delta x_t = -1) = 0$&$P(\delta y_t=0 \cap \delta x_t = -1) = 0$\\
$P(x_t=1 \cap \delta x_t = -1) = \frac{1}{5}$&$P(y_t=1 \cap \delta x_t = -1) = \frac{1}{5}$&$P(\delta y_t=1 \cap \delta x_t = -1) = \frac{1}{5}$\\
$P(x_t=2 \cap \delta x_t = -1) = 0$&$P(y_t=2 \cap \delta x_t = -1) = \frac{1}{5}$&$P(\delta y_t=-1 \cap \delta x_t = -1) = \frac{1}{5}$\\
\end{tabular}
\end{center}
The last two permutations, $(\mathbf{x},\mathbf{\delta y})$ and $(\mathbf{y},\mathbf{\delta y})$, lead to the following table:
\begin{center}
\begin{tabular}{c|c}
$P(x_t=0 \cap \delta y_t = 0) = \frac{1}{5}$&$P(y_t=0 \cap \delta y_t = 0) = \frac{2}{5}$\\
$P(x_t=1 \cap \delta y_t = 0) = \frac{1}{5}$&$P(y_t=1 \cap \delta y_t = 0) = 0$\\
$P(x_t=2 \cap \delta y_t = 0) = 0$&$P(y_t=2 \cap \delta y_t = 0) = 0$\\
$P(x_t=0 \cap \delta y_t = 1) = 0$&$P(y_t=0 \cap \delta y_t = 1) = 0$\\
$P(x_t=1 \cap \delta y_t = 1) = \frac{1}{5}$&$P(y_t=1 \cap \delta y_t = 1) = \frac{1}{5}$\\
$P(x_t=2 \cap \delta y_t = 1) = \frac{1}{5}$&$P(y_t=2 \cap \delta y_t = 1) = \frac{1}{5}$\\
$P(x_t=0 \cap \delta y_t = -1) = \frac{1}{5}$&$P(y_t=0 \cap \delta y_t = -1) = 0$\\
$P(x_t=1 \cap \delta y_t = -1) = 0$&$P(y_t=1 \cap \delta y_t = -1) = \frac{1}{5}$\\
$P(x_t=2 \cap \delta y_t = -1) = 0$&$P(y_t=2 \cap \delta y_t = -1) =0$\\
\end{tabular}
\end{center}
The individual probabilities are (the ones printed above have been reprinted for convenience)
\begin{center}
\begin{tabular}{c|c}
$P(x_t = 0) = \frac{2}{5}$&$P(\delta x_t = 0) = \frac{1}{5}$\\
$P(x_t = 1) = \frac{2}{5}$&$P(\delta x_t = 1) = \frac{2}{5}$\\
$P(x_t = 2) = \frac{1}{5}$&$P(\delta x_t = -1) = \frac{2}{5}$\\
\hline
$P(y_t = 0) = \frac{2}{5}$&$P(\delta y_t = 0) = \frac{2}{5}$\\
$P(y_t = 1) = \frac{2}{5}$&$P(\delta y_t = 1) = \frac{2}{5}$\\
$P(y_t = 2) = \frac{1}{5}$&$P(\delta y_t = -1) = \frac{1}{5}$
\end{tabular}
\end{center}

Consider the penchants for $(\mathbf{x},\mathbf{\delta x})$.  The conditional probability table is
\begin{center}
\begin{tabular}{c|c|c}
$P(x_t=0 | \delta x_t = 0) = 1$ &
$P(x_t=0 | \delta x_t = 1) = 0$ &
$P(x_t=0 | \delta x_t = -1) = \frac{1}{2}$ \\

$P(x_t=1 | \delta x_t = 0) = 0$ &
$P(x_t=1 | \delta x_t = 1) = \frac{1}{2}$ &
$P(x_t=1 | \delta x_t = -1) = \frac{1}{2}$ \\

$P(x_t=2 | \delta x_t = 0) = 0$ &
$P(x_t=2 | \delta x_t = 1) = \frac{1}{2}$ &
$P(x_t=2 | \delta x_t = -1) = 0$ \\
\hline \\
$P(\delta x_t=0 | x_t = 0) = \frac{1}{2}$ &
$P(\delta x_t=0 | x_t = 1) = 0$ &
$P(\delta x_t=0 | x_t = 2) = 0$ \\

$P(\delta x_t=1 | x_t = 0) = 0$ &
$P(\delta x_t=1 | x_t = 1) = \frac{1}{2}$ &
$P(\delta x_t=1 | x_t = 2) = 1$ \\

$P(\delta x_t=-1 | x_t = 0) = \frac{1}{2}$ &
$P(\delta x_t=-1 | x_t = 1) = \frac{1}{2}$ &
$P(\delta x_t=-1 | x_t = 2) = 0$ \\
\end{tabular}
\end{center}

Consider the penchants for $(\mathbf{\delta x},\mathbf{y})$.  The conditional probability table is
\begin{center}
\begin{tabular}{c|c|c}
$P(\delta x_t=0 | y_t = 0) = \frac{1}{2}$ &
$P(\delta x_t=0 | y_t = 1) = 0$ &
$P(\delta x_t=0 | y_t = 2) = 0$ \\

$P(\delta x_t=1 | y_t = 0) = \frac{1}{2}$ &
$P(\delta x_t=1 | y_t = 1) = \frac{1}{2}$ &
$P(\delta x_t=1 | y_t = 2) = 0$ \\

$P(\delta x_t=-1 | y_t = 0) = 0$ &
$P(\delta x_t=-1 | y_t = 1) = \frac{1}{2}$ &
$P(\delta x_t=-1 | y_t = 2) = 1$ \\
\hline \\
$P(y_t=0 | \delta x_t = 0) = 1$ &
$P(y_t=0 | \delta x_t = 1) = \frac{1}{2}$ &
$P(y_t=0 | \delta x_t = -1) = 0$ \\

$P(y_t=1 | \delta x_t = 0) = 0$ &
$P(y_t=1 | \delta x_t = 1) = \frac{1}{2}$ &
$P(y_t=1 | \delta x_t = -1) = \frac{1}{2}$ \\

$P(y_t=2 | \delta x_t = 0) = 0$ &
$P(y_t=2 | \delta x_t = 1) = 0$ &
$P(y_t=2 | \delta x_t = -1) = \frac{1}{2}$ \\
\end{tabular}
\end{center}

Consider the penchants for $(\mathbf{\delta x},\mathbf{\delta y})$.  The conditional probability table is
\begin{center}
\begin{tabular}{c|c|c}
$P(\delta x_t=0 | \delta y_t = 0) = \frac{1}{2}$ &
$P(\delta x_t=0 | \delta y_t = 1) = 0$ &
$P(\delta x_t=0 | \delta y_t = -1) = 0$ \\

$P(\delta x_t=1 | \delta y_t = 0) = \frac{1}{2}$ &
$P(\delta x_t=1 | \delta y_t = 1) = \frac{1}{2}$ &
$P(\delta x_t=1 | \delta y_t = -1) = 0$ \\

$P(\delta x_t=-1 | \delta y_t = 0) = 0$ &
$P(\delta x_t=-1 | \delta y_t = 1) = \frac{1}{2}$ &
$P(\delta x_t=-1 | \delta y_t = -1) = 1$ \\
\hline \\
$P(\delta y_t=0 | \delta x_t = 0) = 1$ &
$P(\delta y_t=0 | \delta x_t = 1) = \frac{1}{2}$ &
$P(\delta y_t=0 | \delta x_t = -1) = 0$ \\

$P(\delta y_t=1 | \delta x_t = 0) = 0$ &
$P(\delta y_t=1 | \delta x_t = 1) = \frac{1}{2}$ &
$P(\delta y_t=1 | \delta x_t = -1) = \frac{1}{2}$ \\

$P(\delta y_t=-1 | \delta x_t = 0) = 0$ &
$P(\delta y_t=-1 | \delta x_t = 1) = 0$ &
$P(\delta y_t=-1 | \delta x_t = -1) = \frac{1}{2}$ \\
\end{tabular}
\end{center}

Consider the penchants for $(\mathbf{ x},\mathbf{\delta y})$.  The conditional probability table is
\begin{center}
\begin{tabular}{c|c|c}
$P(x_t=0 | \delta y_t = 0) = \frac{1}{2}$ &
$P(x_t=0 | \delta y_t = 1) = 0$ &
$P(x_t=0 | \delta y_t = -1) = 1$ \\

$P(x_t=1 | \delta y_t = 0) = \frac{1}{2}$ &
$P(x_t=1 | \delta y_t = 1) = \frac{1}{2}$ &
$P(x_t=1 | \delta y_t = -1) = 0$ \\

$P(x_t=2 | \delta y_t = 0) = 0$ &
$P(x_t=2 | \delta y_t = 1) = \frac{1}{2}$ &
$P(x_t=2 | \delta y_t = -1) = 0$ \\
\hline \\
$P(\delta y_t=0 | x_t = 0) = \frac{1}{2}$ &
$P(\delta y_t=0 | x_t = 1) = \frac{1}{2}$ &
$P(\delta y_t=0 | x_t = 2) = 0$ \\

$P(\delta y_t=1 | x_t = 0) = 0$ &
$P(\delta y_t=1 | x_t = 1) = \frac{1}{2}$ &
$P(\delta y_t=1 | x_t = 2) = 1$ \\

$P(\delta y_t=-1 | x_t = 0) = \frac{1}{2}$ &
$P(\delta y_t=-1 | x_t = 1) = 0$ &
$P(\delta y_t=-1 | x_t = 2) = 0$ \\
\end{tabular}
\end{center}

Consider the penchants for $(\mathbf{ y},\mathbf{\delta y})$.  The conditional probability table is
\begin{center}
\begin{tabular}{c|c|c}
$P(y_t=0 | \delta y_t = 0) = 1$ &
$P(y_t=0 | \delta y_t = 1) = 0$ &
$P(y_t=0 | \delta y_t = -1) = 0$ \\

$P(y_t=1 | \delta y_t = 0) = 0$ &
$P(y_t=1 | \delta y_t = 1) = \frac{1}{2}$ &
$P(y_t=1 | \delta y_t = -1) = 1$ \\

$P(y_t=2 | \delta y_t = 0) = 0$ &
$P(y_t=2 | \delta y_t = 1) = \frac{1}{2}$ &
$P(y_t=2 | \delta y_t = -1) = 0$ \\
\hline \\
$P(\delta y_t=0 | y_t = 0) = 1$ &
$P(\delta y_t=0 | y_t = 1) = 0$ &
$P(\delta y_t=0 | y_t = 2) = 0$ \\

$P(\delta y_t=1 | y_t = 0) = 0$ &
$P(\delta y_t=1 | y_t = 1) = \frac{1}{2}$ &
$P(\delta y_t=1 | y_t = 2) = 1$ \\

$P(\delta y_t=-1 | y_t = 0) = 0$ &
$P(\delta y_t=-1 | y_t = 1) = \frac{1}{2}$ &
$P(\delta y_t=-1 | y_t = 2) = 0$ \\
\end{tabular}
\end{center}

We now need the three term joint distributions, so
\begin{center}
\begin{tabular}{ccc}
$P(y_t = 0 \cap x_t = 0 \cap \delta x_t = 0) = \frac{1}{5}$ & $P(y_t = 0 \cap x_t = 0 \cap \delta x_t = 1) =0$ & $P(y_t = 0 \cap x_t = 0 \cap \delta x_t = -1) =0$ \\
$P(y_t = 0 \cap x_t = 1 \cap \delta x_t = 0) =0$ & $P(y_t = 0 \cap x_t = 1 \cap \delta x_t = 1) = \frac{1}{5}$ & $P(y_t = 0 \cap x_t = 1 \cap \delta x_t = -1) =0$ \\
$P(y_t = 0 \cap x_t = 2 \cap \delta x_t = 0) =0$ & $P(y_t = 0 \cap x_t = 2 \cap \delta x_t = 1) =0$ & $P(y_t = 0 \cap x_t = 2 \cap \delta x_t = -1) =0$ \\
$P(y_t = 1 \cap x_t = 0 \cap \delta x_t = 0) =0$ & $P(y_t = 1 \cap x_t = 0 \cap \delta x_t = 1) =0$ & $P(y_t = 1 \cap x_t = 0 \cap \delta x_t = -1) =\frac{1}{5}$ \\
$P(y_t = 1 \cap x_t = 1 \cap \delta x_t = 0) =0$ & $P(y_t = 1 \cap x_t = 1 \cap \delta x_t = 1) =0$ & $P(y_t = 1 \cap x_t = 1 \cap \delta x_t = -1) =0$ \\
$P(y_t = 1 \cap x_t = 2 \cap \delta x_t = 0) =0$ & $P(y_t = 1 \cap x_t = 2 \cap \delta x_t = 1) =\frac{1}{5}$ & $P(y_t = 1 \cap x_t = 2 \cap \delta x_t = -1) =0$ \\
$P(y_t = 2 \cap x_t = 0 \cap \delta x_t = 0) =0$ & $P(y_t = 2 \cap x_t = 0 \cap \delta x_t = 1) =0$ & $P(y_t = 2 \cap x_t = 0 \cap \delta x_t = -1) =0$ \\
$P(y_t = 2 \cap x_t = 1 \cap \delta x_t = 0) =0$ & $P(y_t = 2 \cap x_t = 1 \cap \delta x_t = 1) =0$ & $P(y_t = 2 \cap x_t = 1 \cap \delta x_t = -1) =\frac{1}{5}$ \\
$P(y_t = 2 \cap x_t = 2 \cap \delta x_t = 0) =0$ & $P(y_t = 2 \cap x_t = 2 \cap \delta x_t = 1) =0$ & $P(y_t = 2 \cap x_t = 2 \cap \delta x_t = -1) =0$ \\
\hline \\
$P(x_t = 0 \cap y_t = 0 \cap \delta y_t = 0) =\frac{1}{5}$ & $P(x_t = 0 \cap y_t = 0 \cap \delta y_t = 1) =0$ & $P(x_t = 0 \cap y_t = 0 \cap \delta y_t = -1) =0$ \\
$P(x_t = 0 \cap y_t = 1 \cap \delta y_t = 0) =0$ & $P(x_t = 0 \cap y_t = 1 \cap \delta y_t = 1) =0$ & $P(x_t = 0 \cap y_t = 1 \cap \delta y_t = -1) =\frac{1}{5}$ \\
$P(x_t = 0 \cap y_t = 2 \cap \delta y_t = 0) =0$ & $P(x_t = 0 \cap y_t = 2 \cap \delta y_t = 1) =0$ & $P(x_t = 0 \cap y_t = 2 \cap \delta y_t = -1) =0$ \\
$P(x_t = 1 \cap y_t = 0 \cap \delta y_t = 0) =\frac{1}{5}$ & $P(x_t = 1 \cap y_t = 0 \cap \delta y_t = 1) =0$ & $P(x_t = 1 \cap y_t = 0 \cap \delta y_t = -1) =0$ \\
$P(x_t = 1 \cap y_t = 1 \cap \delta y_t = 0) =0$ & $P(x_t = 1 \cap y_t = 1 \cap \delta y_t = 1) =0$ & $P(x_t = 1 \cap y_t = 1 \cap \delta y_t = -1) =0$ \\
$P(x_t = 1 \cap y_t = 2 \cap \delta y_t = 0) =0$ & $P(x_t = 1 \cap y_t = 2 \cap \delta y_t = 1) =\frac{1}{5}$ & $P(x_t = 1 \cap y_t = 2 \cap \delta y_t = -1) =0$ \\
$P(x_t = 2 \cap y_t = 0 \cap \delta y_t = 0) =0$ & $P(x_t = 2 \cap y_t = 0 \cap \delta y_t = 1) =0$ & $P(x_t = 2 \cap y_t = 0 \cap \delta y_t = -1) =0$ \\
$P(x_t = 2 \cap y_t = 1 \cap \delta y_t = 0) =0$ & $P(x_t = 2 \cap y_t = 1 \cap \delta y_t = 1) =\frac{1}{5}$ & $P(x_t = 2 \cap y_t = 1 \cap \delta y_t = -1) =0$ \\
$P(x_t = 2 \cap y_t = 2 \cap \delta y_t = 0) =0$ & $P(x_t = 2 \cap y_t = 2 \cap \delta y_t = 1) =0$ & $P(x_t = 2 \cap y_t = 2 \cap \delta y_t = -1) =0$ 
\end{tabular}
\end{center}

The new conditional distributions are
\begin{center}
\begin{tabular}{ccc}
$P(y_t = 0 | x_t = 0 \cap \delta x_t = 0) = 1$ & $P(y_t = 0 | x_t = 0 \cap \delta x_t = 1) =0$ & $P(y_t = 0 | x_t = 0 \cap \delta x_t = -1) =0$ \\
$P(y_t = 0 | x_t = 1 \cap \delta x_t = 0) =0$ & $P(y_t = 0 | x_t = 1 \cap \delta x_t = 1) = 1$ & $P(y_t = 0 | x_t = 1 \cap \delta x_t = -1) =0$ \\
$P(y_t = 0 | x_t = 2 \cap \delta x_t = 0) =0$ & $P(y_t = 0 | x_t = 2 \cap \delta x_t = 1) =0$ & $P(y_t = 0 | x_t = 2 \cap \delta x_t = -1) =0$ \\
$P(y_t = 1 | x_t = 0 \cap \delta x_t = 0) =0$ & $P(y_t = 1 | x_t = 0 \cap \delta x_t = 1) =0$ & $P(y_t = 1 | x_t = 0 \cap \delta x_t = -1) =1$ \\
$P(y_t = 1 | x_t = 1 \cap \delta x_t = 0) =0$ & $P(y_t = 1 | x_t = 1 \cap \delta x_t = 1) =0$ & $P(y_t = 1 | x_t = 1 \cap \delta x_t = -1) =0$ \\
$P(y_t = 1 | x_t = 2 \cap \delta x_t = 0) =0$ & $P(y_t = 1 | x_t = 2 \cap \delta x_t = 1) =1$ & $P(y_t = 1 | x_t = 2 \cap \delta x_t = -1) =0$ \\
$P(y_t = 2 | x_t = 0 \cap \delta x_t = 0) =0$ & $P(y_t = 2 | x_t = 0 \cap \delta x_t = 1) =0$ & $P(y_t = 2 | x_t = 0 \cap \delta x_t = -1) =0$ \\
$P(y_t = 2 | x_t = 1 \cap \delta x_t = 0) =0$ & $P(y_t = 2 | x_t = 1 \cap \delta x_t = 1) =0$ & $P(y_t = 2 | x_t = 1 \cap \delta x_t = -1) =1$ \\
$P(y_t = 2 | x_t = 2 \cap \delta x_t = 0) =0$ & $P(y_t = 2 | x_t = 2 \cap \delta x_t = 1) =0$ & $P(y_t = 2 | x_t = 2 \cap \delta x_t = -1) =0$ \\
\hline \\
$P(x_t = 0 | y_t = 0 \cap \delta y_t = 0) =\frac{1}{2}$ & $P(x_t = 0 | y_t = 0 \cap \delta y_t = 1) =0$ & $P(x_t = 0 | y_t = 0 \cap \delta y_t = -1) =0$ \\
$P(x_t = 0 | y_t = 1 \cap \delta y_t = 0) =0$ & $P(x_t = 0 | y_t = 1 \cap \delta y_t = 1) =0$ & $P(x_t = 0 | y_t = 1 \cap \delta y_t = -1) =1$ \\
$P(x_t = 0 | y_t = 2 \cap \delta y_t = 0) =0$ & $P(x_t = 0 | y_t = 2 \cap \delta y_t = 1) =0$ & $P(x_t = 0 | y_t = 2 \cap \delta y_t = -1) =0$ \\
$P(x_t = 1 | y_t = 0 \cap \delta y_t = 0) =\frac{1}{2}$ & $P(x_t = 1 | y_t = 0 \cap \delta y_t = 1) =0$ & $P(x_t = 1 | y_t = 0 \cap \delta y_t = -1) =0$ \\
$P(x_t = 1 | y_t = 1 \cap \delta y_t = 0) =0$ & $P(x_t = 1 | y_t = 1 \cap \delta y_t = 1) =0$ & $P(x_t = 1 | y_t = 1 \cap \delta y_t = -1) =0$ \\
$P(x_t = 1 | y_t = 2 \cap \delta y_t = 0) =0$ & $P(x_t = 1 | y_t = 2 \cap \delta y_t = 1) =1$ & $P(x_t = 1 | y_t = 2 \cap \delta y_t = -1) =0$ \\
$P(x_t = 2 | y_t = 0 \cap \delta y_t = 0) =0$ & $P(x_t = 2 | y_t = 0 \cap \delta y_t = 1) =0$ & $P(x_t = 2 | y_t = 0 \cap \delta y_t = -1) =0$ \\
$P(x_t = 2 | y_t = 1 \cap \delta y_t = 0) =0$ & $P(x_t = 2 | y_t = 1 \cap \delta y_t = 1) =1$ & $P(x_t = 2 | y_t = 1 \cap \delta y_t = -1) =0$ \\
$P(x_t = 2 | y_t = 2 \cap \delta y_t = 0) =0$ & $P(x_t = 2 | y_t = 2 \cap \delta y_t = 1) =0$ & $P(x_t = 2 | y_t = 2 \cap \delta y_t = -1) =0$ 
\end{tabular}
\end{center}

In general, the penchant can be written using the law of total probability, i.e
$$
P(E) = P(E|C)P(C) + P(E|\bar{C})P(\bar{C})\;\;,
$$
and Bayes theorem, i.e.\ 
$$
P(E|\bar{C}) = P(\bar{C}|E)\frac{P(E)}{P(\bar{C})}\;\;.
$$
The probability complement rules yield $P(\bar{C}) = 1-P(C)$ and $P(\bar{C}|E) = 1-P(C|E)$.  Applying Bayes again leads to
$$
P(\bar{C}|E) = 1-P(E|C)\frac{P(C)}{P(E)}
$$
Thus,
$$
P(E|\bar{C}) = \left(1-P(E|C)\frac{P(C)}{P(E)}\right)\frac{P(E)}{1-P(C)}\;\;.
$$
This expression implies
\begin{eqnarray}
\rho &=& P(E|C)-P(E|\bar{C})\\
&=& P(E|C)-\left(1-P(E|C)\frac{P(C)}{P(E)}\right)\frac{P(E)}{1-P(C)}\\
&=& P(E|C)-\frac{P(E)}{1-P(C)}+P(E|C)\frac{P(C)}{1-P(C)}\\
&=& P(E|C)\left(1+\frac{P(C)}{1-P(C)}\right)-\frac{P(E)}{1-P(C)}\\
&=& \frac{P(E\cap C)}{P(C)}\left(1+\frac{P(C)}{1-P(C)}\right)-\frac{P(E)}{1-P(C)}\\
&=& P(E\cap C)\left(\frac{1}{P(C)}+\frac{1}{1-P(C)}\right)-\frac{P(E)}{1-P(C)}\;\;.
\end{eqnarray}
This same expression can be derived without Bayes theorem by using the law of total probability rewritten as
$$
P(E|\bar{C}) = \frac{P(E)}{P(\bar{C})} - P(E|C)\frac{P(C)}{P(\bar{C})}
$$
and making the appropriate substitution into the definition of the penchants follows:
\begin{eqnarray}
\rho &=& P(E|C)-P(E|\bar{C})\\
&=& P(E|C)-\left(\frac{P(E)}{P(\bar{C})} - P(E|C)\frac{P(C)}{P(\bar{C})}\right)\\
&=& P(E|C)-\frac{P(E)}{P(\bar{C})} + P(E|C)\frac{P(C)}{P(\bar{C})}\\
&=& P(E|C)\left(1+\frac{P(C)}{P(\bar{C})}\right)-\frac{P(E)}{P(\bar{C})}\\
&=& P(E|C)\left(1+\frac{P(C)}{1-P(C)}\right)-\frac{P(E)}{1-P(C)}\\
&=& \frac{P(E\cap C)}{P(C)}\left(1+\frac{P(C)}{1-P(C)}\right)-\frac{P(E)}{1-P(C)}\\
&=& P(E\cap C)\left(\frac{1}{P(C)}+\frac{1}{1-P(C)}\right)-\frac{P(E)}{1-P(C)}\;\;.
\end{eqnarray}

Consider the cause-effect pair $(E,C)=(x_t=0,y_t=0)$ penchant
\begin{eqnarray}
\rho_{xy}^{(1)} &=& P(x_t = 0 | y_t = 0) - P(x_t = 0 | y_t \neq 0)\\
&=&  P(x_t = 0 | y_t = 0)\left(1+\frac{P(y_t = 0)}{1-P(y_t = 0)}\right)-\frac{P(x_t = 0)}{1-P(y_t = 0)}\\
&=& \frac{1}{2}\left(1+\frac{2}{5}\left(1-\frac{2}{5}\right)^{-1}\right)-\frac{2}{5}\left(1-\frac{2}{5}\right)^{-1}\\
&=& \frac{1}{6}
\end{eqnarray}
and its partner
\begin{eqnarray}
\rho_{yx}^{(1)} &=& P(y_t = 0 | x_t = 0) - P(y_t = 0 | x_t \neq 0)\\
&=& P(y_t = 0|x_t = 0)\left(1+\frac{P(x_t = 0)}{1-P(x_t = 0)}\right)-\frac{P(y_t = 0)}{1-P(x_t = 0)}\\
&=& \frac{1}{2}\left(1+\frac{2}{5}\left(1-\frac{2}{5}\right)^{-1}\right)-\frac{2}{5}\left(1-\frac{2}{5}\right)^{-1}\\
&=& \frac{1}{6}\;\;.
\end{eqnarray}

In general, a cause-effect pair $(C,E)$ depends on $P(E|C)$, $P(E)$, and $P(C)$.  Thus, if $P(E|C)=P(C|E)$ and $P(E)=P(C)$, then the $(C,E)$ penchant is equal to the $(E,C)$ penchant.  It follows that $\langle \rho_{xy} \rangle = \langle \rho_{yx} \rangle$.

The hope here is that the penchants for the cause-effect pairs $(E,C)=(\delta x_t,y_t)$ and its partner will lead to the intuitive conclusion $\mathbf{x}\rightarrow\mathbf{y}$.  The first such pair is $(E,C)=(\delta x_t=0,y_t=0)$ which leads to
\begin{eqnarray}
\rho_{\delta xy}^{(1)} &=& P(\delta x_t = 0 | y_t = 0) - P(\delta x_t = 0 | y_t \neq 0)\\
&=&  P(\delta x_t = 0 | y_t = 0)\left(1+\frac{P(y_t = 0)}{1-P(y_t = 0)}\right)-\frac{P(\delta x_t = 0)}{1-P(y_t = 0)}\\
&=& \frac{1}{2}\left(1+\frac{2}{5}\left(1-\frac{2}{5}\right)^{-1}\right)-\frac{1}{5}\left(1-\frac{2}{5}\right)^{-1}\\
&=& \frac{1}{2}
\end{eqnarray}
and
\begin{eqnarray}
\rho_{y\delta x}^{(1)} &=& P(y_t = 0 | \delta x_t = 0) - P(y_t = 0 | \delta x_t \neq 0)\\
&=& P(y_t = 0|\delta x_t = 0)\left(1+\frac{P(\delta x_t = 0)}{1-P(\delta x_t = 0)}\right)-\frac{P(y_t = 0)}{1-P(\delta x_t = 0)}\\
&=& \left(1+\frac{1}{5}\left(1-\frac{1}{5}\right)^{-1}\right)-\frac{2}{5}\left(1-\frac{1}{5}\right)^{-1}\\
&=& \frac{3}{4}\;\;.
\end{eqnarray}
Thus, $\rho_{y\delta x}^{(1)}>\rho_{\delta xy}^{(1)}$ implying $\mathbf{\delta x}\rightarrow\mathbf{y}$, as expected.  The mean penchants for this cause-effect pair are
\begin{eqnarray}
\langle \rho_{\delta xy}\rangle &=& \frac{1}{9}\left(\sum_{i=-1}^1 \sum_{j=0}^2 P(\delta x_t = i | y_t = j) - P(\delta x_t = i | y_t \neq j)\right)\\
&=& \frac{1}{9}\left(\sum_{i=-1}^1 \sum_{j=0}^2 P(\delta x_t = i | y_t = j)\left(1+\frac{P(y_t = j)}{1-P(y_t = j)}\right)-\frac{P(\delta x_t = i)}{1-P(y_t = j)}\right)\\
&=& \frac{1}{9}\left(P(\delta x_t = 0 | y_t = 0)\left(1+\frac{P(y_t = 0)}{1-P(y_t = 0)}\right)-\frac{P(\delta x_t = 0)}{1-P(y_t = 0)}\right.\\
& &P(\delta x_t = 0 | y_t = 1)\left(1+\frac{P(y_t = 1)}{1-P(y_t = 1)}\right)-\frac{P(\delta x_t = 0)}{1-P(y_t = 1)}\\
& &P(\delta x_t = 0 | y_t = 2)\left(1+\frac{P(y_t = 2)}{1-P(y_t = 2)}\right)-\frac{P(\delta x_t = 0)}{1-P(y_t = 2)}\\
& &P(\delta x_t = 1 | y_t = 0)\left(1+\frac{P(y_t = 0)}{1-P(y_t = 0)}\right)-\frac{P(\delta x_t = 1)}{1-P(y_t = 0)}\\
& &P(\delta x_t = 1 | y_t = 1)\left(1+\frac{P(y_t = 1)}{1-P(y_t = 1)}\right)-\frac{P(\delta x_t = 1)}{1-P(y_t = 1)}\\
& &P(\delta x_t = 1 | y_t = 2)\left(1+\frac{P(y_t = 2)}{1-P(y_t = 2)}\right)-\frac{P(\delta x_t = 1)}{1-P(y_t = 2)}\\
& &P(\delta x_t = -1 | y_t = 0)\left(1+\frac{P(y_t = 0)}{1-P(y_t = 0)}\right)-\frac{P(\delta x_t = -1)}{1-P(y_t = 0)}\\
& &P(\delta x_t = -1 | y_t = 1)\left(1+\frac{P(y_t = 1)}{1-P(y_t = 1)}\right)-\frac{P(\delta x_t = -1)}{1-P(y_t = 1)}\\
& &\left. P(\delta x_t = -1 | y_t = 2)\left(1+\frac{P(y_t = 2)}{1-P(y_t = 2)}\right)-\frac{P(\delta x_t = -1)}{1-P(y_t = 2)}\right)\\
&=& \frac{1}{9}\left(\frac{1}{2}\left(1+\frac{\frac{2}{5}}{1-\frac{2}{5}}\right)-\frac{\frac{1}{5}}{1-\frac{2}{5}}\right.\\
& &-\frac{\frac{1}{5}}{1-\frac{2}{5}}\\
& &-\frac{\frac{1}{5}}{1-\frac{1}{5}}\\
& &\frac{1}{2}\left(1+\frac{\frac{2}{5}}{1-\frac{2}{5}}\right)-\frac{\frac{2}{5}}{1-\frac{2}{5}}\\
& &\frac{1}{2}\left(1+\frac{\frac{2}{5}}{1-\frac{2}{5}}\right)-\frac{\frac{2}{5}}{1-\frac{2}{5}}\\
& &-\frac{\frac{2}{5}}{1-\frac{1}{5}}\\
& &-\frac{\frac{2}{5}}{1-\frac{2}{5}}\\
& &\frac{1}{2}\left(1+\frac{\frac{2}{5}}{1-\frac{2}{5}}\right)-\frac{\frac{2}{5}}{1-\frac{2}{5}}\\
& &\left. \left(1+\frac{\frac{1}{5}}{1-\frac{1}{5}}\right)-\frac{\frac{2}{5}}{1-\frac{1}{5}}\right)\\
&=& \frac{1}{9}\left(\frac{1}{2}-\frac{1}{3}-\frac{1}{4}+\frac{1}{6}+\frac{1}{6}-\frac{1}{2}-\frac{2}{3}+\frac{1}{6}+\frac{3}{4}\right)\\
&=& 0
\end{eqnarray}
and
\begin{eqnarray}
\langle \rho_{y\delta x}\rangle &=& \frac{1}{9}\left(\sum_{i=-1}^1 \sum_{j=0}^2 P(y_t = j | \delta x_t = i) - P(y_t = j | \delta x_t \neq i)\right)\\
&=& \frac{1}{9}\left(\sum_{i=-1}^1 \sum_{j=0}^2 P(y_t = j | \delta x_t = i)\left(1+\frac{P(\delta x_t = i)}{1-P(\delta x_t = i)}\right)-\frac{P(y_t = j)}{1-P(\delta x_t = i)}\right)\\
&=& \frac{1}{9}\left(P(y_t = 0 | \delta x_t = 0)\left(1+\frac{P(\delta x_t = 0)}{1-P(\delta x_t = 0)}\right)-\frac{P(y_t = 0)}{1-P(\delta x_t = 0)}\right.\\
& & P(y_t = 0 | \delta x_t = 1)\left(1+\frac{P(\delta x_t = 1)}{1-P(\delta x_t = 1)}\right)-\frac{P(y_t = 0)}{1-P(\delta x_t = 1)}\\
& & P(y_t = 0 | \delta x_t = -1)\left(1+\frac{P(\delta x_t = -1)}{1-P(\delta x_t = -1)}\right)-\frac{P(y_t = 0)}{1-P(\delta x_t = -1)}\\
& & P(y_t = 1 | \delta x_t = 0)\left(1+\frac{P(\delta x_t = 0)}{1-P(\delta x_t = 0)}\right)-\frac{P(y_t = 1)}{1-P(\delta x_t = 0)}\\
& & P(y_t = 1 | \delta x_t = 1)\left(1+\frac{P(\delta x_t = 1)}{1-P(\delta x_t = 1)}\right)-\frac{P(y_t = 1)}{1-P(\delta x_t = 1)}\\
& & P(y_t = 1 | \delta x_t = -1)\left(1+\frac{P(\delta x_t = -1)}{1-P(\delta x_t = -1)}\right)-\frac{P(y_t = 1)}{1-P(\delta x_t = -1)}\\
& & P(y_t = 2 | \delta x_t = 0)\left(1+\frac{P(\delta x_t = 0)}{1-P(\delta x_t = 0)}\right)-\frac{P(y_t = 2)}{1-P(\delta x_t = 0)}\\
& & P(y_t = 2 | \delta x_t = 1)\left(1+\frac{P(\delta x_t = 1)}{1-P(\delta x_t = 1)}\right)-\frac{P(y_t = 2)}{1-P(\delta x_t = 1)}\\
& & \left.P(y_t = 2 | \delta x_t = -1)\left(1+\frac{P(\delta x_t = -1)}{1-P(\delta x_t = -1)}\right)-\frac{P(y_t = 2)}{1-P(\delta x_t = -1)}\right)\\
&=& \frac{1}{9}\left(\left(1+\frac{\frac{1}{5}}{1-\frac{1}{5}}\right)-\frac{\frac{2}{5}}{1-\frac{1}{5}}\right.\\
& & \frac{1}{2}\left(1+\frac{\frac{2}{5}}{1-\frac{2}{5}}\right)-\frac{\frac{2}{5}}{1-\frac{2}{5}}\\
& & -\frac{\frac{2}{5}}{1-\frac{2}{5}}\\
& & -\frac{\frac{2}{5}}{1-\frac{1}{5}}\\
& & \frac{1}{2}\left(1+\frac{\frac{2}{5}}{1-\frac{2}{5}}\right)-\frac{\frac{2}{5}}{1-\frac{2}{5}}\\
& & \frac{1}{2}\left(1+\frac{\frac{2}{5}}{1-\frac{2}{5}}\right)-\frac{\frac{2}{5}}{1-\frac{2}{5}}\\
& & -\frac{\frac{1}{5}}{1-\frac{1}{5}}\\
& & -\frac{\frac{1}{5}}{1-\frac{2}{5}}\\
& & \left.\frac{1}{2}\left(1+\frac{\frac{2}{5}}{1-\frac{2}{5}}\right)-\frac{\frac{1}{5}}{1-\frac{2}{5}}\right)\\
&=& \frac{1}{9}\left(\frac{3}{4}+\frac{1}{6}-\frac{2}{3}-\frac{1}{2}+\frac{1}{6}+\frac{1}{6}-\frac{1}{4}-\frac{1}{3}+\frac{1}{2}\right)\\
&=& 0\;\;.
\end{eqnarray}
It follows that $\langle \rho_{y\delta x}\rangle = \langle \rho_{\delta xy}\rangle$ which does not imply that $\mathbf{x}\rightarrow\mathbf{y}$.  The result does not agree with intuition.

There are several other cause-effect pairs to investigate in this example including the ``primary penchant comparisons'' $(\mathbf{x},\mathbf{y})$, $(\delta \mathbf{x},\mathbf{y})$, $(\mathbf{x},\delta \mathbf{y})$, and $(\delta \mathbf{x},\delta \mathbf{y})$, the ``self penchant comparisons'' $(\mathbf{x},\delta \mathbf{x})$ and $(\mathbf{y},\delta \mathbf{y})$, the ``higher order penchant comparisons'' $(\mathbf{x}\cap\delta\mathbf{x},\mathbf{y})$ and $(\mathbf{y}\cap\delta\mathbf{y},\mathbf{x})$, and the ``highest order penchant comparison'' $(\mathbf{x}\cap\delta\mathbf{x},\mathbf{y}\cap\delta\mathbf{y})$.  Given the intution for this example system, i.e.\ $\mathbf{x}\rightarrow\mathbf{y}$, the following predictions for the mean penchant comparisons for each of the cause-effect pairs mentioned above can be made:
\begin{center}
\begin{tabular}{r|cllll}
Cause-Effect Pair & Prediction & Confirmed? & Confirmed Num? & Calculated? & Cal Num?\\
$(\mathbf{x},\mathbf{y})$ & {\bf ??} & n/a & n/a & yes & yes\\
$(\delta \mathbf{x},\mathbf{y})$ & $\langle \rho_{y\delta x}\rangle > \langle \rho_{\delta xy}\rangle$ & no & no & yes & yes\\
$(\mathbf{x},\delta \mathbf{y})$ & {\bf ??} & n/a & n/a & no & no\\
$(\delta \mathbf{x},\delta \mathbf{y})$ &  $\langle \rho_{\delta y\delta x}\rangle > \langle \rho_{\delta x\delta y}\rangle$ & no & no & no & no\\
$(\mathbf{x},\delta \mathbf{x})$ & $\langle \rho_{x\delta x}\rangle > \langle \rho_{\delta xx}\rangle$ & no & no & no & no\\
$(\mathbf{y},\delta \mathbf{y})$ & $\langle \rho_{y\delta y}\rangle > \langle \rho_{\delta yy}\rangle$ & no & no & no & no\\
$(\mathbf{x}\cap\delta\mathbf{x},\mathbf{y})$ & $\langle \rho_{(x\delta x)y}\rangle < \langle \rho_{y(x\delta x)}\rangle$ & no & no & no & no\\ 
$(\mathbf{y}\cap\delta\mathbf{y},\mathbf{x})$ & {\bf ??} & no & no & no & no\\
$(\mathbf{x}\cap\delta\mathbf{x},\mathbf{y}\cap\delta\mathbf{y})$ & $\langle \rho_{(x\delta x)(y\delta y)}\rangle < \langle \rho_{(y\delta y)(x\delta x)}\rangle$ & no & no & no & no
\end{tabular}
\end{center}

The penchant comparison calculations can be done by hand (as seen above) but they are tedious.  So, the next step is to write an algorithm that reproduces the above results and allows me to fill out the above table.  The computational cost of the penchant comparison calculation increase dramatically for the higher and highest order penchants (as compared to the primary and self penchants).  There will also be an issue of bin sizes in the algorithms where histograms will be used to approximately the probabilities used above.  The dependency of the algorithm on the bin size needs to be studied.  

The penchants calculated by hand do not seem to agree with intuition.  However, the calculations can be confirmed numerically, so it may appear as though this method is not an effective test for causal dependence.  I want to point out that this may only be true for the (very) short library lengths used in this example.  The short library length was used to help keep the manual calculations manageable, however, increasing the library length (but following the same construction pattern) leads to penchant calculations that do agree with intuition, i.e.\
\begin{equation}
\left.
\begin{array}{c}
\mathbf{x} = \{0,1,2,1,0\}\\
\mathbf{y} = \{0,0,1,2,1\}\\
\mathbf{\delta x} = \{0,1,1,-1,-1\}\\
\mathbf{\delta y} = \{0,0,1,1,-1\}
\end{array}
\right] \Rightarrow \left(\langle \rho_{y\delta x}\rangle, \langle \rho_{\delta xy}\rangle\right) = \left(0,0\right) \rightarrow \langle \rho_{y\delta x}\rangle = \langle \rho_{\delta xy}\rangle
\end{equation}
and just doubling the library length leads to
\begin{equation}
\left.
\begin{array}{c}
\mathbf{x} = \{0,1,2,1,0,1,2,1,0,1\}\\
\mathbf{y} = \{0,0,1,2,1,0,1,2,1,0\}\\
\mathbf{\delta x} = \{0,1,1,-1,-1,1,1,-1,-1,1\}\\
\mathbf{\delta y} = \{0,0,1,1,-1,1,1,-1,-1,1\}
\end{array}
\right] \Rightarrow \left(\langle \rho_{y\delta x}\rangle, \langle \rho_{\delta xy}\rangle\right) \approx \left(-0.137,-0.162\right) \rightarrow \langle \rho_{y\delta x}\rangle > \langle \rho_{\delta xy}\rangle
\end{equation}
and 
\begin{eqnarray}
& &\left.
\begin{array}{c}
\mathbf{x} = \{0,1,2,1,0,1,2,1,0,1,2,1,0,1,2,1,0,1,2,1,0,1,2,1,0\}\\
\mathbf{y} = \{0,0,1,2,1,0,1,2,1,0,1,2,1,0,1,2,1,0,1,2,1,0,1,2,1\}\\
\mathbf{\delta x} = \{0,1,1,-1,-1,1,1,-1,-1,1,1,-1,-1,1,1,-1,-1,1,1,-1,-1,1,1,-1,-1\}\\
\mathbf{\delta y} = \{0,0,1,1,-1,-1,1,1,-1,-1,1,1,-1,-1,1,1,-1,-1,1,1,-1,-1,1,1,-1,-1\}
\end{array}
\right] \\
&\Rightarrow& \left(\langle \rho_{y\delta x}\rangle, \langle \rho_{\delta xy}\rangle\right) \approx \left(-0.267
,-0.3\right) \rightarrow \langle \rho_{y\delta x}\rangle > \langle \rho_{\delta xy}\rangle
\end{eqnarray}
Thus, the results agree with intuition if the library lengths are longer than those used in the manual calculation example.  This point should be kept in mind and the effect of library length on the LIR penchant method (which probably needs a new name) should be investigated.  All of the above number where calculated using histograms with only three, the final conclusions do not change for the number of bins is increased to ten but the actual value of the penchants do.  Again, this effect should also be explored.

The main task in estimating causal influence using the LIR method described above will revolve around comparing mean penchants to determine, e.g.\ $\langle \rho_{yx}\rangle > \langle \rho_{xy}\rangle$.  As single-valued metric will be constructed to facilitate the discussion as follows:
$$
\Delta_{yx} = \langle \rho_{yx}\rangle - \langle \rho_{xy}\rangle\;\;.
$$
$\Delta_{yx} > 0\Rightarrow \langle \rho_{yx}\rangle > \langle \rho_{xy}\rangle$, $\Delta_{yx} < 0\Rightarrow \langle \rho_{yx}\rangle < \langle \rho_{xy}\rangle$, and $\Delta_{yx} = 0\Rightarrow \langle \rho_{yx}\rangle = \langle \rho_{xy}\rangle$.  Thus, the sign of $\Delta$ can be used to estimate the causal influence as $\Delta_{yx} > 0\Rightarrow \mathbf{x}\rightarrow\mathbf{y}$, $\Delta_{yx} < 0\Rightarrow \mathbf{y}\rightarrow\mathbf{x}$, and $\Delta_{yx} = 0\Rightarrow$ no conclusion can be made.

The mean is a linear operator, so in general
\begin{eqnarray}
\Delta_{yx} &=& \langle \rho_{yx}\rangle - \langle \rho_{xy}\rangle\\
&=& \langle \rho_{yx} - \rho_{xy}\rangle\\
&=& \left\langle \left( P(y_t\cap x_t)\left(\frac{1}{P(x_t)}+\frac{1}{1-P(x_t)}\right)-\frac{P(y_t)}{1-P(x_t)}\right) \right.\\
& & \left.- \left( P(x_t\cap y_t)\left(\frac{1}{P(y_t)}+\frac{1}{1-P(y_t)}\right)-\frac{P(x_t)}{1-P(y_t)} \right) \right\rangle\\
&=& \left\langle P(y_t\cap x_t) \left( \frac{1}{P(x_t)}+\frac{1}{1-P(x_t)} - \frac{1}{P(y_t)}-\frac{1}{1-P(y_t)}\right) -\frac{P(y_t)}{1-P(x_t)}+\frac{P(x_t)}{1-P(y_t)}  \right\rangle
\end{eqnarray}
because $P(A\cap B) = P(B\cap A)$.  As an example, consider the time series $\mathbf{\delta x}$ and $\mathbf{y}$ from above:
\begin{eqnarray*}
\Delta_{y\delta x} &=& \langle \rho_{y\delta x}\rangle - \langle \rho_{\delta xy}\rangle\\
&=& \left\langle P(y_t\cap \delta x_t) \left( \frac{1}{P(\delta x_t)}+\frac{1}{1-P(\delta x_t)} - \frac{1}{P(y_t)}-\frac{1}{1-P(y_t)}\right) -\frac{P(y_t)}{1-P(\delta x_t)}+\frac{P(\delta x_t)}{1-P(y_t)}  \right\rangle\\
&=& \frac{1}{9}\left(\sum_{i=-1}^1 \sum_{j=0}^2 P(y_t=j\cap \delta x_t=i) \left( \frac{1}{P(\delta x_t=i)}+\frac{1}{1-P(\delta x_t=i)} - \frac{1}{P(y_t=j)}-\frac{1}{1-P(y_t=j)}\right)\right. \\
& &\left. -\frac{P(y_t=j)}{1-P(\delta x_t=i)}+\frac{P(\delta x_t=i)}{1-P(y_t=j)}\right)\\
&=& \frac{1}{9}\left(P(y_t=0\cap \delta x_t=-1) \left( \frac{1}{P(\delta x_t=-1)}+\frac{1}{1-P(\delta x_t=-1)} - \frac{1}{P(y_t=0)}-\frac{1}{1-P(y_t=0)}\right)\right.\\
& &-\frac{P(y_t=0)}{1-P(\delta x_t=-1)}+\frac{P(\delta x_t=-1)}{1-P(y_t=0)}\\
& &+ P(y_t=1\cap \delta x_t=-1) \left( \frac{1}{P(\delta x_t=-1)}+\frac{1}{1-P(\delta x_t=-1)} - \frac{1}{P(y_t=1)}-\frac{1}{1-P(y_t=1)}\right)\\
& &-\frac{P(y_t=1)}{1-P(\delta x_t=-1)}+\frac{P(\delta x_t=-1)}{1-P(y_t=1)}\\
& &+ P(y_t=2\cap \delta x_t=-1) \left( \frac{1}{P(\delta x_t=-1)}+\frac{1}{1-P(\delta x_t=-1)} - \frac{1}{P(y_t=2)}-\frac{1}{1-P(y_t=2)}\right)\\
& &-\frac{P(y_t=2)}{1-P(\delta x_t=-1)}+\frac{P(\delta x_t=-1)}{1-P(y_t=2)}\\
& &+P(y_t=0\cap \delta x_t=0) \left( \frac{1}{P(\delta x_t=0)}+\frac{1}{1-P(\delta x_t=0)} - \frac{1}{P(y_t=0)}-\frac{1}{1-P(y_t=0)}\right)\\
& &-\frac{P(y_t=0)}{1-P(\delta x_t=0)}+\frac{P(\delta x_t=0)}{1-P(y_t=0)}\\
& &+ P(y_t=1\cap \delta x_t=0) \left( \frac{1}{P(\delta x_t=0)}+\frac{1}{1-P(\delta x_t=0)} - \frac{1}{P(y_t=1)}-\frac{1}{1-P(y_t=1)}\right)\\
& &-\frac{P(y_t=1)}{1-P(\delta x_t=0)}+\frac{P(\delta x_t=0)}{1-P(y_t=1)}\\
& &+ P(y_t=2\cap \delta x_t=0) \left( \frac{1}{P(\delta x_t=0)}+\frac{1}{1-P(\delta x_t=0)} - \frac{1}{P(y_t=2)}-\frac{1}{1-P(y_t=2)}\right)\\
& &-\frac{P(y_t=2)}{1-P(\delta x_t=0)}+\frac{P(\delta x_t=0)}{1-P(y_t=2)}\\
& &+P(y_t=0\cap \delta x_t=1) \left( \frac{1}{P(\delta x_t=1)}+\frac{1}{1-P(\delta x_t=1)} - \frac{1}{P(y_t=0)}-\frac{1}{1-P(y_t=0)}\right)\\
& &-\frac{P(y_t=0)}{1-P(\delta x_t=1)}+\frac{P(\delta x_t=1)}{1-P(y_t=0)}\\
& &+ P(y_t=1\cap \delta x_t=1) \left( \frac{1}{P(\delta x_t=1)}+\frac{1}{1-P(\delta x_t=1)} - \frac{1}{P(y_t=1)}-\frac{1}{1-P(y_t=1)}\right)\\
& &-\frac{P(y_t=1)}{1-P(\delta x_t=1)}+\frac{P(\delta x_t=1)}{1-P(y_t=1)}\\
& &+ P(y_t=2\cap \delta x_t=1) \left( \frac{1}{P(\delta x_t=1)}+\frac{1}{1-P(\delta x_t=1)} - \frac{1}{P(y_t=2)}-\frac{1}{1-P(y_t=2)}\right)\\
& &\left.-\frac{P(y_t=2)}{1-P(\delta x_t=1)}+\frac{P(\delta x_t=1)}{1-P(y_t=2)}\right)
\end{eqnarray*}
$\ldots$continued$\ldots$
\begin{eqnarray*}
&=& \frac{1}{9}\left(-\frac{\frac{2}{5}}{1-\frac{2}{5}}+\frac{\frac{2}{5}}{1-\frac{2}{5}}+ \frac{1}{5} \left( \frac{1}{\frac{2}{5}}+\frac{1}{1-\frac{2}{5}} - \frac{1}{\frac{2}{5}}-\frac{1}{1-\frac{2}{5}}\right)\right.\\
& &-\frac{\frac{2}{5}}{1-\frac{2}{5}}+\frac{\frac{2}{5}}{1-\frac{2}{5}}+ \frac{1}{5} \left( \frac{1}{\frac{2}{5}}+\frac{1}{1-\frac{2}{5}} - \frac{1}{\frac{1}{5}}-\frac{1}{1-\frac{1}{5}}\right)\\
& &-\frac{\frac{1}{5}}{1-\frac{2}{5}}+\frac{\frac{2}{5}}{1-\frac{1}{5}}+\frac{1}{5} \left( \frac{1}{\frac{1}{5}}+\frac{1}{1-\frac{1}{5}} - \frac{1}{\frac{2}{5}}-\frac{1}{1-\frac{2}{5}}\right)\\
& &-\frac{\frac{2}{5}}{1-\frac{1}{5}}+\frac{\frac{1}{5}}{1-\frac{2}{5}}-\frac{\frac{2}{5}}{1-\frac{1}{5}}+\frac{\frac{1}{5}}{1-\frac{2}{5}}\\
& &-\frac{\frac{1}{5}}{1-\frac{1}{5}}+\frac{\frac{1}{5}}{1-\frac{1}{5}}+\frac{1}{5} \left( \frac{1}{\frac{2}{5}}+\frac{1}{1-\frac{2}{5}} - \frac{1}{\frac{2}{5}}-\frac{1}{1-\frac{2}{5}}\right)\\
& &-\frac{\frac{2}{5}}{1-\frac{2}{5}}+\frac{\frac{2}{5}}{1-\frac{2}{5}}+\frac{1}{5} \left( \frac{1}{\frac{2}{5}}+\frac{1}{1-\frac{2}{5}} - \frac{1}{\frac{2}{5}}-\frac{1}{1-\frac{2}{5}}\right)\\
& &\left.-\frac{\frac{2}{5}}{1-\frac{2}{5}}+\frac{\frac{2}{5}}{1-\frac{2}{5}}-\frac{\frac{1}{5}}{1-\frac{2}{5}}+\frac{\frac{2}{5}}{1-\frac{1}{5}}\right)\\
&=& \frac{1}{9}\left(-\frac{2}{3}+\frac{2}{3}+\frac{1}{5} \left(\frac{5}{2}+\frac{5}{3}-\frac{5}{2}-\frac{5}{3}\right)\right.\\
& &-\frac{2}{3}+\frac{2}{3}+ \frac{1}{5} \left(\frac{5}{2}+\frac{5}{3}-\frac{5}{1}-\frac{5}{4}\right)\\
& &-\frac{1}{3}+\frac{2}{4}+\frac{1}{5} \left(\frac{5}{1}+\frac{5}{4}-\frac{5}{2}-\frac{5}{3}\right)\\
& &-\frac{2}{4}+\frac{1}{3}-\frac{2}{4}+\frac{1}{3}\\
& &-\frac{1}{4}+\frac{1}{4}+\frac{1}{5} \left(\frac{5}{2}+\frac{5}{3}-\frac{5}{2}-\frac{5}{3}\right)\\
& &-\frac{2}{3}+\frac{2}{3}+\frac{1}{5} \left(\frac{5}{2}+\frac{5}{3}-\frac{5}{2}-\frac{5}{3}\right)\\
& &\left.-\frac{2}{3}+\frac{2}{3}-\frac{1}{3}+\frac{2}{4}\right)\\
&=& 0\;\;,
\end{eqnarray*}
as expected from above.  The variable $\Delta_{EC}$ is the mean difference of the available (dependent on the number of bins in the histograms) penchants between two times series (or, in general, any two data sets; the temporal aspect of this causal analysis is only captured in the LIR techinques, i.e.\ in the definitions of $\delta x$ and $\delta y$, and not in the definitions of the penchants).  $\Delta_{EC}$ will become the workhorse of our causal inference and thus, will be called the {\em leaning} of the pair of data sets.  Giving $\Delta_{EC}$ a name is simply to help save time typing in the discussions that follow.

Consider the notional time series $(\mathbf{x},\mathbf{y})$ extended to a library length of $L=25$.  This leads to the following plot:
%\begin{figure}[!H]
%\includegraphics[scale=0.55]{notionalTS.eps}
%\end{figure}

This figure implies
$$
\begin{array}{ccc}
x & & y\\
\uparrow &\mathrel{\ooalign{\hss$\nearrow$\hss\cr$\nwarrow$}} & \uparrow\\
dx &\rightarrow & dy
\end{array}
$$
Consider the sine time series with varying noise levels and histogram bins:
%\begin{figure}[!H]
%\includegraphics[scale=0.55]{SineTSxy.eps}
%\caption{Should be $X\rightarrow Y$; i.e.\ positive leaning (positive on colorbar)}
%\end{figure}
%\begin{figure}[!H]
%\includegraphics[scale=0.55]{SineTSxy_3C.eps}
%\caption{Should be $X\rightarrow Y$; i.e.\ positive leaning (positive on colorbar)}
%\end{figure}
%\begin{figure}[!H]
%\includegraphics[scale=0.55]{SineTSdxy.eps}
%\caption{Should be $dX\rightarrow Y$; i.e.\ positive leaning (positive on colorbar)}
%\end{figure}
%\begin{figure}[!H]
%\includegraphics[scale=0.55]{SineTSdxy_3C.eps}
%\caption{Should be $dX\rightarrow Y$; i.e.\ positive leaning (positive on colorbar)}
%\end{figure}
%\begin{figure}[!H]
%\includegraphics[scale=0.55]{SineTSxdy.eps}
%\caption{Should be $X\rightarrow dY$; i.e.\ positive leaning (positive on colorbar)}
%\end{figure}
%\begin{figure}[!H]
%\includegraphics[scale=0.55]{SineTSxdy_3C.eps}
%\caption{Should be $X\rightarrow dY$; i.e.\ positive leaning (positive on colorbar)}
%\end{figure}
%\begin{figure}[!H]
%\includegraphics[scale=0.55]{SineTSdxdy.eps}
%\caption{Should be $dX\rightarrow dY$; i.e.\ positive leaning (positive on colorbar)}
%\end{figure}
%\begin{figure}[!H]
%\includegraphics[scale=0.55]{SineTSdxdy_3C.eps}
%\caption{Should be $dX\rightarrow dY$; i.e.\ positive leaning (positive on colorbar)}
%\end{figure}

\newpage
\section{Example Time Series 1: Clean Impulse w/ Noisy Response}
Consider the time series $X\equiv\{x_t\}$ and $Y\equiv\{y_t=x_{t-1}+B\eta_t\}$ with $\eta_t\sim \mathcal{N}(0,1)$, $B\in[0,1]$, and $x_t = \kappa\;\forall t\in\mathbb{T}$ and $x_t = 0\;\forall t\notin\mathbb{T}$.  The $X$ peak value $\kappa$ and the set of times at which the peak values occur, $\mathbb{T}$, are assumed to not affect the causal inference between $X$ and $Y$ (assuming reasonable $\mathbb{T}$, e.g., assuming $\exists\;t\notin\mathbb{T}$).  The truth for this example time series is $X\rightarrow Y$.

The following set of ``leaning reports'' were generated a library length $L=20$, $B=0.3$, $\mathbb{T} = \{1,5,15\}$, and $\kappa = 2$; the full command was 
\begin{verbatim}
[x,y] = wTS(20,0.3,[1 5 15],[2 2 2]);
\end{verbatim}
The leaning reports differ only in the number of bins used in the leaning calculations.  The report commands are shown for clarity.

\begin{verbatim}
>> leaning_report(x,y,3,1e-6,[-2,-1,0,1,2])
-------------------------
Lagged Leanings:
  [lag,leaning] :: guess
  -2,0.2150162 :: x_{t+-2}->y
  -1,0.2649977 :: x_{t+-1}->y
  0,0.3138691 :: x_{t+0}->y
  1,0.1623932 :: x_{t+1}->y
  2,0.1846154 :: x_{t+2}->y
-------------------------
LIR Leanings:
  [ID,leaning] :: guess
  (x,y),0.3138691 :: x->y
  (dx,y),-0.2027365 :: y->dx
  (dx,dy),0.0150518 :: dx->dy
  (x,dy),0.4682899 :: x->dy
  (dx,x),-0.6050109 :: x->dx
  (dy,y),-0.1274689 :: y->dy
  (x&dx,y),-0.0168633 :: y->x&dx
  (y&dy,x),-0.2476780 :: x->y&dy
  (x&dx,y&dy),0.0034400 :: x&dx->y&dy
\end{verbatim} 
\begin{verbatim}
>> leaning_report(x,y,5,1e-6,[-2,-1,0,1,2])
-------------------------
Lagged Leanings:
  [lag,leaning] :: guess
  -2,0.6428571 :: x_{t+-2}->y
  -1,0.6269024 :: x_{t+-1}->y
  0,0.6431373 :: x_{t+0}->y
  1,0.5428571 :: x_{t+1}->y
  2,0.5743590 :: x_{t+2}->y
-------------------------
LIR Leanings:
  [ID,leaning] :: guess
  (x,y),0.6431373 :: x->y
  (dx,y),0.0603486 :: dx->y
  (dx,dy),-0.0329019 :: dy->dx
  (x,dy),0.2580981 :: x->dy
  (dx,x),-0.6050109 :: x->dx
  (dy,y),-0.0016151 :: y->dy
  (x&dx,y),-0.1065015 :: y->x&dx
  (y&dy,x),-0.1981424 :: x->y&dy
  (x&dx,y&dy),0.0995528 :: x&dx->y&dy
\end{verbatim} 
\begin{verbatim}
>> leaning_report(x,y,10,1e-6,[-2,-1,0,1,2])
-------------------------
Lagged Leanings:
  [lag,leaning] :: guess
  -2,0.4158482 :: x_{t+-2}->y
  -1,0.3853554 :: x_{t+-1}->y
  0,0.3980392 :: x_{t+0}->y
  1,0.3204365 :: x_{t+1}->y
  2,0.3225962 :: x_{t+2}->y
-------------------------
LIR Leanings:
  [ID,leaning] :: guess
  (x,y),0.3980392 :: x->y
  (dx,y),0.0008913 :: dx->y
  (dx,dy),-0.0010890 :: dy->dx
  (x,dy),0.1571900 :: x->dy
  (dx,x),-0.6050109 :: x->dx
  (dy,y),-0.0325069 :: y->dy
  (x&dx,y),-0.0548762 :: y->x&dx
  (y&dy,x),-0.2935443 :: x->y&dy
  (x&dx,y&dy),0.0510836 :: x&dx->y&dy
\end{verbatim} 
\begin{verbatim}
>> leaning_report(x,y,15,1e-6,[-2,-1,0,1,2])
-------------------------
Lagged Leanings:
  [lag,leaning] :: guess
  -2,0.3407955 :: x_{t+-2}->y
  -1,0.3331653 :: x_{t+-1}->y
  0,0.3445715 :: x_{t+0}->y
  1,0.2874359 :: x_{t+1}->y
  2,0.2793407 :: x_{t+2}->y
-------------------------
LIR Leanings:
  [ID,leaning] :: guess
  (x,y),0.3445715 :: x->y
  (dx,y),0.0389137 :: dx->y
  (dx,dy),0.0063290 :: dx->dy
  (x,dy),0.1098856 :: x->dy
  (dx,x),-0.6050109 :: x->dx
  (dy,y),-0.0129562 :: y->dy
  (x&dx,y),-0.0652997 :: y->x&dx
  (y&dy,x),-0.3002158 :: x->y&dy
  (x&dx,y&dy),0.0510836 :: x&dx->y&dy
\end{verbatim}
Consider the lagged leaning
\begin{verbatim}
x_{t+-1}
\end{verbatim}
which is 
\begin{eqnarray}
\Delta_{y_t x_{t-1}} &=& \langle \rho_{y_t x_{t-1}}\rangle - \langle \rho_{x_{t-1} y_t}\rangle\\
&=& \langle P(y_t| x_{t-1}) - P(y_t| x_{t^\prime\neq t-1})\rangle - \langle P(x_{t-1} |y_t)-P(x_{t-1} |y_{t^\prime\neq t})\rangle
\end{eqnarray}
%\begin{figure}[!H]
%\includegraphics[scale=0.55]{laggedleans_wTS.eps}
%\caption{Should be $x\rightarrow y$; i.e.\ positive leaning}
%\end{figure}

\newpage
%Consider 
%\begin{verbatim}
%[x,y] = wTS(20,0,[1 5 15],[2 2 2]);
%\end{verbatim}
%which is the clean impulse time series from above with no noise in the response, %i.e.\ $B=0$.  The penchants $\rho_{yx}$, $\rho_{xy}$ and the leaning $\Delta_{yx}$ %are all shown below.
%\begin{figure}[!H]
%\includegraphics[scale=0.55]{sTWpenIMG.eps}
%\end{figure}
%$$
%\rho_{yx} = \begin{array}{ccc}
%   -0.1765    &     0  & -0.6078\\
%         0    &     0  &      0\\
%    0.1765    &     0  & -0.1765
%\end{array}
$$
$$
%\rho_{xy} = \begin{array}{ccc}
%   -0.1765   &      0 &  -0.6078\\
%         0   &      0 &        0\\
%    0.1765   &      0 &  -0.1765
%\end{array}
%$$
%$$
%\Delta_{yx} = \begin{array}{ccc}
%         0    &     0  & -0.7843\\
%         0    &     0  &       0\\
%    0.7843    &     0  &       0
%\end{array}
%$$

%\begin{verbatim}
%[x,y] = wTS(20,0,[1 5 15],[2 2 2]);
%\end{verbatim}
%\begin{figure}[!H]
%\includegraphics[scale=0.55]{LIRimgwTS000.eps}
%\end{figure}
%\begin{verbatim}
%[x,y] = wTS(20,0.5,[1 5 15],[2 2 2]);
%\end{verbatim}
%\begin{figure}[!H]
%includegraphics[scale=0.55]{LIRimgwTS050.eps}
%\end{figure}

Consider
\begin{eqnarray*}
x &=& \left\{0,0,2,0,0,2,0,0,2,0\right\}\\
y &=& \left\{0,0,0,2,0,0,2,0,0,2\right\}
\end{eqnarray*}
and a penchant defined as
$$
\rho_{y_{t}x_{t-1}} = P\left( y_t = 2 | x_{t-1} = 2\right) - P\left( y_t = 2 | x_{t-1} \neq 2\right)
$$
Estimating the probabilities with frequency counts is straightforward for the first term, i.e.\
$$
P\left( y_t = 2 | x_{t-1} = 2\right) = \frac{n_{EC}}{n_C} = \frac{3}{3} = 1\;\;,
$$
where $n_{EC}$ is the number of times $y_t=2$ and $x_{t-1}=2$ appears in the pair of time series (where, in this case, $y_t$ is assumed to be the effect $E$ caused by the cause $C$, $x_{t-1}=2$) and $n_{C}$ is the number of times the assumed cause, $x_{t-1}=2$, has appeared.  The conditional probability $P(A|B)$ is not defined if $P(B) = 0$, thus $n_C >0$ by definition, and $n_{EC}<=n_C$ because even though the assumed effect, $y_t=2$, might occur without the assumed cause, $x_{t-1}=2$ in the data, the count $n_{EC}$ is defined to be when they occur together.  It follows that this definition of $P\left( y_t = 2 | x_{t-1} = 2\right)$ can be interpreted as a probability.  

The second term in the penchant is trickier.  Consider the following frequency count,
$$
P\left( y_t = 2 | x_{t-1} \neq 2\right) = \frac{n_{EnC}}{n_{nC}} = \frac{0}{6} = 0\;\;,
$$
where $n_{EnC}$ is the number of times the assumed effect, $y_t=2$ appears given that the assumed cause $x_{t-1}=2$ has {\em not} occurred, i.e.\ $x_{t-1}\neq 2$, and $n_{nC}$ is the number of times the assumed cause $x_{t-1}=2$ has {\em not} occurred, i.e.\ the number of times $x_{t-1}\neq 2$.  There are seven zeros in $\{x_t$\}, so it may be assumed that $n_{nC}=7$ rather than $n_{nC}=6$ as used above.  But, the condition is on $x_{t-1}$, not $x_t$.  Thus, the time series actually being compared are
\begin{eqnarray*}
\tilde{x} &=& \left\{0,0,2,0,0,2,0,0,2\right\}\\
\tilde{y} &=& \left\{0,0,2,0,0,2,0,0,2\right\}
\end{eqnarray*}
which are both shorter than there counterparts above by a single value.  It follows that $\tilde{x}_t = x_{t-1}$ and $\tilde{y}_t=y_t$.  This subsetting of the time series will be discussed again later.

Again, this definition can be interpreted as a probability (for very similar reason to those mentioned above).  However, this term can be rewritten using Bayes' theorem or the law of total probability as
$$
P\left( y_t = 2 | x_{t-1} \neq 2\right) = \frac{P\left( y_t = 2 \right)}{P\left( x_{t-1} \neq 2 \right)} - P\left( y_t = 2 | x_{t-1} = 2\right)\frac{P\left( x_{t-1} = 2 \right)}{P\left( x_{t-1} \neq 2 \right)}\;\;.
$$
Substituting the frequency counts leads to
$$
P\left( y_t = 2 | x_{t-1} \neq 2\right) = \frac{n_E}{n_{nC}} - \frac{n_{EC}}{n_C}\frac{n_C}{n_{nC}} = \frac{3}{6} - \frac{3}{3}\frac{3}{6} = 0\;\;,
$$
where the only frequency seen here but not used above is $n_E$ the number of times the assumed effect $y_t=2$ has appeared.  Also consider the completeness relation $P\left( x_{t-1} \neq 2 \right) = 1 - P\left( x_{t-1} = 2 \right)$.  This substitution leads to
$$
P\left( y_t = 2 | x_{t-1} \neq 2\right) = \frac{P\left( y_t = 2 \right)}{1-P\left( x_{t-1} = 2 \right)} - P\left( y_t = 2 | x_{t-1} = 2\right)\frac{P\left( x_{t-1} = 2 \right)}{1-P\left( x_{t-1} = 2 \right)}
$$
and
$$
P\left( y_t = 2 | x_{t-1} \neq 2\right) = \frac{n_E}{1-n_{C}} - \frac{n_{EC}}{n_C}\frac{n_C}{1-n_{C}} = \frac{3}{3} - \frac{3}{3}\frac{3}{3} = 0\;\;.
$$
The last expression has removed any reference to $P\left( x_{t-1} \neq 2 \right)$ and $n_{nC}$, which might be considered unobservable.  This particular penchant is the same for both calculation methods.

Consider both these calculation methods for all the possible penchants in this pair of time series, i.e.\
\begin{center}
\begin{tabular}{c|c|c|c|c|c}
$\rho_{y_{t}x_{t-1}}$ & $P(E|C)$ & $P(E|\bar{C})$ method 1 & $P(E|\bar{C})$ method 2 & result 1 & result 2\\
\hline
 $y_{t}=2,x_{t-1}=2$& 1 & 0 & 0 & 1 & 1\\
 $y_{t}=2,x_{t-1}=0$& 0 & 1 & 1 & -1 & -1\\
 $y_{t}=0,x_{t-1}=2$& 0 & 1 & 1 & -1 & -1\\
 $y_{t}=0,x_{t-1}=0$& 1 & 0 & 0 & 1 & 1
\end{tabular}
\end{center}
For this example, both methods are equal for all the penchants.  The main concern here is that the second conditional probability term in the definition of the penchant has been replaced with probabilities of the assumed causes and effects.  This substitution addresses the argument that this second conditional probability term is not observable (e.g.\ Pearl's argument that there's no way to know what an effect given ``no cause'' actually is supposed to mean).  However, it introduces concerns about the underlying assumption that the assumed cause must precede the assumed effect.  This concern is partially addressed by subsetting $\{x\}$ and $\{y\}$ into $\{\tilde{x}\}$ and $\{\tilde{y}\}$ because for any given $t$, $\tilde{x}_t$ precedes $\tilde{y}_t$.  But, once is this assumptions still true when frequency counts are used to estimate the necessary probabilities; e.g.\ when $P(x_{t-1}=0)$ and $P(y_t=0)$ are estimated above, is it sufficient to use   
$$
P(x_{t-1}=0) = \frac{n_{x0}}{L}
$$ 
and 
$$
P(y_t=0) = \frac{n_{y0}}{L}\;\;,
$$
where $n_{x0}$ is the number of times $\tilde{x}_t = 0$, $n_{y0}$ is the number of times $\tilde{y}_t = 0$, and $L$ is the library length of both $\{\tilde{x}\}$ and $\{\tilde{y}\}$?  The working assumption is yes, but the issue will be addressed primarily by testing the algorithm.

The algorithm used to calculate the penchants calculates only $\rho_{y_{t}=2,x_{t-1}=2}$ and $\rho_{y_{t}=0,x_{t-1}=0}$ but not $\rho_{y_{t}=2,x_{t-1}=0}$ and $\rho_{y_{t}=0,x_{t-1}=2}$ because the former are the only $(y_{t},x_{t-1})$ pairs that actually appear in the data.  The observed mean penchant is then 
$$
\bar{\rho_{y_{t},x_{t-1}}} = \frac{1}{2}\left( \rho_{y_{t}=2,x_{t-1}=2} + \rho_{y_{t}=0,x_{t-1}=0} \right) = 1\;\;,
$$
which implies the probability that $x_{t-1}$ drives $y_{t}$ is higher than the probability that it does not.  The observed mean leaning is then
$$
\bar{l}_{xy} = \bar{\rho_{y_{t},x_{t-1}}} - \bar{\rho_{x_{t},y_{t-1}}}\;\;,
$$
and the casual inference would be $\bar{l}>0\Rightarrow \mathbf{x}\rightarrow\mathbf{y}$, $\bar{l}<0\Rightarrow \mathbf{y}\rightarrow\mathbf{x}$, and $\bar{l}=0\Rightarrow ??$; i.e.\ a positive observed mean leaning implies $\mathbf{x}$ drives $\mathbf{y}$ more than $\mathbf{y}$ drives $\mathbf{x}$ with an analogous conclusion for a negative observed mean leaning.   

The following report was generating by testing the observed mean leaning against all of the casual pair data sets from [INSERT REF]:
\begin{verbose}
Test 1: 29/88 (0.3295) correct [total]; 9/22 (0.4091) correct [TS];
Test 2: 30/88 (0.3409) correct [total]; 8/22 (0.3636) correct [TS];
Test 3: 32/88 (0.3636) correct [total]; 9/22 (0.4091) correct [TS];
Test 4: 30/88 (0.3409) correct [total]; 8/22 (0.3636) correct [TS];
Test 5: 30/88 (0.3409) correct [total]; 8/22 (0.3636) correct [TS];
Test 6: 31/88 (0.3523) correct [total]; 9/22 (0.4091) correct [TS];
Test 7: 31/88 (0.3523) correct [total]; 9/22 (0.4091) correct [TS];
Test 8: 31/88 (0.3523) correct [total]; 9/22 (0.4091) correct [TS];
Test 9: 31/88 (0.3523) correct [total]; 9/22 (0.4091) correct [TS];
Test 10: 31/88 (0.3523) correct [total]; 9/22 (0.4091) correct [TS];
\end{verbose}



\end{document}

