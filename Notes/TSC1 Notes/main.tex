\documentclass[a4paper,11pt]{article}

\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[margin=0.5in]{geometry}
\usepackage{caption}
\usepackage{subcaption}


\begin{document}

{\Huge TSC1 Notes}

\hfill\rule{150mm}{.1pt}

\hfill{\small \today}

\section{Simple Linear Example}
Consider a simple driver-response system:
\begin{eqnarray*}
x_t &=& \sin(t)\\
y_t &=& x_{t-1} + \eta_t\\
&=& \sin(t-1) + \eta_t\\
&=& \sin(t)\cos(1)-\cos(t)\sin(1)+ \eta_t
\end{eqnarray*}
with $\eta_t\sim \mathcal{N}(0,1)$.  Define
$$
\delta x_t \equiv \frac{dx}{dt} \approx \frac{\Delta x}{\Delta t} = x_t-x_{t-1}
$$
and
$$
\delta y_t \equiv \frac{dy}{dt} \approx \frac{\Delta y}{\Delta t} = y_t-y_{t-1}\;\;.
$$
It follows that
\begin{eqnarray*}
\delta x_t &=& \sin(t)-\sin(t-1)\\
&=& \sin(t)-\left(\sin(t)\cos(1)-\cos(t)\sin(1)\right)\\
&=& \sin(t)\left(1-\cos(1)\right)+\sin(1)\cos(t)\\
&\equiv& \kappa_1\sin(t)+\kappa_2\cos(t)
\end{eqnarray*}
with $\kappa_1 = 1-\cos(1)$ and $\kappa_2 = \sin(1)$, and
\begin{eqnarray*}
\delta y_t &=& x_{t-1} + \eta_t - x_{t-2} - \eta_{t-1}\\
&=& \sin(t-1) - \sin(t-2) + \eta_t - \eta_{t-1}\\
&=& \left(\sin(t)\cos(1)-\cos(t)\sin(1)\right) - \left(\sin(t)\cos(2)-\cos(t)\sin(2)\right)+\eta^\prime_t \\
&=& \sin(t)\left(\cos(1)-\cos(2)\right)+\cos(t)\left(\sin(2)-\sin(1)\right)+\eta^\prime_t\\
&\equiv& k_1\sin(t)+k_2\cos(t)+\eta^\prime_t
\end{eqnarray*}
with $\eta^\prime_t = \eta_t - \eta_{t-1} \sim \mathcal{N}(0,2)$\footnote{The difference of two normal distributions with means $\mu_1$ and $\mu_2$ and variances $\sigma^2_1$ and $\sigma^2_2$ is another normal distribution with mean $\mu_1-\mu_2$ and variance $\sigma^2_1+\sigma^2_2$.}, $k_1 = \left(\cos(1)-\cos(2)\right)$ and $k_2 = \left(\sin(2)-\sin(1)\right)$.

The main idea of Local Impulse Response (LIR) causality inference is to use a subsetting procedure on some (or all) of the four time series $x_t$, $y_t$, $\delta x_t$, and $\delta y_t$ to determine the causality of the system.

Consider a few extreme points in the driver cycle, e.g.\ $t=n\pi$ with $n=0,1,2,3,4,\ldots$.  The driver values are
$$
x_{n\pi} = \sin(n\pi) = 0\;\;,
$$
the response values are
$$
y_{n\pi} = \sin(n\pi)\cos(1)-\cos(n\pi)\sin(1)+ \eta_{n\pi} = \eta_{n\pi} + \left(-1\right)^n\sin(1)\;\;,
$$
the local change in the driver values are
$$
\delta x_{n\pi} = \kappa_1\sin(n\pi)+\kappa_2\cos(n\pi) = \left(-1\right)^n \kappa_2\;\;,
$$
and the local change in the response values are
$$
\delta y_{n\pi} = k_1\sin(n\pi)+k_2\cos(n\pi)+\eta^\prime_{n\pi} = \eta^\prime_{n\pi} + \left(-1\right)^n k_2\;\;.
$$

Consider $t=n\pi/2$ with $n=1,2,3,4,\ldots$.  The driver values are
$$
x_{n\frac{\pi}{2}} = \sin\left(n\frac{\pi}{2}\right) = (-1)^n\;\;,
$$
the response values are
$$
y_{n\frac{\pi}{2}} = \sin\left(n\frac{\pi}{2}\right)\cos(1)-\cos\left(n\frac{\pi}{2}\right)\sin(1)+ \eta_{n\frac{\pi}{2}} = \eta_{n\frac{\pi}{2}} + (-1)^n\cos(1)\;\;,
$$
the local change in thd driver values are
$$
\delta x_{n\frac{\pi}{2}} = \kappa_1\sin\left(n\frac{\pi}{2}\right)+\kappa_2\cos\left(n\frac{\pi}{2}\right) = \left(-1\right)^n \kappa_1\;\;,
$$
and the local change in the response values are
$$
\delta y_{n\frac{\pi}{2}} = k_1\sin\left(n\frac{\pi}{2}\right)+k_2\cos\left(n\frac{\pi}{2}\right)+\eta^\prime_{n\frac{\pi}{2}} = \eta^\prime_{n\frac{\pi}{2}} + \left(-1\right)^n k_1\;\;.
$$

\section{Subsetting for LIR variance}
Consider $\mathbf{X}=\{x_t\}$ and $\mathbf{Y}=\{y_t\}$ given $t\in[0,4\pi]$.  Let $L$ be the library length of $\mathbf{X}$ and $\mathbf{Y}$.  These systems have five points where $t=n\pi$ for $n=0,1,2,3$ and $4$. Thus, an $m$-binned histogram of $\mathbf{X}$, where $m\ge L$, would have a bin, $b_0$, centered at $x_t=0$ that contains the five points $\mathbf{X}_{0} = \{x_{n\pi}\}$.  (If $m<L$, then $\mathbf{b}_0$ would contain at least five points but the total number of points in $\mathbf{b}_0$ would be a function of the total number of bins, assuming bins of equal sizes.)  

Consider the set of time steps $\mathbf{T}=\{t=n\pi\}\;\forall n=0,1,2,3,4$ for which the values in $\mathbf{X_0}$ are achieved.  The local impulses immediately preceding $\mathbf{b}_0$ are $\mathbf{\delta X}_\mathbf{T}=\{\delta x_{n\pi}\}$, which also contains five points.  However, those five points would not appear in a single bin of an m-binned histogram of $\{\delta x_t\}$ (given $m\ge L$).  The set $\mathbf{\delta X}_\mathbf{T}$ would actually be split into two separate bins in such a histogram, one for the three points equal to $\kappa_2$ and one for the two points equal to $-\kappa_2$.  Thus, the time steps associated all of the points in a given histogram bin of a given time series, e.g.\ $\mathbf{b}_0$, do not necessarily correspond to points in a single histogram bin of different (though related) times series, e.g.\ $\mathbf{\delta X}_\mathbf{T}$.  This idea is straightforward but it is the basic idea underlying the subsetting method for calculating the LIR variance.

The subsetting is premised on the following:
\begin{itemize}
\item The local temporal response causally depends on the local temporal change in the driver; e.g.\ $y_{t}$ causally depends on $\delta x_{t}$
\item The local temporal response causally depends on the immediately preceding response; e.g.\ $y_{t}$ causally depends on $y_{t-1}$
\item The local temporal response does not causally depend on the immediately preceding driver except through the local temporal change in the driver; e.g.\ $y_{t}$ does not causally depend on $x_{t-1}$ except through $\delta x_t$
\end{itemize}
Thus, the subsetting procedure is as follows:
\begin{enumerate}
\item Create an $m$-binned histogram of the response signal $\mathbf{R} = \{r_t\}$.
\item Given $m$ bins $\mathbf{b}_i$ where $i$ denotes the center of the $m$th bin, create an $m^\prime$-binned histogram of the change in the driver signal $\mathbf{\delta D} = =\{\delta d_t\} = \{d_t-d_{t-1}\}$ at the time steps $\mathbf{\tau} = \{t\;|\;r_t\in \mathbf{b}_i\}$.
\item Given $m^\prime$ bins $\mathbf{b^\prime}_j$ where $j$ denotes the center of the $m^\prime$th bin, find the variance of the response at the time steps immediately following (i.e.\ $t+1$) the time steps $\mathbf{\tau^\prime} = \{t\;|\;\delta d_t\in \mathbf{b}_j\}$.
\end{enumerate}
As an example, consider $\mathbf{R} = \mathbf{X}$ and $\mathbf{D} = \mathbf{Y}$.  An $m$-binned histogram of $\mathbf{X}$ would lead to a bin centered at zero, $b_0$, that contains at least five points evaluated at $t=n\pi\;\forall n=0,1,2,3,4$.  Thus, the change in the driver signal, i.e.\ $\{\delta y_t\}$, evaluated at $\mathbf{\tau}$ contains at least the five points $\eta^\prime_{n\pi} + \left(-1\right)^n k_2$.  The $m^\prime$-binned histogram of step 2 would split these five points among different bins (depending on both the sign of $k_2$ and the value of $\eta^\prime_{n\pi}$ for a given $n$).  If each of the points is placed into a bin alone, then the variance calculations of step 3 become $\mathop{var}\left(\sin(n\pi+1)\right)\;\forall n=0,1,2,3,4$, which is five zeros because the variance of a single point is zero.  Suppose all five points are placed into a single bin.  The variance calculation of step 3 then becomes 
\begin{eqnarray}
\mathop{var}\left(\{\sin(n\pi+1)\; | n=0,1,2,3,4\}\right) &=& \mathop{var}\left(\{\sin(n\pi)\cos(1)+\cos(n\pi)\sin(1)\; | n=0,1,2,3,4\}\right)\\
&=& \mathop{var}\left(\{(-1)^n\kappa_2\; | n=0,1,2,3,4\}\right)\\
&=& \mathop{var}\left(\{\kappa_2,-\kappa_2,\kappa_2,-\kappa_2,\kappa_2\}\right)\\
&=& \frac{1}{5}\left((\kappa_2-\mu)^2+(-\kappa_2-\mu)^2+(\kappa_2-\mu)^2+(-\kappa_2-\mu)^2+(\kappa_2-\mu)^2\right)\\
&=& \frac{1}{5}\left(\frac{16}{25}\kappa_2^2+\frac{36}{25}\kappa_2^2+\frac{16}{25}\kappa_2^2+\frac{36}{25}\kappa_2^2+\frac{16}{25}\kappa_2^2\right)\\
&=& \frac{1}{5}\frac{120}{25}\kappa_2^2\\
&=& \frac{24}{25}\kappa_2^2\
\end{eqnarray}
where $\mu=\kappa_2/5$.  Thus, the LIR variance depends strongly on the number of bins used to contruct the histograms in steps 1 and 2.  

In this particular example, $\mathbf{Y}$ is known to be the response and $\mathbf{X}$ is known to be the driver.  It may be assumed that our assignment of $\mathbf{R} = \mathbf{X}$ and $\mathbf{D} = \mathbf{Y}$ may proven ``false'' by comparing the variances given this assignment and it's complement, i.e.\ $\mathop{LIRvar}|\mathbf{R} = \mathbf{X},\mathbf{D} = \mathbf{Y}$ and $\mathop{LIRvar}|\mathbf{R} = \mathbf{Y},\mathbf{D} = \mathbf{X}$.  It may be assumed that the lower LIR variance is indicative of a stronger causal inference, i.e.\ if $\mathop{LIRvar}|\mathbf{R} = \mathbf{X},\mathbf{D} = \mathbf{Y}$ is greater than $\mathop{LIRvar}|\mathbf{R} = \mathbf{Y},\mathbf{D} = \mathbf{X}$, then it may be assumed $\mathbf{R} = \mathbf{Y},\mathbf{D} = \mathbf{X}$ is the more ``correct'' assignment.  Notice, however, that we have already shown that the incorrect assignment of $\mathbf{R} = \mathbf{X}$ and $\mathbf{D} = \mathbf{Y}$ can lead to an LIR variance of variance.  The correct assignment of $\mathbf{R} = \mathbf{Y}$ and $\mathbf{D} = \mathbf{X}$ cannot lead to an LIR variance less than zero (variances are nonnegative).  Thus, it seems that comparing LIR variances is not a robust method for causal inference.  

\section{LIR Approach to Probabilistic Causality}
Probabilistic causality is centered on the definition that a cause $C$ is said to {\em cause} (or {\em drive}) an effect $E$ if
$$
P\left(E|C\right) > P\left(E|\bar{C}\right)\;\;,
$$
i.e.\ $C$ causes $E$ if the probability of $E$ given $C$ is higher than the probability of $E$ given not $C$.  The LIR causal inference method involves using e.g.\ $\{x_t\}$, $\{y_t\}$, $\{\delta x_t\}$, and $\{\delta y_t\}$ to determine the direction of causal influence in a system of two times series $\{x_t\}$ and $\{y_t\}$.  It follows that applying LIR causal inference to probabilistic causality involves evaluating the above inequality given e.g.\ $C = \{x_t\}$, $\{y_t\}$, $\{\delta x_t\}$, or $\{\delta y_t\}$ and $E = \{x_t\}$, $\{y_t\}$, $\{\delta x_t\}$, or $\{\delta y_t\}$ given $E\neq C$ and for different temporal offsets.

The conditional probabilities are estimated using histograms of the time series data as follows:
$$
P\left(E|C\right) \approx \frac{1}{L} H\left(E|C\right) = \frac{H\left(E\cap C\right)}{H(C)}
$$
where $H(A)$ is an $m$-binned histogram of $A$, $L$ is the library length of the $E$ and $C$ time series (which are assumed to be the same length).  Similarly,
$$
P\left(E|\bar{C}\right) \approx \frac{1}{L} H\left(E|\bar{C}\right) = \frac{H\left(E\cap \bar{C}\right)}{H(\bar{C})}\;\;.
$$

Define the {\em causal penchant} 
$$
\rho_{EC} = P\left(E|C\right) - P\left(E|\bar{C}\right) \approx \frac{H\left(E\cap C\right)}{H(C)}-\frac{H\left(E\cap \bar{C}\right)}{H(\bar{C})}\;\;.
$$
If $C$ causes $E$, then $\rho_{EC} > 0$.  Otherwise, i.e.\ $\rho_{EC} \le 0$, the causal influence of the system is {\em undefined}(?).  Causal influence between a pair of time series is accomplished by comparing penchants.

Consider two short time series $\mathbf{x}=\{x_t\} = \{0,1,2,1,0\}$ and $\mathbf{y}=y_t=x_{t-1}$ (i.e.\ $\{y_t\} = \{0,0,1,2,1\}$.  The time steps are indexed by $t=0,1,2,3,4$.  The individual joint probabilities can be found using the frequencies of occurrence as
\begin{eqnarray}
P(y_t=0 \cap x_t = 0) &=& \frac{1}{5}\\
P(y_t=1 \cap x_t = 0) &=& \frac{1}{5}\\
P(y_t=2 \cap x_t = 0) &=& 0\\
P(y_t=0 \cap x_t = 1) &=& \frac{1}{5}\\
P(y_t=1 \cap x_t = 1) &=& 0\\
P(y_t=2 \cap x_t = 1) &=& \frac{1}{5}\\
P(y_t=0 \cap x_t = 2) &=& 0\\
P(y_t=1 \cap x_t = 2) &=& \frac{1}{5}\\
P(y_t=2 \cap x_t = 2) &=& 0
\end{eqnarray}
The individual probabilities are found similarly:
\begin{eqnarray}
P(x_t = 0) &=& \frac{2}{5}\\
P(x_t = 1) &=& \frac{2}{5}\\
P(x_t = 2) &=& \frac{1}{5}\\
P(y_t = 0) &=& \frac{2}{5}\\
P(y_t = 1) &=& \frac{2}{5}\\
P(y_t = 2) &=& \frac{1}{5}
\end{eqnarray}
The joint probabilities are symmetric (i.e.\ $P(x_t\cap y_t) = P(y_t\cap x_t)$).  Thus, the two above sets of probabilities are sufficient to find all the conditionals.  
\begin{center}
\begin{tabular}{c|c|c}
$P(y_t=0 | x_t = 0) = \frac{P(y_t=0 \cap x_t = 0)}{P(x_t = 0)} = \frac{1}{2}$ &
$P(y_t=0 | x_t = 1) = \frac{P(y_t=0 \cap x_t = 1)}{P(x_t = 1)} = \frac{1}{2}$ &
$P(y_t=0 | x_t = 2) = \frac{P(y_t=0 \cap x_t = 2)}{P(x_t = 2)} = 0$ \\

$P(y_t=1 | x_t = 0) = \frac{P(y_t=1 \cap x_t = 0)}{P(x_t = 0)} = \frac{1}{2}$ &
$P(y_t=1 | x_t = 1) = \frac{P(y_t=1 \cap x_t = 1)}{P(x_t = 1)} = 0$ &
$P(y_t=1 | x_t = 2) = \frac{P(y_t=1 \cap x_t = 2)}{P(x_t = 2)} = 1$ \\

$P(y_t=2 | x_t = 0) = \frac{P(y_t=2 \cap x_t = 0)}{P(x_t = 0)} = 0$ &
$P(y_t=2 | x_t = 1) = \frac{P(y_t=2 \cap x_t = 1)}{P(x_t = 1)} = \frac{1}{2}$ &
$P(y_t=2 | x_t = 2) = \frac{P(y_t=2 \cap x_t = 2)}{P(x_t = 2)} = 0$ \\
\hline \\
$P(x_t=0 | y_t = 0) = \frac{P(y_t=0 \cap x_t = 0)}{P(y_t = 0)} = \frac{1}{2}$ &
$P(x_t=0 | y_t = 1) = \frac{P(y_t=1 \cap x_t = 0)}{P(y_t = 1)} = \frac{1}{2}$ &
$P(x_t=0 | y_t = 2) = \frac{P(y_t=2 \cap x_t = 0)}{P(y_t = 2)} = 0$ \\

$P(x_t=1 | y_t = 0) = \frac{P(y_t=0 \cap x_t = 1)}{P(y_t = 0)} = \frac{1}{2}$ &
$P(x_t=1 | y_t = 1) = \frac{P(y_t=1 \cap x_t = 1)}{P(y_t = 1)} = 0$ &
$P(x_t=1 | y_t = 2) = \frac{P(y_t=2 \cap x_t = 1)}{P(y_t = 2)} = 1$ \\

$P(x_t=2 | y_t = 0) = \frac{P(y_t=0 \cap x_t = 2)}{P(y_t = 0)} = 0$ &
$P(x_t=2 | y_t = 1) = \frac{P(y_t=1 \cap x_t = 2)}{P(y_t = 1)} = \frac{1}{2}$ &
$P(x_t=2 | y_t = 2) = \frac{P(y_t=2 \cap x_t = 2)}{P(y_t = 2)} = 0$ \\
\end{tabular}
\end{center}

The next step is to investigate the difference series: $\mathbf{\delta x} = \delta x_t = x_t - x_{t-1} = \{0,1,1,-1,-1\}$ and $\mathbf{\delta y} =\delta y_t = y_t - y_{t-1} = \{0,0,1,1,-1\}$.  There are now four time series, $\mathbf{x}$, $\mathbf{y}$, $\mathbf{\delta x}$, and $\mathbf{\delta y}$, with six permutations to investigate, $(\mathbf{x},\mathbf{y})$, $(\mathbf{x},\mathbf{\delta y})$, $(\mathbf{\delta x},\mathbf{y})$, $(\mathbf{\delta x},\mathbf{\delta y})$, $(\mathbf{x},\mathbf{\delta x})$, and $(\mathbf{y},\mathbf{\delta y})$.  It has already been determined that no conclusions can be drawn about $(\mathbf{x},\mathbf{y})$.  The following table is for reference in the counting exercises below:
\begin{center}
\begin{tabular}{c|c|c|c|c}
t & $\mathbf{x}$ & $\mathbf{y}$ & $\mathbf{\delta x}$ & $\mathbf{\delta y}$ \\
\hline 
0 & 0 & 0 & 0 & 0\\
1 & 1 & 0 & 1 & 0\\
2 & 2 & 1 & 1 & 1\\
3 & 1 & 2 & -1 & 1\\
4 & 0 & 1 & -1 & -1
\end{tabular}
\end{center}

Consider the three permutations: $(\mathbf{x},\mathbf{\delta x})$, $(\mathbf{\delta x},\mathbf{y})$, and $(\mathbf{\delta x},\mathbf{\delta y})$.  The joint probabilities are as follows:
\begin{center}
\begin{tabular}{c|c|c}
$P(x_t=0 \cap \delta x_t = 0) = \frac{1}{5}$&$P(y_t=0 \cap \delta x_t = 0) = \frac{1}{5}$&$P(\delta y_t=0 \cap \delta x_t = 0) = \frac{1}{5}$\\
$P(x_t=1 \cap \delta x_t = 0) = 0$&$P(y_t=1 \cap \delta x_t = 0) = 0$&$P(\delta y_t=1 \cap \delta x_t = 0) = 0$\\
$P(x_t=2 \cap \delta x_t = 0) = 0$&$P(y_t=2 \cap \delta x_t = 0) = 0$&$P(\delta y_t=-1 \cap \delta x_t = 0) = 0$\\
$P(x_t=0 \cap \delta x_t = 1) = 0$&$P(y_t=0 \cap \delta x_t = 1) = \frac{1}{5}$&$P(\delta y_t=0 \cap \delta x_t = 1) = \frac{1}{5}$\\
$P(x_t=1 \cap \delta x_t = 1) = \frac{1}{5}$&$P(y_t=1 \cap \delta x_t = 1) = \frac{1}{5}$&$P(\delta y_t=1 \cap \delta x_t = 1) = \frac{1}{5}$\\
$P(x_t=2 \cap \delta x_t = 1) = \frac{1}{5}$&$P(y_t=2 \cap \delta x_t = 1) = 0$&$P(\delta y_t=-1 \cap \delta x_t = 1) =0 $\\
$P(x_t=0 \cap \delta x_t = -1) = \frac{1}{5}$&$P(y_t=0 \cap \delta x_t = -1) = 0$&$P(\delta y_t=0 \cap \delta x_t = -1) = 0$\\
$P(x_t=1 \cap \delta x_t = -1) = \frac{1}{5}$&$P(y_t=1 \cap \delta x_t = -1) = \frac{1}{5}$&$P(\delta y_t=1 \cap \delta x_t = -1) = \frac{1}{5}$\\
$P(x_t=2 \cap \delta x_t = -1) = 0$&$P(y_t=2 \cap \delta x_t = -1) = \frac{1}{5}$&$P(\delta y_t=-1 \cap \delta x_t = -1) = \frac{1}{5}$\\
\end{tabular}
\end{center}
The last two permutations, $(\mathbf{x},\mathbf{\delta y})$ and $(\mathbf{y},\mathbf{\delta y})$, lead to the following table:
\begin{center}
\begin{tabular}{c|c}
$P(x_t=0 \cap \delta y_t = 0) = \frac{1}{5}$&$P(y_t=0 \cap \delta y_t = 0) = \frac{2}{5}$\\
$P(x_t=1 \cap \delta y_t = 0) = \frac{1}{5}$&$P(y_t=1 \cap \delta y_t = 0) = 0$\\
$P(x_t=2 \cap \delta y_t = 0) = 0$&$P(y_t=2 \cap \delta y_t = 0) = 0$\\
$P(x_t=0 \cap \delta y_t = 1) = 0$&$P(y_t=0 \cap \delta y_t = 1) = 0$\\
$P(x_t=1 \cap \delta y_t = 1) = \frac{1}{5}$&$P(y_t=1 \cap \delta y_t = 1) = \frac{1}{5}$\\
$P(x_t=2 \cap \delta y_t = 1) = \frac{1}{5}$&$P(y_t=2 \cap \delta y_t = 1) = \frac{1}{5}$\\
$P(x_t=0 \cap \delta y_t = -1) = \frac{1}{5}$&$P(y_t=0 \cap \delta y_t = -1) = 0$\\
$P(x_t=1 \cap \delta y_t = -1) = 0$&$P(y_t=1 \cap \delta y_t = -1) = \frac{1}{5}$\\
$P(x_t=2 \cap \delta y_t = -1) = 0$&$P(y_t=2 \cap \delta y_t = -1) =0$\\
\end{tabular}
\end{center}
The individual probabilities are (the ones printed above have been reprinted for convenience)
\begin{center}
\begin{tabular}{c|c}
$P(x_t = 0) = \frac{2}{5}$&$P(\delta x_t = 0) = \frac{1}{5}$\\
$P(x_t = 1) = \frac{2}{5}$&$P(\delta x_t = 1) = \frac{2}{5}$\\
$P(x_t = 2) = \frac{1}{5}$&$P(\delta x_t = -1) = \frac{2}{5}$\\
\hline
$P(y_t = 0) = \frac{2}{5}$&$P(\delta y_t = 0) = \frac{2}{5}$\\
$P(y_t = 1) = \frac{2}{5}$&$P(\delta y_t = 1) = \frac{2}{5}$\\
$P(y_t = 2) = \frac{1}{5}$&$P(\delta y_t = -1) = \frac{1}{5}$
\end{tabular}
\end{center}

Consider the penchants for $(\mathbf{x},\mathbf{\delta x})$.  The conditional probability table is
\begin{center}
\begin{tabular}{c|c|c}
$P(x_t=0 | \delta x_t = 0) = 1$ &
$P(x_t=0 | \delta x_t = 1) = 0$ &
$P(x_t=0 | \delta x_t = -1) = \frac{1}{2}$ \\

$P(x_t=1 | \delta x_t = 0) = 0$ &
$P(x_t=1 | \delta x_t = 1) = \frac{1}{2}$ &
$P(x_t=1 | \delta x_t = -1) = \frac{1}{2}$ \\

$P(x_t=2 | \delta x_t = 0) = 0$ &
$P(x_t=2 | \delta x_t = 1) = \frac{1}{2}$ &
$P(x_t=2 | \delta x_t = -1) = 0$ \\
\hline \\
$P(\delta x_t=0 | x_t = 0) = \frac{1}{2}$ &
$P(\delta x_t=0 | x_t = 1) = 0$ &
$P(\delta x_t=0 | x_t = 2) = 0$ \\

$P(\delta x_t=1 | x_t = 0) = 0$ &
$P(\delta x_t=1 | x_t = 1) = \frac{1}{2}$ &
$P(\delta x_t=1 | x_t = 2) = 1$ \\

$P(\delta x_t=-1 | x_t = 0) = \frac{1}{2}$ &
$P(\delta x_t=-1 | x_t = 1) = \frac{1}{2}$ &
$P(\delta x_t=-1 | x_t = 2) = 0$ \\
\end{tabular}
\end{center}

Consider the penchants for $(\mathbf{\delta x},\mathbf{y})$.  The conditional probability table is
\begin{center}
\begin{tabular}{c|c|c}
$P(\delta x_t=0 | y_t = 0) = \frac{1}{2}$ &
$P(\delta x_t=0 | y_t = 1) = 0$ &
$P(\delta x_t=0 | y_t = 2) = 0$ \\

$P(\delta x_t=1 | y_t = 0) = \frac{1}{2}$ &
$P(\delta x_t=1 | y_t = 1) = \frac{1}{2}$ &
$P(\delta x_t=1 | y_t = 2) = 0$ \\

$P(\delta x_t=-1 | y_t = 0) = 0$ &
$P(\delta x_t=-1 | y_t = 1) = \frac{1}{2}$ &
$P(\delta x_t=-1 | y_t = 2) = 1$ \\
\hline \\
$P(y_t=0 | \delta x_t = 0) = 1$ &
$P(y_t=0 | \delta x_t = 1) = \frac{1}{2}$ &
$P(y_t=0 | \delta x_t = -1) = 0$ \\

$P(y_t=1 | \delta x_t = 0) = 0$ &
$P(y_t=1 | \delta x_t = 1) = \frac{1}{2}$ &
$P(y_t=1 | \delta x_t = -1) = \frac{1}{2}$ \\

$P(y_t=2 | \delta x_t = 0) = 0$ &
$P(y_t=2 | \delta x_t = 1) = 0$ &
$P(y_t=2 | \delta x_t = -1) = \frac{1}{2}$ \\
\end{tabular}
\end{center}

Consider the penchants for $(\mathbf{\delta x},\mathbf{\delta y})$.  The conditional probability table is
\begin{center}
\begin{tabular}{c|c|c}
$P(\delta x_t=0 | \delta y_t = 0) = \frac{1}{2}$ &
$P(\delta x_t=0 | \delta y_t = 1) = 0$ &
$P(\delta x_t=0 | \delta y_t = -1) = 0$ \\

$P(\delta x_t=1 | \delta y_t = 0) = \frac{1}{2}$ &
$P(\delta x_t=1 | \delta y_t = 1) = \frac{1}{2}$ &
$P(\delta x_t=1 | \delta y_t = -1) = 0$ \\

$P(\delta x_t=-1 | \delta y_t = 0) = 0$ &
$P(\delta x_t=-1 | \delta y_t = 1) = \frac{1}{2}$ &
$P(\delta x_t=-1 | \delta y_t = -1) = 1$ \\
\hline \\
$P(\delta y_t=0 | \delta x_t = 0) = 1$ &
$P(\delta y_t=0 | \delta x_t = 1) = \frac{1}{2}$ &
$P(\delta y_t=0 | \delta x_t = -1) = 0$ \\

$P(\delta y_t=1 | \delta x_t = 0) = 0$ &
$P(\delta y_t=1 | \delta x_t = 1) = \frac{1}{2}$ &
$P(\delta y_t=1 | \delta x_t = -1) = \frac{1}{2}$ \\

$P(\delta y_t=-1 | \delta x_t = 0) = 0$ &
$P(\delta y_t=-1 | \delta x_t = 1) = 0$ &
$P(\delta y_t=-1 | \delta x_t = -1) = \frac{1}{2}$ \\
\end{tabular}
\end{center}

Consider the penchants for $(\mathbf{ x},\mathbf{\delta y})$.  The conditional probability table is
\begin{center}
\begin{tabular}{c|c|c}
$P(x_t=0 | \delta y_t = 0) = \frac{1}{2}$ &
$P(x_t=0 | \delta y_t = 1) = 0$ &
$P(x_t=0 | \delta y_t = -1) = 1$ \\

$P(x_t=1 | \delta y_t = 0) = \frac{1}{2}$ &
$P(x_t=1 | \delta y_t = 1) = \frac{1}{2}$ &
$P(x_t=1 | \delta y_t = -1) = 0$ \\

$P(x_t=2 | \delta y_t = 0) = 0$ &
$P(x_t=2 | \delta y_t = 1) = \frac{1}{2}$ &
$P(x_t=2 | \delta y_t = -1) = 0$ \\
\hline \\
$P(\delta y_t=0 | x_t = 0) = \frac{1}{2}$ &
$P(\delta y_t=0 | x_t = 1) = \frac{1}{2}$ &
$P(\delta y_t=0 | x_t = 2) = 0$ \\

$P(\delta y_t=1 | x_t = 0) = 0$ &
$P(\delta y_t=1 | x_t = 1) = \frac{1}{2}$ &
$P(\delta y_t=1 | x_t = 2) = 1$ \\

$P(\delta y_t=-1 | x_t = 0) = \frac{1}{2}$ &
$P(\delta y_t=-1 | x_t = 1) = 0$ &
$P(\delta y_t=-1 | x_t = 2) = 0$ \\
\end{tabular}
\end{center}

Consider the penchants for $(\mathbf{ y},\mathbf{\delta y})$.  The conditional probability table is
\begin{center}
\begin{tabular}{c|c|c}
$P(y_t=0 | \delta y_t = 0) = 1$ &
$P(y_t=0 | \delta y_t = 1) = 0$ &
$P(y_t=0 | \delta y_t = -1) = 0$ \\

$P(y_t=1 | \delta y_t = 0) = 0$ &
$P(y_t=1 | \delta y_t = 1) = \frac{1}{2}$ &
$P(y_t=1 | \delta y_t = -1) = 1$ \\

$P(y_t=2 | \delta y_t = 0) = 0$ &
$P(y_t=2 | \delta y_t = 1) = \frac{1}{2}$ &
$P(y_t=2 | \delta y_t = -1) = 0$ \\
\hline \\
$P(\delta y_t=0 | y_t = 0) = 1$ &
$P(\delta y_t=0 | y_t = 1) = 0$ &
$P(\delta y_t=0 | y_t = 2) = 0$ \\

$P(\delta y_t=1 | y_t = 0) = 0$ &
$P(\delta y_t=1 | y_t = 1) = \frac{1}{2}$ &
$P(\delta y_t=1 | y_t = 2) = 1$ \\

$P(\delta y_t=-1 | y_t = 0) = 0$ &
$P(\delta y_t=-1 | y_t = 1) = \frac{1}{2}$ &
$P(\delta y_t=-1 | y_t = 2) = 0$ \\
\end{tabular}
\end{center}

We now need the three term joint distributions, so
\begin{center}
\begin{tabular}{ccc}
$P(y_t = 0 \cap x_t = 0 \cap \delta x_t = 0) = \frac{1}{5}$ & $P(y_t = 0 \cap x_t = 0 \cap \delta x_t = 1) =0$ & $P(y_t = 0 \cap x_t = 0 \cap \delta x_t = -1) =0$ \\
$P(y_t = 0 \cap x_t = 1 \cap \delta x_t = 0) =0$ & $P(y_t = 0 \cap x_t = 1 \cap \delta x_t = 1) = \frac{1}{5}$ & $P(y_t = 0 \cap x_t = 1 \cap \delta x_t = -1) =0$ \\
$P(y_t = 0 \cap x_t = 2 \cap \delta x_t = 0) =0$ & $P(y_t = 0 \cap x_t = 2 \cap \delta x_t = 1) =0$ & $P(y_t = 0 \cap x_t = 2 \cap \delta x_t = -1) =0$ \\
$P(y_t = 1 \cap x_t = 0 \cap \delta x_t = 0) =0$ & $P(y_t = 1 \cap x_t = 0 \cap \delta x_t = 1) =0$ & $P(y_t = 1 \cap x_t = 0 \cap \delta x_t = -1) =\frac{1}{5}$ \\
$P(y_t = 1 \cap x_t = 1 \cap \delta x_t = 0) =0$ & $P(y_t = 1 \cap x_t = 1 \cap \delta x_t = 1) =0$ & $P(y_t = 1 \cap x_t = 1 \cap \delta x_t = -1) =0$ \\
$P(y_t = 1 \cap x_t = 2 \cap \delta x_t = 0) =0$ & $P(y_t = 1 \cap x_t = 2 \cap \delta x_t = 1) =\frac{1}{5}$ & $P(y_t = 1 \cap x_t = 2 \cap \delta x_t = -1) =0$ \\
$P(y_t = 2 \cap x_t = 0 \cap \delta x_t = 0) =0$ & $P(y_t = 2 \cap x_t = 0 \cap \delta x_t = 1) =0$ & $P(y_t = 2 \cap x_t = 0 \cap \delta x_t = -1) =0$ \\
$P(y_t = 2 \cap x_t = 1 \cap \delta x_t = 0) =0$ & $P(y_t = 2 \cap x_t = 1 \cap \delta x_t = 1) =0$ & $P(y_t = 2 \cap x_t = 1 \cap \delta x_t = -1) =\frac{1}{5}$ \\
$P(y_t = 2 \cap x_t = 2 \cap \delta x_t = 0) =0$ & $P(y_t = 2 \cap x_t = 2 \cap \delta x_t = 1) =0$ & $P(y_t = 2 \cap x_t = 2 \cap \delta x_t = -1) =0$ \\
\hline \\
$P(x_t = 0 \cap y_t = 0 \cap \delta y_t = 0) =\frac{1}{5}$ & $P(x_t = 0 \cap y_t = 0 \cap \delta y_t = 1) =0$ & $P(x_t = 0 \cap y_t = 0 \cap \delta y_t = -1) =0$ \\
$P(x_t = 0 \cap y_t = 1 \cap \delta y_t = 0) =0$ & $P(x_t = 0 \cap y_t = 1 \cap \delta y_t = 1) =0$ & $P(x_t = 0 \cap y_t = 1 \cap \delta y_t = -1) =\frac{1}{5}$ \\
$P(x_t = 0 \cap y_t = 2 \cap \delta y_t = 0) =0$ & $P(x_t = 0 \cap y_t = 2 \cap \delta y_t = 1) =0$ & $P(x_t = 0 \cap y_t = 2 \cap \delta y_t = -1) =0$ \\
$P(x_t = 1 \cap y_t = 0 \cap \delta y_t = 0) =\frac{1}{5}$ & $P(x_t = 1 \cap y_t = 0 \cap \delta y_t = 1) =0$ & $P(x_t = 1 \cap y_t = 0 \cap \delta y_t = -1) =0$ \\
$P(x_t = 1 \cap y_t = 1 \cap \delta y_t = 0) =0$ & $P(x_t = 1 \cap y_t = 1 \cap \delta y_t = 1) =0$ & $P(x_t = 1 \cap y_t = 1 \cap \delta y_t = -1) =0$ \\
$P(x_t = 1 \cap y_t = 2 \cap \delta y_t = 0) =0$ & $P(x_t = 1 \cap y_t = 2 \cap \delta y_t = 1) =\frac{1}{5}$ & $P(x_t = 1 \cap y_t = 2 \cap \delta y_t = -1) =0$ \\
$P(x_t = 2 \cap y_t = 0 \cap \delta y_t = 0) =0$ & $P(x_t = 2 \cap y_t = 0 \cap \delta y_t = 1) =0$ & $P(x_t = 2 \cap y_t = 0 \cap \delta y_t = -1) =0$ \\
$P(x_t = 2 \cap y_t = 1 \cap \delta y_t = 0) =0$ & $P(x_t = 2 \cap y_t = 1 \cap \delta y_t = 1) =\frac{1}{5}$ & $P(x_t = 2 \cap y_t = 1 \cap \delta y_t = -1) =0$ \\
$P(x_t = 2 \cap y_t = 2 \cap \delta y_t = 0) =0$ & $P(x_t = 2 \cap y_t = 2 \cap \delta y_t = 1) =0$ & $P(x_t = 2 \cap y_t = 2 \cap \delta y_t = -1) =0$ 
\end{tabular}
\end{center}

The new conditional distributions are
\begin{center}
\begin{tabular}{ccc}
$P(y_t = 0 | x_t = 0 \cap \delta x_t = 0) = 1$ & $P(y_t = 0 | x_t = 0 \cap \delta x_t = 1) =0$ & $P(y_t = 0 | x_t = 0 \cap \delta x_t = -1) =0$ \\
$P(y_t = 0 | x_t = 1 \cap \delta x_t = 0) =0$ & $P(y_t = 0 | x_t = 1 \cap \delta x_t = 1) = 1$ & $P(y_t = 0 | x_t = 1 \cap \delta x_t = -1) =0$ \\
$P(y_t = 0 | x_t = 2 \cap \delta x_t = 0) =0$ & $P(y_t = 0 | x_t = 2 \cap \delta x_t = 1) =0$ & $P(y_t = 0 | x_t = 2 \cap \delta x_t = -1) =0$ \\
$P(y_t = 1 | x_t = 0 \cap \delta x_t = 0) =0$ & $P(y_t = 1 | x_t = 0 \cap \delta x_t = 1) =0$ & $P(y_t = 1 | x_t = 0 \cap \delta x_t = -1) =1$ \\
$P(y_t = 1 | x_t = 1 \cap \delta x_t = 0) =0$ & $P(y_t = 1 | x_t = 1 \cap \delta x_t = 1) =0$ & $P(y_t = 1 | x_t = 1 \cap \delta x_t = -1) =0$ \\
$P(y_t = 1 | x_t = 2 \cap \delta x_t = 0) =0$ & $P(y_t = 1 | x_t = 2 \cap \delta x_t = 1) =1$ & $P(y_t = 1 | x_t = 2 \cap \delta x_t = -1) =0$ \\
$P(y_t = 2 | x_t = 0 \cap \delta x_t = 0) =0$ & $P(y_t = 2 | x_t = 0 \cap \delta x_t = 1) =0$ & $P(y_t = 2 | x_t = 0 \cap \delta x_t = -1) =0$ \\
$P(y_t = 2 | x_t = 1 \cap \delta x_t = 0) =0$ & $P(y_t = 2 | x_t = 1 \cap \delta x_t = 1) =0$ & $P(y_t = 2 | x_t = 1 \cap \delta x_t = -1) =1$ \\
$P(y_t = 2 | x_t = 2 \cap \delta x_t = 0) =0$ & $P(y_t = 2 | x_t = 2 \cap \delta x_t = 1) =0$ & $P(y_t = 2 | x_t = 2 \cap \delta x_t = -1) =0$ \\
\hline \\
$P(x_t = 0 | y_t = 0 \cap \delta y_t = 0) =\frac{1}{2}$ & $P(x_t = 0 | y_t = 0 \cap \delta y_t = 1) =0$ & $P(x_t = 0 | y_t = 0 \cap \delta y_t = -1) =0$ \\
$P(x_t = 0 | y_t = 1 \cap \delta y_t = 0) =0$ & $P(x_t = 0 | y_t = 1 \cap \delta y_t = 1) =0$ & $P(x_t = 0 | y_t = 1 \cap \delta y_t = -1) =1$ \\
$P(x_t = 0 | y_t = 2 \cap \delta y_t = 0) =0$ & $P(x_t = 0 | y_t = 2 \cap \delta y_t = 1) =0$ & $P(x_t = 0 | y_t = 2 \cap \delta y_t = -1) =0$ \\
$P(x_t = 1 | y_t = 0 \cap \delta y_t = 0) =\frac{1}{2}$ & $P(x_t = 1 | y_t = 0 \cap \delta y_t = 1) =0$ & $P(x_t = 1 | y_t = 0 \cap \delta y_t = -1) =0$ \\
$P(x_t = 1 | y_t = 1 \cap \delta y_t = 0) =0$ & $P(x_t = 1 | y_t = 1 \cap \delta y_t = 1) =0$ & $P(x_t = 1 | y_t = 1 \cap \delta y_t = -1) =0$ \\
$P(x_t = 1 | y_t = 2 \cap \delta y_t = 0) =0$ & $P(x_t = 1 | y_t = 2 \cap \delta y_t = 1) =1$ & $P(x_t = 1 | y_t = 2 \cap \delta y_t = -1) =0$ \\
$P(x_t = 2 | y_t = 0 \cap \delta y_t = 0) =0$ & $P(x_t = 2 | y_t = 0 \cap \delta y_t = 1) =0$ & $P(x_t = 2 | y_t = 0 \cap \delta y_t = -1) =0$ \\
$P(x_t = 2 | y_t = 1 \cap \delta y_t = 0) =0$ & $P(x_t = 2 | y_t = 1 \cap \delta y_t = 1) =1$ & $P(x_t = 2 | y_t = 1 \cap \delta y_t = -1) =0$ \\
$P(x_t = 2 | y_t = 2 \cap \delta y_t = 0) =0$ & $P(x_t = 2 | y_t = 2 \cap \delta y_t = 1) =0$ & $P(x_t = 2 | y_t = 2 \cap \delta y_t = -1) =0$ 
\end{tabular}
\end{center}

In general, the penchant can be written using the law of total probability, i.e
$$
P(E) = P(E|C)P(C) + P(E|\bar{C})P(\bar{C})\;\;,
$$
and Bayes theorem, i.e.\ 
$$
P(E|\bar{C}) = P(\bar{C}|E)\frac{P(E)}{P(\bar{C})}\;\;.
$$
The probability complement rules yield $P(\bar{C}) = 1-P(C)$ and $P(\bar{C}|E) = 1-P(C|E)$.  Applying Bayes again leads to
$$
P(\bar{C}|E) = 1-P(E|C)\frac{P(C)}{P(E)}
$$
Thus,
$$
P(E|\bar{C}) = \left(1-P(E|C)\frac{P(C)}{P(E)}\right)\frac{P(E)}{1-P(C)}\;\;.
$$
This expression implies
\begin{eqnarray}
\rho &=& P(E|C)-P(E|\bar{C})\\
&=& P(E|C)-\left(1-P(E|C)\frac{P(C)}{P(E)}\right)\frac{P(E)}{1-P(C)}\\
&=& P(E|C)-\frac{P(E)}{1-P(C)}+P(E|C)\frac{P(C)}{1-P(C)}\\
&=& P(E|C)\left(1+\frac{P(C)}{1-P(C)}\right)-\frac{P(E)}{1-P(C)}\\
&=& \frac{P(E\cap C)}{P(C)}\left(1+\frac{P(C)}{1-P(C)}\right)-\frac{P(E)}{1-P(C)}\\
&=& P(E\cap C)\left(\frac{1}{P(C)}+\frac{1}{1-P(C)}\right)-\frac{P(E)}{1-P(C)}\;\;.
\end{eqnarray}
This same expression can be derived without Bayes theorem by using the law of total probability rewritten as
$$
P(E|\bar{C}) = \frac{P(E)}{P(\bar{C})} - P(E|C)\frac{P(C)}{P(\bar{C})}
$$
and making the appropriate substitution into the definition of the penchants follows:
\begin{eqnarray}
\rho &=& P(E|C)-P(E|\bar{C})\\
&=& P(E|C)-\left(\frac{P(E)}{P(\bar{C})} - P(E|C)\frac{P(C)}{P(\bar{C})}\right)\\
&=& P(E|C)-\frac{P(E)}{P(\bar{C})} + P(E|C)\frac{P(C)}{P(\bar{C})}\\
&=& P(E|C)\left(1+\frac{P(C)}{P(\bar{C})}\right)-\frac{P(E)}{P(\bar{C})}\\
&=& P(E|C)\left(1+\frac{P(C)}{1-P(C)}\right)-\frac{P(E)}{1-P(C)}\\
&=& \frac{P(E\cap C)}{P(C)}\left(1+\frac{P(C)}{1-P(C)}\right)-\frac{P(E)}{1-P(C)}\\
&=& P(E\cap C)\left(\frac{1}{P(C)}+\frac{1}{1-P(C)}\right)-\frac{P(E)}{1-P(C)}\;\;.
\end{eqnarray}

Consider the cause-effect pair $(E,C)=(x_t=0,y_t=0)$ penchant
\begin{eqnarray}
\rho_{xy}^{(1)} &=& P(x_t = 0 | y_t = 0) - P(x_t = 0 | y_t \neq 0)\\
&=&  P(x_t = 0 | y_t = 0)\left(1+\frac{P(y_t = 0)}{1-P(y_t = 0)}\right)-\frac{P(x_t = 0)}{1-P(y_t = 0)}\\
&=& \frac{1}{2}\left(1+\frac{2}{5}\left(1-\frac{2}{5}\right)^{-1}\right)-\frac{2}{5}\left(1-\frac{2}{5}\right)^{-1}\\
&=& \frac{1}{6}
\end{eqnarray}
and its partner
\begin{eqnarray}
\rho_{yx}^{(1)} &=& P(y_t = 0 | x_t = 0) - P(y_t = 0 | x_t \neq 0)\\
&=& P(y_t = 0|x_t = 0)\left(1+\frac{P(x_t = 0)}{1-P(x_t = 0)}\right)-\frac{P(y_t = 0)}{1-P(x_t = 0)}\\
&=& \frac{1}{2}\left(1+\frac{2}{5}\left(1-\frac{2}{5}\right)^{-1}\right)-\frac{2}{5}\left(1-\frac{2}{5}\right)^{-1}\\
&=& \frac{1}{6}\;\;.
\end{eqnarray}

In general, a cause-effect pair $(C,E)$ depends on $P(E|C)$, $P(E)$, and $P(C)$.  Thus, if $P(E|C)=P(C|E)$ and $P(E)=P(C)$, then the $(C,E)$ penchant is equal to the $(E,C)$ penchant.  It follows that $\langle \rho_{xy} \rangle = \langle \rho_{yx} \rangle$.

The hope here is that the penchants for the cause-effect pairs $(E,C)=(\delta x_t,y_t)$ and its partner will lead to the intuitive conclusion $\mathbf{x}\rightarrow\mathbf{y}$.  The first such pair is $(E,C)=(\delta x_t=0,y_t=0)$ which leads to
\begin{eqnarray}
\rho_{\delta xy}^{(1)} &=& P(\delta x_t = 0 | y_t = 0) - P(\delta x_t = 0 | y_t \neq 0)\\
&=&  P(\delta x_t = 0 | y_t = 0)\left(1+\frac{P(y_t = 0)}{1-P(y_t = 0)}\right)-\frac{P(\delta x_t = 0)}{1-P(y_t = 0)}\\
&=& \frac{1}{2}\left(1+\frac{2}{5}\left(1-\frac{2}{5}\right)^{-1}\right)-\frac{1}{5}\left(1-\frac{2}{5}\right)^{-1}\\
&=& \frac{1}{2}
\end{eqnarray}
and
\begin{eqnarray}
\rho_{y\delta x}^{(1)} &=& P(y_t = 0 | \delta x_t = 0) - P(y_t = 0 | \delta x_t \neq 0)\\
&=& P(y_t = 0|\delta x_t = 0)\left(1+\frac{P(\delta x_t = 0)}{1-P(\delta x_t = 0)}\right)-\frac{P(y_t = 0)}{1-P(\delta x_t = 0)}\\
&=& \left(1+\frac{1}{5}\left(1-\frac{1}{5}\right)^{-1}\right)-\frac{2}{5}\left(1-\frac{1}{5}\right)^{-1}\\
&=& \frac{3}{4}\;\;.
\end{eqnarray}
Thus, $\rho_{y\delta x}^{(1)}>\rho_{\delta xy}^{(1)}$ implying $\mathbf{\delta x}\rightarrow\mathbf{y}$, as expected.  The mean penchants for this cause-effect pair are
\begin{eqnarray}
\langle \rho_{\delta xy}\rangle &=& \frac{1}{9}\left(\sum_{i=-1}^1 \sum_{j=0}^2 P(\delta x_t = i | y_t = j) - P(\delta x_t = i | y_t \neq j)\right)\\
&=& \frac{1}{9}\left(\sum_{i=-1}^1 \sum_{j=0}^2 P(\delta x_t = i | y_t = j)\left(1+\frac{P(y_t = j)}{1-P(y_t = j)}\right)-\frac{P(\delta x_t = i)}{1-P(y_t = j)}\right)\\
&=& \frac{1}{9}\left(P(\delta x_t = 0 | y_t = 0)\left(1+\frac{P(y_t = 0)}{1-P(y_t = 0)}\right)-\frac{P(\delta x_t = 0)}{1-P(y_t = 0)}\right.\\
& &P(\delta x_t = 0 | y_t = 1)\left(1+\frac{P(y_t = 1)}{1-P(y_t = 1)}\right)-\frac{P(\delta x_t = 0)}{1-P(y_t = 1)}\\
& &P(\delta x_t = 0 | y_t = 2)\left(1+\frac{P(y_t = 2)}{1-P(y_t = 2)}\right)-\frac{P(\delta x_t = 0)}{1-P(y_t = 2)}\\
& &P(\delta x_t = 1 | y_t = 0)\left(1+\frac{P(y_t = 0)}{1-P(y_t = 0)}\right)-\frac{P(\delta x_t = 1)}{1-P(y_t = 0)}\\
& &P(\delta x_t = 1 | y_t = 1)\left(1+\frac{P(y_t = 1)}{1-P(y_t = 1)}\right)-\frac{P(\delta x_t = 1)}{1-P(y_t = 1)}\\
& &P(\delta x_t = 1 | y_t = 2)\left(1+\frac{P(y_t = 2)}{1-P(y_t = 2)}\right)-\frac{P(\delta x_t = 1)}{1-P(y_t = 2)}\\
& &P(\delta x_t = -1 | y_t = 0)\left(1+\frac{P(y_t = 0)}{1-P(y_t = 0)}\right)-\frac{P(\delta x_t = -1)}{1-P(y_t = 0)}\\
& &P(\delta x_t = -1 | y_t = 1)\left(1+\frac{P(y_t = 1)}{1-P(y_t = 1)}\right)-\frac{P(\delta x_t = -1)}{1-P(y_t = 1)}\\
& &\left. P(\delta x_t = -1 | y_t = 2)\left(1+\frac{P(y_t = 2)}{1-P(y_t = 2)}\right)-\frac{P(\delta x_t = -1)}{1-P(y_t = 2)}\right)\\
&=& \frac{1}{9}\left(\frac{1}{2}\left(1+\frac{\frac{2}{5}}{1-\frac{2}{5}}\right)-\frac{\frac{1}{5}}{1-\frac{2}{5}}\right.\\
& &-\frac{\frac{1}{5}}{1-\frac{2}{5}}\\
& &-\frac{\frac{1}{5}}{1-\frac{1}{5}}\\
& &\frac{1}{2}\left(1+\frac{\frac{2}{5}}{1-\frac{2}{5}}\right)-\frac{\frac{2}{5}}{1-\frac{2}{5}}\\
& &\frac{1}{2}\left(1+\frac{\frac{2}{5}}{1-\frac{2}{5}}\right)-\frac{\frac{2}{5}}{1-\frac{2}{5}}\\
& &-\frac{\frac{2}{5}}{1-\frac{1}{5}}\\
& &-\frac{\frac{2}{5}}{1-\frac{2}{5}}\\
& &\frac{1}{2}\left(1+\frac{\frac{2}{5}}{1-\frac{2}{5}}\right)-\frac{\frac{2}{5}}{1-\frac{2}{5}}\\
& &\left. \left(1+\frac{\frac{1}{5}}{1-\frac{1}{5}}\right)-\frac{\frac{2}{5}}{1-\frac{1}{5}}\right)\\
&=& \frac{1}{9}\left(\frac{1}{2}-\frac{1}{3}-\frac{1}{4}+\frac{1}{6}+\frac{1}{6}-\frac{1}{2}-\frac{2}{3}+\frac{1}{6}+\frac{3}{4}\right)\\
&=& 0
\end{eqnarray}
and
\begin{eqnarray}
\langle \rho_{y\delta x}\rangle &=& \frac{1}{9}\left(\sum_{i=-1}^1 \sum_{j=0}^2 P(y_t = j | \delta x_t = i) - P(y_t = j | \delta x_t \neq i)\right)\\
&=& \frac{1}{9}\left(\sum_{i=-1}^1 \sum_{j=0}^2 P(y_t = j | \delta x_t = i)\left(1+\frac{P(\delta x_t = i)}{1-P(\delta x_t = i)}\right)-\frac{P(y_t = j)}{1-P(\delta x_t = i)}\right)\\
&=& \frac{1}{9}\left(P(y_t = 0 | \delta x_t = 0)\left(1+\frac{P(\delta x_t = 0)}{1-P(\delta x_t = 0)}\right)-\frac{P(y_t = 0)}{1-P(\delta x_t = 0)}\right.\\
& & P(y_t = 0 | \delta x_t = 1)\left(1+\frac{P(\delta x_t = 1)}{1-P(\delta x_t = 1)}\right)-\frac{P(y_t = 0)}{1-P(\delta x_t = 1)}\\
& & P(y_t = 0 | \delta x_t = -1)\left(1+\frac{P(\delta x_t = -1)}{1-P(\delta x_t = -1)}\right)-\frac{P(y_t = 0)}{1-P(\delta x_t = -1)}\\
& & P(y_t = 1 | \delta x_t = 0)\left(1+\frac{P(\delta x_t = 0)}{1-P(\delta x_t = 0)}\right)-\frac{P(y_t = 1)}{1-P(\delta x_t = 0)}\\
& & P(y_t = 1 | \delta x_t = 1)\left(1+\frac{P(\delta x_t = 1)}{1-P(\delta x_t = 1)}\right)-\frac{P(y_t = 1)}{1-P(\delta x_t = 1)}\\
& & P(y_t = 1 | \delta x_t = -1)\left(1+\frac{P(\delta x_t = -1)}{1-P(\delta x_t = -1)}\right)-\frac{P(y_t = 1)}{1-P(\delta x_t = -1)}\\
& & P(y_t = 2 | \delta x_t = 0)\left(1+\frac{P(\delta x_t = 0)}{1-P(\delta x_t = 0)}\right)-\frac{P(y_t = 2)}{1-P(\delta x_t = 0)}\\
& & P(y_t = 2 | \delta x_t = 1)\left(1+\frac{P(\delta x_t = 1)}{1-P(\delta x_t = 1)}\right)-\frac{P(y_t = 2)}{1-P(\delta x_t = 1)}\\
& & \left.P(y_t = 2 | \delta x_t = -1)\left(1+\frac{P(\delta x_t = -1)}{1-P(\delta x_t = -1)}\right)-\frac{P(y_t = 2)}{1-P(\delta x_t = -1)}\right)\\
&=& \frac{1}{9}\left(\left(1+\frac{\frac{1}{5}}{1-\frac{1}{5}}\right)-\frac{\frac{2}{5}}{1-\frac{1}{5}}\right.\\
& & \frac{1}{2}\left(1+\frac{\frac{2}{5}}{1-\frac{2}{5}}\right)-\frac{\frac{2}{5}}{1-\frac{2}{5}}\\
& & -\frac{\frac{2}{5}}{1-\frac{2}{5}}\\
& & -\frac{\frac{2}{5}}{1-\frac{1}{5}}\\
& & \frac{1}{2}\left(1+\frac{\frac{2}{5}}{1-\frac{2}{5}}\right)-\frac{\frac{2}{5}}{1-\frac{2}{5}}\\
& & \frac{1}{2}\left(1+\frac{\frac{2}{5}}{1-\frac{2}{5}}\right)-\frac{\frac{2}{5}}{1-\frac{2}{5}}\\
& & -\frac{\frac{1}{5}}{1-\frac{1}{5}}\\
& & -\frac{\frac{1}{5}}{1-\frac{2}{5}}\\
& & \left.\frac{1}{2}\left(1+\frac{\frac{2}{5}}{1-\frac{2}{5}}\right)-\frac{\frac{1}{5}}{1-\frac{2}{5}}\right)\\
&=& \frac{1}{9}\left(\frac{3}{4}+\frac{1}{6}-\frac{2}{3}-\frac{1}{2}+\frac{1}{6}+\frac{1}{6}-\frac{1}{4}-\frac{1}{3}+\frac{1}{2}\right)\\
&=& 0\;\;.
\end{eqnarray}
It follows that $\langle \rho_{y\delta x}\rangle = \langle \rho_{\delta xy}\rangle$ which does not imply that $\mathbf{x}\rightarrow\mathbf{y}$.  The result does not agree with intuition.

There are several other cause-effect pairs to investigate in this example including the ``primary penchant comparisons'' $(\mathbf{x},\mathbf{y})$, $(\delta \mathbf{x},\mathbf{y})$, $(\mathbf{x},\delta \mathbf{y})$, and $(\delta \mathbf{x},\delta \mathbf{y})$, the ``self penchant comparisons'' $(\mathbf{x},\delta \mathbf{x})$ and $(\mathbf{y},\delta \mathbf{y})$, the ``higher order penchant comparisons'' $(\mathbf{x}\cap\delta\mathbf{x},\mathbf{y})$ and $(\mathbf{y}\cap\delta\mathbf{y},\mathbf{x})$, and the ``highest order penchant comparison'' $(\mathbf{x}\cap\delta\mathbf{x},\mathbf{y}\cap\delta\mathbf{y})$.  Given the intution for this example system, i.e.\ $\mathbf{x}\rightarrow\mathbf{y}$, the following predictions for the mean penchant comparisons for each of the cause-effect pairs mentioned above can be made:
\begin{center}
\begin{tabular}{r|cllll}
Cause-Effect Pair & Prediction & Confirmed? & Confirmed Num? & Calculated? & Cal Num?\\
$(\mathbf{x},\mathbf{y})$ & {\bf ??} & n/a & n/a & yes & yes\\
$(\delta \mathbf{x},\mathbf{y})$ & $\langle \rho_{y\delta x}\rangle > \langle \rho_{\delta xy}\rangle$ & no & no & yes & yes\\
$(\mathbf{x},\delta \mathbf{y})$ & {\bf ??} & n/a & n/a & no & no\\
$(\delta \mathbf{x},\delta \mathbf{y})$ &  $\langle \rho_{\delta y\delta x}\rangle > \langle \rho_{\delta x\delta y}\rangle$ & no & no & no & no\\
$(\mathbf{x},\delta \mathbf{x})$ & $\langle \rho_{x\delta x}\rangle > \langle \rho_{\delta xx}\rangle$ & no & no & no & no\\
$(\mathbf{y},\delta \mathbf{y})$ & $\langle \rho_{y\delta y}\rangle > \langle \rho_{\delta yy}\rangle$ & no & no & no & no\\
$(\mathbf{x}\cap\delta\mathbf{x},\mathbf{y})$ & $\langle \rho_{(x\delta x)y}\rangle < \langle \rho_{y(x\delta x)}\rangle$ & no & no & no & no\\ 
$(\mathbf{y}\cap\delta\mathbf{y},\mathbf{x})$ & {\bf ??} & no & no & no & no\\
$(\mathbf{x}\cap\delta\mathbf{x},\mathbf{y}\cap\delta\mathbf{y})$ & $\langle \rho_{(x\delta x)(y\delta y)}\rangle < \langle \rho_{(y\delta y)(x\delta x)}\rangle$ & no & no & no & no
\end{tabular}
\end{center}

The penchant comparison calculations can be done by hand (as seen above) but they are tedious.  So, the next step is to write an algorithm that reproduces the above results and allows me to fill out the above table.  The computational cost of the penchant comparison calculation increase dramatically for the higher and highest order penchants (as compared to the primary and self penchants).  There will also be an issue of bin sizes in the algorithms where histograms will be used to approximately the probabilities used above.  The dependency of the algorithm on the bin size needs to be studied.  

The penchants calculated by hand do not seem to agree with intuition.  However, the calculations can be confirmed numerically, so it may appear as though this method is not an effective test for causal dependence.  I want to point out that this may only be true for the (very) short library lengths used in this example.  The short library length was used to help keep the manual calculations manageable, however, increasing the library length (but following the same construction pattern) leads to penchant calculations that do agree with intuition, i.e.\
\begin{equation}
\left.
\begin{array}{c}
\mathbf{x} = \{0,1,2,1,0\}\\
\mathbf{y} = \{0,0,1,2,1\}\\
\mathbf{\delta x} = \{0,1,1,-1,-1\}\\
\mathbf{\delta y} = \{0,0,1,1,-1\}
\end{array}
\right] \Rightarrow \left(\langle \rho_{y\delta x}\rangle, \langle \rho_{\delta xy}\rangle\right) = \left(0,0\right) \rightarrow \langle \rho_{y\delta x}\rangle = \langle \rho_{\delta xy}\rangle
\end{equation}
and just doubling the library length leads to
\begin{equation}
\left.
\begin{array}{c}
\mathbf{x} = \{0,1,2,1,0,1,2,1,0,1\}\\
\mathbf{y} = \{0,0,1,2,1,0,1,2,1,0\}\\
\mathbf{\delta x} = \{0,1,1,-1,-1,1,1,-1,-1,1\}\\
\mathbf{\delta y} = \{0,0,1,1,-1,1,1,-1,-1,1\}
\end{array}
\right] \Rightarrow \left(\langle \rho_{y\delta x}\rangle, \langle \rho_{\delta xy}\rangle\right) \approx \left(-0.137,-0.162\right) \rightarrow \langle \rho_{y\delta x}\rangle > \langle \rho_{\delta xy}\rangle
\end{equation}
and 
\begin{eqnarray}
& &\left.
\begin{array}{c}
\mathbf{x} = \{0,1,2,1,0,1,2,1,0,1,2,1,0,1,2,1,0,1,2,1,0,1,2,1,0\}\\
\mathbf{y} = \{0,0,1,2,1,0,1,2,1,0,1,2,1,0,1,2,1,0,1,2,1,0,1,2,1\}\\
\mathbf{\delta x} = \{0,1,1,-1,-1,1,1,-1,-1,1,1,-1,-1,1,1,-1,-1,1,1,-1,-1,1,1,-1,-1\}\\
\mathbf{\delta y} = \{0,0,1,1,-1,-1,1,1,-1,-1,1,1,-1,-1,1,1,-1,-1,1,1,-1,-1,1,1,-1,-1\}
\end{array}
\right] \\
&\Rightarrow& \left(\langle \rho_{y\delta x}\rangle, \langle \rho_{\delta xy}\rangle\right) \approx \left(-0.267
,-0.3\right) \rightarrow \langle \rho_{y\delta x}\rangle > \langle \rho_{\delta xy}\rangle
\end{eqnarray}
Thus, the results agree with intuition if the library lengths are longer than those used in the manual calculation example.  This point should be kept in mind and the effect of library length on the LIR penchant method (which probably needs a new name) should be investigated.  All of the above number where calculated using histograms with only three, the final conclusions do not change for the number of bins is increased to ten but the actual value of the penchants do.  Again, this effect should also be explored.

The main task in estimating causal influence using the LIR method described above will revolve around comparing mean penchants to determine, e.g.\ $\langle \rho_{yx}\rangle > \langle \rho_{xy}\rangle$.  As single-valued metric will be constructed to facilitate the discussion as follows:
$$
\Delta_{yx} = \langle \rho_{yx}\rangle - \langle \rho_{xy}\rangle\;\;.
$$
$\Delta_{yx} > 0\Rightarrow \langle \rho_{yx}\rangle > \langle \rho_{xy}\rangle$, $\Delta_{yx} < 0\Rightarrow \langle \rho_{yx}\rangle < \langle \rho_{xy}\rangle$, and $\Delta_{yx} = 0\Rightarrow \langle \rho_{yx}\rangle = \langle \rho_{xy}\rangle$.  Thus, the sign of $\Delta$ can be used to estimate the causal influence as $\Delta_{yx} > 0\Rightarrow \mathbf{x}\rightarrow\mathbf{y}$, $\Delta_{yx} < 0\Rightarrow \mathbf{y}\rightarrow\mathbf{x}$, and $\Delta_{yx} = 0\Rightarrow$ no conclusion can be made.

The mean is a linear operator, so in general
\begin{eqnarray}
\Delta_{yx} &=& \langle \rho_{yx}\rangle - \langle \rho_{xy}\rangle\\
&=& \langle \rho_{yx} - \rho_{xy}\rangle\\
&=& \left\langle \left( P(y_t\cap x_t)\left(\frac{1}{P(x_t)}+\frac{1}{1-P(x_t)}\right)-\frac{P(y_t)}{1-P(x_t)}\right) \right.\\
& & \left.- \left( P(x_t\cap y_t)\left(\frac{1}{P(y_t)}+\frac{1}{1-P(y_t)}\right)-\frac{P(x_t)}{1-P(y_t)} \right) \right\rangle\\
&=& \left\langle P(y_t\cap x_t) \left( \frac{1}{P(x_t)}+\frac{1}{1-P(x_t)} - \frac{1}{P(y_t)}-\frac{1}{1-P(y_t)}\right) -\frac{P(y_t)}{1-P(x_t)}+\frac{P(x_t)}{1-P(y_t)}  \right\rangle
\end{eqnarray}
because $P(A\cap B) = P(B\cap A)$.  As an example, consider the time series $\mathbf{\delta x}$ and $\mathbf{y}$ from above:
\begin{eqnarray*}
\Delta_{y\delta x} &=& \langle \rho_{y\delta x}\rangle - \langle \rho_{\delta xy}\rangle\\
&=& \left\langle P(y_t\cap \delta x_t) \left( \frac{1}{P(\delta x_t)}+\frac{1}{1-P(\delta x_t)} - \frac{1}{P(y_t)}-\frac{1}{1-P(y_t)}\right) -\frac{P(y_t)}{1-P(\delta x_t)}+\frac{P(\delta x_t)}{1-P(y_t)}  \right\rangle\\
&=& \frac{1}{9}\left(\sum_{i=-1}^1 \sum_{j=0}^2 P(y_t=j\cap \delta x_t=i) \left( \frac{1}{P(\delta x_t=i)}+\frac{1}{1-P(\delta x_t=i)} - \frac{1}{P(y_t=j)}-\frac{1}{1-P(y_t=j)}\right)\right. \\
& &\left. -\frac{P(y_t=j)}{1-P(\delta x_t=i)}+\frac{P(\delta x_t=i)}{1-P(y_t=j)}\right)\\
&=& \frac{1}{9}\left(P(y_t=0\cap \delta x_t=-1) \left( \frac{1}{P(\delta x_t=-1)}+\frac{1}{1-P(\delta x_t=-1)} - \frac{1}{P(y_t=0)}-\frac{1}{1-P(y_t=0)}\right)\right.\\
& &-\frac{P(y_t=0)}{1-P(\delta x_t=-1)}+\frac{P(\delta x_t=-1)}{1-P(y_t=0)}\\
& &+ P(y_t=1\cap \delta x_t=-1) \left( \frac{1}{P(\delta x_t=-1)}+\frac{1}{1-P(\delta x_t=-1)} - \frac{1}{P(y_t=1)}-\frac{1}{1-P(y_t=1)}\right)\\
& &-\frac{P(y_t=1)}{1-P(\delta x_t=-1)}+\frac{P(\delta x_t=-1)}{1-P(y_t=1)}\\
& &+ P(y_t=2\cap \delta x_t=-1) \left( \frac{1}{P(\delta x_t=-1)}+\frac{1}{1-P(\delta x_t=-1)} - \frac{1}{P(y_t=2)}-\frac{1}{1-P(y_t=2)}\right)\\
& &-\frac{P(y_t=2)}{1-P(\delta x_t=-1)}+\frac{P(\delta x_t=-1)}{1-P(y_t=2)}\\
& &+P(y_t=0\cap \delta x_t=0) \left( \frac{1}{P(\delta x_t=0)}+\frac{1}{1-P(\delta x_t=0)} - \frac{1}{P(y_t=0)}-\frac{1}{1-P(y_t=0)}\right)\\
& &-\frac{P(y_t=0)}{1-P(\delta x_t=0)}+\frac{P(\delta x_t=0)}{1-P(y_t=0)}\\
& &+ P(y_t=1\cap \delta x_t=0) \left( \frac{1}{P(\delta x_t=0)}+\frac{1}{1-P(\delta x_t=0)} - \frac{1}{P(y_t=1)}-\frac{1}{1-P(y_t=1)}\right)\\
& &-\frac{P(y_t=1)}{1-P(\delta x_t=0)}+\frac{P(\delta x_t=0)}{1-P(y_t=1)}\\
& &+ P(y_t=2\cap \delta x_t=0) \left( \frac{1}{P(\delta x_t=0)}+\frac{1}{1-P(\delta x_t=0)} - \frac{1}{P(y_t=2)}-\frac{1}{1-P(y_t=2)}\right)\\
& &-\frac{P(y_t=2)}{1-P(\delta x_t=0)}+\frac{P(\delta x_t=0)}{1-P(y_t=2)}\\
& &+P(y_t=0\cap \delta x_t=1) \left( \frac{1}{P(\delta x_t=1)}+\frac{1}{1-P(\delta x_t=1)} - \frac{1}{P(y_t=0)}-\frac{1}{1-P(y_t=0)}\right)\\
& &-\frac{P(y_t=0)}{1-P(\delta x_t=1)}+\frac{P(\delta x_t=1)}{1-P(y_t=0)}\\
& &+ P(y_t=1\cap \delta x_t=1) \left( \frac{1}{P(\delta x_t=1)}+\frac{1}{1-P(\delta x_t=1)} - \frac{1}{P(y_t=1)}-\frac{1}{1-P(y_t=1)}\right)\\
& &-\frac{P(y_t=1)}{1-P(\delta x_t=1)}+\frac{P(\delta x_t=1)}{1-P(y_t=1)}\\
& &+ P(y_t=2\cap \delta x_t=1) \left( \frac{1}{P(\delta x_t=1)}+\frac{1}{1-P(\delta x_t=1)} - \frac{1}{P(y_t=2)}-\frac{1}{1-P(y_t=2)}\right)\\
& &\left.-\frac{P(y_t=2)}{1-P(\delta x_t=1)}+\frac{P(\delta x_t=1)}{1-P(y_t=2)}\right)
\end{eqnarray*}
$\ldots$continued$\ldots$
\begin{eqnarray*}
&=& \frac{1}{9}\left(-\frac{\frac{2}{5}}{1-\frac{2}{5}}+\frac{\frac{2}{5}}{1-\frac{2}{5}}+ \frac{1}{5} \left( \frac{1}{\frac{2}{5}}+\frac{1}{1-\frac{2}{5}} - \frac{1}{\frac{2}{5}}-\frac{1}{1-\frac{2}{5}}\right)\right.\\
& &-\frac{\frac{2}{5}}{1-\frac{2}{5}}+\frac{\frac{2}{5}}{1-\frac{2}{5}}+ \frac{1}{5} \left( \frac{1}{\frac{2}{5}}+\frac{1}{1-\frac{2}{5}} - \frac{1}{\frac{1}{5}}-\frac{1}{1-\frac{1}{5}}\right)\\
& &-\frac{\frac{1}{5}}{1-\frac{2}{5}}+\frac{\frac{2}{5}}{1-\frac{1}{5}}+\frac{1}{5} \left( \frac{1}{\frac{1}{5}}+\frac{1}{1-\frac{1}{5}} - \frac{1}{\frac{2}{5}}-\frac{1}{1-\frac{2}{5}}\right)\\
& &-\frac{\frac{2}{5}}{1-\frac{1}{5}}+\frac{\frac{1}{5}}{1-\frac{2}{5}}-\frac{\frac{2}{5}}{1-\frac{1}{5}}+\frac{\frac{1}{5}}{1-\frac{2}{5}}\\
& &-\frac{\frac{1}{5}}{1-\frac{1}{5}}+\frac{\frac{1}{5}}{1-\frac{1}{5}}+\frac{1}{5} \left( \frac{1}{\frac{2}{5}}+\frac{1}{1-\frac{2}{5}} - \frac{1}{\frac{2}{5}}-\frac{1}{1-\frac{2}{5}}\right)\\
& &-\frac{\frac{2}{5}}{1-\frac{2}{5}}+\frac{\frac{2}{5}}{1-\frac{2}{5}}+\frac{1}{5} \left( \frac{1}{\frac{2}{5}}+\frac{1}{1-\frac{2}{5}} - \frac{1}{\frac{2}{5}}-\frac{1}{1-\frac{2}{5}}\right)\\
& &\left.-\frac{\frac{2}{5}}{1-\frac{2}{5}}+\frac{\frac{2}{5}}{1-\frac{2}{5}}-\frac{\frac{1}{5}}{1-\frac{2}{5}}+\frac{\frac{2}{5}}{1-\frac{1}{5}}\right)\\
&=& \frac{1}{9}\left(-\frac{2}{3}+\frac{2}{3}+\frac{1}{5} \left(\frac{5}{2}+\frac{5}{3}-\frac{5}{2}-\frac{5}{3}\right)\right.\\
& &-\frac{2}{3}+\frac{2}{3}+ \frac{1}{5} \left(\frac{5}{2}+\frac{5}{3}-\frac{5}{1}-\frac{5}{4}\right)\\
& &-\frac{1}{3}+\frac{2}{4}+\frac{1}{5} \left(\frac{5}{1}+\frac{5}{4}-\frac{5}{2}-\frac{5}{3}\right)\\
& &-\frac{2}{4}+\frac{1}{3}-\frac{2}{4}+\frac{1}{3}\\
& &-\frac{1}{4}+\frac{1}{4}+\frac{1}{5} \left(\frac{5}{2}+\frac{5}{3}-\frac{5}{2}-\frac{5}{3}\right)\\
& &-\frac{2}{3}+\frac{2}{3}+\frac{1}{5} \left(\frac{5}{2}+\frac{5}{3}-\frac{5}{2}-\frac{5}{3}\right)\\
& &\left.-\frac{2}{3}+\frac{2}{3}-\frac{1}{3}+\frac{2}{4}\right)\\
&=& 0\;\;,
\end{eqnarray*}
as expected from above.  The variable $\Delta_{EC}$ is the mean difference of the available (dependent on the number of bins in the histograms) penchants between two times series (or, in general, any two data sets; the temporal aspect of this causal analysis is only captured in the LIR techinques, i.e.\ in the definitions of $\delta x$ and $\delta y$, and not in the definitions of the penchants).  $\Delta_{EC}$ will become the workhorse of our causal inference and thus, will be called the {\em leaning} of the pair of data sets.  Giving $\Delta_{EC}$ a name is simply to help save time typing in the discussions that follow.

\end{document}



