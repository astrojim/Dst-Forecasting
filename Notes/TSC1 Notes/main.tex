\documentclass[a4paper,11pt]{article}

\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[margin=0.5in]{geometry}
\usepackage{caption}
\usepackage{subcaption}


\begin{document}

{\Huge TSC1 Notes}

\hfill\rule{150mm}{.1pt}

\hfill{\small \today}

\section{Simple Linear Example}
Consider a simple driver-response system:
\begin{eqnarray*}
x_t &=& \sin(t)\\
y_t &=& x_{t-1} + \eta_t\\
&=& \sin(t-1) + \eta_t\\
&=& \sin(t)\cos(1)-\cos(t)\sin(1)+ \eta_t
\end{eqnarray*}
with $\eta_t\sim \mathcal{N}(0,1)$.  Define
$$
\delta x_t \equiv \frac{dx}{dt} \approx \frac{\Delta x}{\Delta t} = x_t-x_{t-1}
$$
and
$$
\delta y_t \equiv \frac{dy}{dt} \approx \frac{\Delta y}{\Delta t} = y_t-y_{t-1}\;\;.
$$
It follows that
\begin{eqnarray*}
\delta x_t &=& \sin(t)-\sin(t-1)\\
&=& \sin(t)-\left(\sin(t)\cos(1)-\cos(t)\sin(1)\right)\\
&=& \sin(t)\left(1-\cos(1)\right)+\sin(1)\cos(t)\\
&\equiv& \kappa_1\sin(t)+\kappa_2\cos(t)
\end{eqnarray*}
with $\kappa_1 = 1-\cos(1)$ and $\kappa_2 = \sin(1)$, and
\begin{eqnarray*}
\delta y_t &=& x_{t-1} + \eta_t - x_{t-2} - \eta_{t-1}\\
&=& \sin(t-1) - \sin(t-2) + \eta_t - \eta_{t-1}\\
&=& \left(\sin(t)\cos(1)-\cos(t)\sin(1)\right) - \left(\sin(t)\cos(2)-\cos(t)\sin(2)\right)+\eta^\prime_t \\
&=& \sin(t)\left(\cos(1)-\cos(2)\right)+\cos(t)\left(\sin(2)-\sin(1)\right)+\eta^\prime_t\\
&\equiv& k_1\sin(t)+k_2\cos(t)+\eta^\prime_t
\end{eqnarray*}
with $\eta^\prime_t = \eta_t - \eta_{t-1} \sim \mathcal{N}(0,2)$\footnote{The difference of two normal distributions with means $\mu_1$ and $\mu_2$ and variances $\sigma^2_1$ and $\sigma^2_2$ is another normal distribution with mean $\mu_1-\mu_2$ and variance $\sigma^2_1+\sigma^2_2$.}, $k_1 = \left(\cos(1)-\cos(2)\right)$ and $k_2 = \left(\sin(2)-\sin(1)\right)$.

The main idea of Local Impulse Response (LIR) causality inference is to use a subsetting procedure on some (or all) of the four time series $x_t$, $y_t$, $\delta x_t$, and $\delta y_t$ to determine the causality of the system.

Consider a few extreme points in the driver cycle, e.g.\ $t=n\pi$ with $n=0,1,2,3,4,\ldots$.  The driver values are
$$
x_{n\pi} = \sin(n\pi) = 0\;\;,
$$
the response values are
$$
y_{n\pi} = \sin(n\pi)\cos(1)-\cos(n\pi)\sin(1)+ \eta_{n\pi} = \eta_{n\pi} + \left(-1\right)^n\sin(1)\;\;,
$$
the local change in the driver values are
$$
\delta x_{n\pi} = \kappa_1\sin(n\pi)+\kappa_2\cos(n\pi) = \left(-1\right)^n \kappa_2\;\;,
$$
and the local change in the response values are
$$
\delta y_{n\pi} = k_1\sin(n\pi)+k_2\cos(n\pi)+\eta^\prime_{n\pi} = \eta^\prime_{n\pi} + \left(-1\right)^n k_2\;\;.
$$

Consider $t=n\pi/2$ with $n=1,2,3,4,\ldots$.  The driver values are
$$
x_{n\frac{\pi}{2}} = \sin\left(n\frac{\pi}{2}\right) = (-1)^n\;\;,
$$
the response values are
$$
y_{n\frac{\pi}{2}} = \sin\left(n\frac{\pi}{2}\right)\cos(1)-\cos\left(n\frac{\pi}{2}\right)\sin(1)+ \eta_{n\frac{\pi}{2}} = \eta_{n\frac{\pi}{2}} + (-1)^n\cos(1)\;\;,
$$
the local change in thd driver values are
$$
\delta x_{n\frac{\pi}{2}} = \kappa_1\sin\left(n\frac{\pi}{2}\right)+\kappa_2\cos\left(n\frac{\pi}{2}\right) = \left(-1\right)^n \kappa_1\;\;,
$$
and the local change in the response values are
$$
\delta y_{n\frac{\pi}{2}} = k_1\sin\left(n\frac{\pi}{2}\right)+k_2\cos\left(n\frac{\pi}{2}\right)+\eta^\prime_{n\frac{\pi}{2}} = \eta^\prime_{n\frac{\pi}{2}} + \left(-1\right)^n k_1\;\;.
$$

\section{Subsetting for LIR variance}
Consider $\mathbf{X}=\{x_t\}$ and $\mathbf{Y}=\{y_t\}$ given $t\in[0,4\pi]$.  Let $L$ be the library length of $\mathbf{X}$ and $\mathbf{Y}$.  These systems have five points where $t=n\pi$ for $n=0,1,2,3$ and $4$. Thus, an $m$-binned histogram of $\mathbf{X}$, where $m\ge L$, would have a bin, $b_0$, centered at $x_t=0$ that contains the five points $\mathbf{X}_{0} = \{x_{n\pi}\}$.  (If $m<L$, then $\mathbf{b}_0$ would contain at least five points but the total number of points in $\mathbf{b}_0$ would be a function of the total number of bins, assuming bins of equal sizes.)  

Consider the set of time steps $\mathbf{T}=\{t=n\pi\}\;\forall n=0,1,2,3,4$ for which the values in $\mathbf{X_0}$ are acheived.  The local impulses immediately preceeding $\mathbf{b}_0$ are $\mathbf{\delta X}_\mathbf{T}=\{\delta x_{n\pi}\}$, which also contains five points.  However, those five points would not appear in a single bin of an m-binned histogram of $\{\delta x_t\}$ (given $m\ge L$).  The set $\mathbf{\delta X}_\mathbf{T}$ would actually be split into two separate bins in such a histogram, one for the three points equal to $\kappa_2$ and one for the two points equal to $-\kappa_2$.  Thus, the time steps associated all of the points in a given histogram bin of a given time series, e.g.\ $\mathbf{b}_0$, do not nessecarily correspond to points in a single histogram bin of different (though related) times series, e.g.\ $\mathbf{\delta X}_\mathbf{T}$.  This idea is straightforward but it is the basic idea underlying the subsetting method for calculating the LIR variance.

The subsetting is premised on the following:
\begin{itemize}
\item The local temporal response causally depends on the local temporal change in the driver; e.g.\ $y_{t}$ causally depends on $\delta x_{t}$
\item The local temporal response causally depends on the immediately preceeding response; e.g.\ $y_{t}$ causally depends on $y_{t-1}$
\item The local temporal response does not causally depend on the immediately preceeding driver except through the local temporal change in the driver; e.g.\ $y_{t}$ does not causally depend on $x_{t-1}$ except through $\delta x_t$
\end{itemize}
Thus, the subsetting procedure is as follows:
\begin{enumerate}
\item Create an $m$-binned histogram of the response signal $\mathbf{R} = \{r_t\}$.
\item Given $m$ bins $\mathbf{b}_i$ where $i$ denotes the center of the $m$th bin, create an $m^\prime$-binned histogram of the change in the driver signal $\mathbf{\delta D} = =\{\delta d_t\} = \{d_t-d_{t-1}\}$ at the time steps $\mathbf{\tau} = \{t\;|\;r_t\in \mathbf{b}_i\}$.
\item Given $m^\prime$ bins $\mathbf{b^\prime}_j$ where $j$ denotes the center of the $m^\prime$th bin, find the variance of the response at the time steps immediately following (i.e.\ $t+1$) the time steps $\mathbf{\tau^\prime} = \{t\;|\;\delta d_t\in \mathbf{b}_j\}$.
\end{enumerate}
As an example, consider $\mathbf{R} = \mathbf{X}$ and $\mathbf{D} = \mathbf{Y}$.  An $m$-binned histogram of $\mathbf{X}$ would lead to a bin centered at zero, $b_0$, that contains at least five points evaluated at $t=n\pi\;\forall n=0,1,2,3,4$.  Thus, the change in the driver signal, i.e.\ $\{\delta y_t\}$, evaluated at $\mathbf{\tau}$ contains at least the five points $\eta^\prime_{n\pi} + \left(-1\right)^n k_2$.  The $m^\prime$-binned histogram of step 2 would split these five points among different bins (depending on both the sign of $k_2$ and the value of $\eta^\prime_{n\pi}$ for a given $n$).  If each of the points is placed into a bin alone, then the variance calculations of step 3 become $\mathop{var}\left(\sin(n\pi+1)\right)\;\forall n=0,1,2,3,4$, which is five zeros because the variance of a single point is zero.  Suppose all five points are placed into a single bin.  The variance calculation of step 3 then becomes 
\begin{eqnarray}
\mathop{var}\left(\{\sin(n\pi+1)\; | n=0,1,2,3,4\}\right) &=& \mathop{var}\left(\{\sin(n\pi)\cos(1)+\cos(n\pi)\sin(1)\; | n=0,1,2,3,4\}\right)\\
&=& \mathop{var}\left(\{(-1)^n\kappa_2\; | n=0,1,2,3,4\}\right)\\
&=& \mathop{var}\left(\{\kappa_2,-\kappa_2,\kappa_2,-\kappa_2,\kappa_2\}\right)\\
&=& \frac{1}{5}\left((\kappa_2-\mu)^2+(-\kappa_2-\mu)^2+(\kappa_2-\mu)^2+(-\kappa_2-\mu)^2+(\kappa_2-\mu)^2\right)\\
&=& \frac{1}{5}\left(\frac{16}{25}\kappa_2^2+\frac{36}{25}\kappa_2^2+\frac{16}{25}\kappa_2^2+\frac{36}{25}\kappa_2^2+\frac{16}{25}\kappa_2^2\right)\\
&=& \frac{1}{5}\frac{120}{25}\kappa_2^2\\
&=& \frac{24}{25}\kappa_2^2\
\end{eqnarray}
where $\mu=\kappa_2/5$.  Thus, the LIR variance depends strongly on the number of bins used to contruct the histograms in steps 1 and 2.  

In this particular example, $\mathbf{Y}$ is known to be the response and $\mathbf{X}$ is known to be the driver.  It may be assumed that our assignment of $\mathbf{R} = \mathbf{X}$ and $\mathbf{D} = \mathbf{Y}$ may proven ``false'' by comparing the variances given this assignment and it's complement, i.e.\ $\mathop{LIRvar}|\mathbf{R} = \mathbf{X},\mathbf{D} = \mathbf{Y}$ and $\mathop{LIRvar}|\mathbf{R} = \mathbf{Y},\mathbf{D} = \mathbf{X}$.  It may be assumed that the lower LIR variance is indicative of a stronger causal inference, i.e.\ if $\mathop{LIRvar}|\mathbf{R} = \mathbf{X},\mathbf{D} = \mathbf{Y}$ is greater than $\mathop{LIRvar}|\mathbf{R} = \mathbf{Y},\mathbf{D} = \mathbf{X}$, then it may be assumed $\mathbf{R} = \mathbf{Y},\mathbf{D} = \mathbf{X}$ is the more ``correct'' assignment.  Notice, however, that we have already shown that the incorrect assignment of $\mathbf{R} = \mathbf{X}$ and $\mathbf{D} = \mathbf{Y}$ can lead to an LIR variance of variance.  The correct assignment of $\mathbf{R} = \mathbf{Y}$ and $\mathbf{D} = \mathbf{X}$ cannot lead to an LIR variance less than zero (variances are nonnegative).  Thus, it seems that comparing LIR variances is not a robust method for causal inference.  

\section{LIR Approach to Probabilistic Causality}
Probabilistic causality is centered on the definition that a cause $C$ is said to {\em cause} (or {\em drive}) an effect $E$ if
$$
P\left(E|C\right) > P\left(E|\bar{C}\right)\;\;,
$$
i.e.\ $C$ causes $E$ if the probability of $E$ given $C$ is higher than the probability of $E$ given not $C$.  The LIR causal inference method involves using e.g.\ $\{x_t\}$, $\{y_t\}$, $\{\delta x_t\}$, and $\{\delta y_t\}$ to determine the direction of causal influence in a system of two times series $\{x_t\}$ and $\{y_t\}$.  It follows that applying LIR causal inference to probabilistic causality involves evaluating the above inequality given e.g.\ $C = \{x_t\}$, $\{y_t\}$, $\{\delta x_t\}$, or $\{\delta y_t\}$ and $E = \{x_t\}$, $\{y_t\}$, $\{\delta x_t\}$, or $\{\delta y_t\}$ given $E\neq C$ and for different temporal offsets.

The conditional probabilities are estimated using histograms of the time series data as follows:
$$
P\left(E|C\right) \approx \frac{1}{L} H\left(E|C\right) = \frac{H\left(E\cap C\right)}{H(C)}
$$
where $H(A)$ is an $m$-binned histogram of $A$, $L$ is the library length of the $E$ and $C$ time series (which are assumed to be the same length).  Similarly,
$$
P\left(E|\bar{C}\right) \approx \frac{1}{L} H\left(E|\bar{C}\right) = \frac{H\left(E\cap \bar{C}\right)}{H(\bar{C})}\;\;.
$$

Define the {\em causal penchant} 
$$
\rho_{EC} = P\left(E|C\right) - P\left(E|\bar{C}\right) \approx \frac{H\left(E\cap C\right)}{H(C)}-\frac{H\left(E\cap \bar{C}\right)}{H(\bar{C})}\;\;.
$$
If $C$ causes $E$, then $\rho_{EC} > 0$.  Otherwise, i.e.\ $\rho_{EC} \le 0$, the causal influence of the system is {\em undefined}(?).  Causal influence between a pair of time series is accomplished by comparing penchants.

Consider two short time seires $\mathbf{x}=\{x_t\} = \{0,1,2,1,0\}$ and $\mathbf{y}=y_t=x_{t-1}$ (i.e.\ $\{y_t\} = \{0,0,1,2,1\}$.  The time steps are indexed by $t=0,1,2,3,4$.  The joint histogram $H(\mathbf{x}\cap\mathbf{y})$ is given by the following frequency table:
\begin{center}
\begin{tabular}{l|ccccc}
 & $x_0$ & $x_1$ & $x_2$ & $x_3$ & $x_4$ \\
 \hline \\
 $y_0$ & 1 & 1 & 1 & 1 & 1\\
 $y_1$ & 1 & 1 & 1 & 1 & 1\\
 $y_2$ & 1 & 1 & 1 & 1 & 1\\
 $y_3$ & 1 & 1 & 1 & 1 & 1\\
 $y_4$ & 1 & 1 & 1 & 1 & 1\\
\end{tabular}
\end{center}
The individual joint probabilities can be found using the frequencies of occurence as
\begin{eqnarray}
P(y_t=0 \cap x_t = 0) &=& \frac{1}{5}\\
P(y_t=1 \cap x_t = 0) &=& \frac{1}{5}\\
P(y_t=2 \cap x_t = 0) &=& 0\\
P(y_t=0 \cap x_t = 1) &=& \frac{1}{5}\\
P(y_t=1 \cap x_t = 1) &=& 0\\
P(y_t=2 \cap x_t = 1) &=& \frac{1}{5}\\
P(y_t=0 \cap x_t = 2) &=& 0\\
P(y_t=1 \cap x_t = 2) &=& \frac{1}{5}\\
P(y_t=2 \cap x_t = 2) &=& 0
\end{eqnarray}
The individual probabilities are found similarly:
\begin{eqnarray}
P(x_t = 0) &=& \frac{2}{5}\\
P(x_t = 1) &=& \frac{2}{5}\\
P(x_t = 2) &=& \frac{1}{5}\\
P(y_t = 0) &=& \frac{2}{5}\\
P(y_t = 1) &=& \frac{2}{5}\\
P(y_t = 2) &=& \frac{1}{5}
\end{eqnarray}
The joint probabilities are symmetric (i.e.\ $P(x_t\cap y_t) = P(y_t\cap x_t)$).  Thus, the two above sets of probabilities are sufficient to find all the conditionals.  
\begin{center}
\begin{tabular}{c|c|c}
$P(y_t=0 | x_t = 0) = \frac{P(y_t=0 \cap x_t = 0)}{P(x_t = 0)} = \frac{1}{2}$ &
$P(y_t=0 | x_t = 1) = \frac{P(y_t=0 \cap x_t = 1)}{P(x_t = 1)} = \frac{1}{2}$ &
$P(y_t=0 | x_t = 2) = \frac{P(y_t=0 \cap x_t = 2)}{P(x_t = 2)} = 0$ \\

$P(y_t=1 | x_t = 0) = \frac{P(y_t=1 \cap x_t = 0)}{P(x_t = 0)} = \frac{1}{2}$ &
$P(y_t=1 | x_t = 1) = \frac{P(y_t=1 \cap x_t = 1)}{P(x_t = 1)} = 0$ &
$P(y_t=1 | x_t = 2) = \frac{P(y_t=1 \cap x_t = 2)}{P(x_t = 2)} = 1$ \\

$P(y_t=2 | x_t = 0) = \frac{P(y_t=2 \cap x_t = 0)}{P(x_t = 0)} = 0$ &
$P(y_t=2 | x_t = 1) = \frac{P(y_t=2 \cap x_t = 1)}{P(x_t = 1)} = \frac{1}{2}$ &
$P(y_t=2 | x_t = 2) = \frac{P(y_t=2 \cap x_t = 2)}{P(x_t = 2)} = 0$ \\
\hline \\
$P(x_t=0 | y_t = 0) = \frac{P(y_t=0 \cap x_t = 0)}{P(y_t = 0)} = \frac{1}{2}$ &
$P(x_t=0 | y_t = 1) = \frac{P(y_t=1 \cap x_t = 0)}{P(y_t = 1)} = \frac{1}{2}$ &
$P(x_t=0 | y_t = 2) = \frac{P(y_t=2 \cap x_t = 0)}{P(y_t = 2)} = 0$ \\

$P(x_t=1 | y_t = 0) = \frac{P(y_t=0 \cap x_t = 1)}{P(y_t = 0)} = \frac{1}{2}$ &
$P(x_t=1 | y_t = 1) = \frac{P(y_t=1 \cap x_t = 1)}{P(y_t = 1)} = 0$ &
$P(x_t=1 | y_t = 2) = \frac{P(y_t=2 \cap x_t = 1)}{P(y_t = 2)} = 1$ \\

$P(x_t=2 | y_t = 0) = \frac{P(y_t=0 \cap x_t = 2)}{P(y_t = 0)} = 0$ &
$P(x_t=2 | y_t = 1) = \frac{P(y_t=1 \cap x_t = 2)}{P(y_t = 1)} = \frac{1}{2}$ &
$P(x_t=2 | y_t = 2) = \frac{P(y_t=2 \cap x_t = 2)}{P(y_t = 2)} = 0$ \\
\end{tabular}
\end{center}
Consider the penchant
$$
\rho_{xy} = P(x_t = 0 | y_t = 0) - P(x_t = 0 | y_t \neq 0) = \frac{1}{2} - \frac{1}{2} = 0
$$
and its partner
$$
\rho_{yx} = P(y_t = 0 | x_t = 0) - P(y_t = 0 | x_t \neq 0) = \frac{1}{2} - \frac{1}{2} = 0\;\;.
$$
These two values are equal, but they are also only for a single points.  Consider comparing the two time series using mean penchants,
\begin{eqnarray}
\langle \rho_{xy} \rangle &=& \frac{1}{9}\left[ \sum_{i=0}^2 \sum_{j=0}^2 \left(P(x_t = i | y_t = j) - P(x_t = i | y_t \neq j)\right)\right] \\
&=& \frac{1}{9}\left[ 0 + 0 -1 -\frac{1}{2} -\frac{3}{2} +\frac{1}{2} -\frac{1}{2} +\frac{1}{2} -\frac{1}{2}\right] \\
&=& \frac{1}{3}
\end{eqnarray}
and $\langle \rho_{yx} \rangle = \langle \rho_{xy} \rangle$.  This last statement is seen from the table above.  Thus, even the mean penchants are equal and there seems to be no useful conclusions from the penchants regarding causal influence.

The next step is to investigate the difference series: $\mathbf{\delta x} = \delta x_t = x_t - x_{t-1} = \{0,1,1,-1,-1\}$ and $\mathbf{\delta y} =\delta y_t = y_t - y_{t-1} = \{0,0,1,1,-1\}$.  There are now four time series, $\mathbf{x}$, $\mathbf{y}$, $\mathbf{\delta x}$, and $\mathbf{\delta y}$, with six permutations to investigate, $(\mathbf{x},\mathbf{y})$, $(\mathbf{x},\mathbf{\delta y})$, $(\mathbf{\delta x},\mathbf{y})$, $(\mathbf{\delta x},\mathbf{\delta y})$, $(\mathbf{x},\mathbf{\delta x})$, and $(\mathbf{y},\mathbf{\delta y})$.  It has already been determined that no conclusions can be drawn about $(\mathbf{x},\mathbf{y})$.  The following table is for reference in the counting exercises below:
\begin{center}
\begin{tabular}{c|c|c|c|c}
t & $\mathbf{x}$ & $\mathbf{y}$ & $\mathbf{\delta x}$ & $\mathbf{\delta y}$ \\
\hline 
0 & 0 & 0 & 0 & 0\\
1 & 1 & 0 & 1 & 0\\
2 & 2 & 1 & 1 & 1\\
3 & 1 & 2 & -1 & 1\\
4 & 0 & 1 & -1 & -1
\end{tabular}
\end{center}

Consider the three permutations: $(\mathbf{x},\mathbf{\delta x})$, $(\mathbf{\delta x},\mathbf{y})$, and $(\mathbf{\delta x},\mathbf{\delta y})$.  The joint probabilities are as follows:
\begin{center}
\begin{tabular}{c|c|c}
$P(x_t=0 \cap \delta x_t = 0) = \frac{1}{5}$&$P(y_t=0 \cap \delta x_t = 0) = \frac{1}{5}$&$P(\delta y_t=0 \cap \delta x_t = 0) = \frac{1}{5}$\\
$P(x_t=1 \cap \delta x_t = 0) = 0$&$P(y_t=1 \cap \delta x_t = 0) = 0$&$P(\delta y_t=1 \cap \delta x_t = 0) = 0$\\
$P(x_t=2 \cap \delta x_t = 0) = 0$&$P(y_t=2 \cap \delta x_t = 0) = 0$&$P(\delta y_t=-1 \cap \delta x_t = 0) = 0$\\
$P(x_t=0 \cap \delta x_t = 1) = 0$&$P(y_t=0 \cap \delta x_t = 1) = \frac{1}{5}$&$P(\delta y_t=0 \cap \delta x_t = 1) = \frac{1}{5}$\\
$P(x_t=1 \cap \delta x_t = 1) = \frac{1}{5}$&$P(y_t=1 \cap \delta x_t = 1) = \frac{1}{5}$&$P(\delta y_t=1 \cap \delta x_t = 1) = \frac{1}{5}$\\
$P(x_t=2 \cap \delta x_t = 1) = \frac{1}{5}$&$P(y_t=2 \cap \delta x_t = 1) = 0$&$P(\delta y_t=-1 \cap \delta x_t = 1) =0 $\\
$P(x_t=0 \cap \delta x_t = -1) = \frac{1}{5}$&$P(y_t=0 \cap \delta x_t = -1) = 0$&$P(\delta y_t=0 \cap \delta x_t = -1) = 0$\\
$P(x_t=1 \cap \delta x_t = -1) = \frac{1}{5}$&$P(y_t=1 \cap \delta x_t = -1) = \frac{1}{5}$&$P(\delta y_t=1 \cap \delta x_t = -1) = \frac{1}{5}$\\
$P(x_t=2 \cap \delta x_t = -1) = 0$&$P(y_t=2 \cap \delta x_t = -1) = \frac{1}{5}$&$P(\delta y_t=-1 \cap \delta x_t = -1) = \frac{1}{5}$\\
\end{tabular}
\end{center}
The last two permutations, $(\mathbf{x},\mathbf{\delta y})$ and $(\mathbf{y},\mathbf{\delta y})$, lead to the following table:
\begin{center}
\begin{tabular}{c|c}
$P(x_t=0 \cap \delta y_t = 0) = \frac{1}{5}$&$P(y_t=0 \cap \delta y_t = 0) = \frac{2}{5}$\\
$P(x_t=1 \cap \delta y_t = 0) = \frac{1}{5}$&$P(y_t=1 \cap \delta y_t = 0) = 0$\\
$P(x_t=2 \cap \delta y_t = 0) = 0$&$P(y_t=2 \cap \delta y_t = 0) = 0$\\
$P(x_t=0 \cap \delta y_t = 1) = 0$&$P(y_t=0 \cap \delta y_t = 1) = 0$\\
$P(x_t=1 \cap \delta y_t = 1) = \frac{1}{5}$&$P(y_t=1 \cap \delta y_t = 1) = \frac{1}{5}$\\
$P(x_t=2 \cap \delta y_t = 1) = \frac{1}{5}$&$P(y_t=2 \cap \delta y_t = 1) = \frac{1}{5}$\\
$P(x_t=0 \cap \delta y_t = -1) = \frac{1}{5}$&$P(y_t=0 \cap \delta y_t = -1) = 0$\\
$P(x_t=1 \cap \delta y_t = -1) = 0$&$P(y_t=1 \cap \delta y_t = -1) = \frac{1}{5}$\\
$P(x_t=2 \cap \delta y_t = -1) = 0$&$P(y_t=2 \cap \delta y_t = -1) =0$\\
\end{tabular}
\end{center}
The individual probabilities are (the ones printed above have been reprinted for convenience)
\begin{center}
\begin{tabular}{c|c}
$P(x_t = 0) = \frac{2}{5}$&$P(\delta x_t = 0) = \frac{1}{5}$\\
$P(x_t = 1) = \frac{2}{5}$&$P(\delta x_t = 1) = \frac{2}{5}$\\
$P(x_t = 2) = \frac{1}{5}$&$P(\delta x_t = -1) = \frac{2}{5}$\\
\hline
$P(y_t = 0) = \frac{2}{5}$&$P(\delta y_t = 0) = \frac{2}{5}$\\
$P(y_t = 1) = \frac{2}{5}$&$P(\delta y_t = 1) = \frac{2}{5}$\\
$P(y_t = 2) = \frac{1}{5}$&$P(\delta y_t = -1) = \frac{1}{5}$
\end{tabular}
\end{center}

Consider the penchants for $(\mathbf{x},\mathbf{\delta x})$.  The conditional probability table is
\begin{center}
\begin{tabular}{c|c|c}
$P(x_t=0 | \delta x_t = 0) = 1$ &
$P(x_t=0 | \delta x_t = 1) = 0$ &
$P(x_t=0 | \delta x_t = -1) = \frac{1}{2}$ \\

$P(x_t=1 | \delta x_t = 0) = 0$ &
$P(x_t=1 | \delta x_t = 1) = \frac{1}{2}$ &
$P(x_t=1 | \delta x_t = -1) = \frac{1}{2}$ \\

$P(x_t=2 | \delta x_t = 0) = 0$ &
$P(x_t=2 | \delta x_t = 1) = \frac{1}{2}$ &
$P(x_t=2 | \delta x_t = -1) = 0$ \\
\hline \\
$P(\delta x_t=0 | x_t = 0) = \frac{1}{2}$ &
$P(\delta x_t=0 | x_t = 1) = 0$ &
$P(\delta x_t=0 | x_t = 2) = 0$ \\

$P(\delta x_t=1 | x_t = 0) = 0$ &
$P(\delta x_t=1 | x_t = 1) = \frac{1}{2}$ &
$P(\delta x_t=1 | x_t = 2) = 1$ \\

$P(\delta x_t=-1 | x_t = 0) = \frac{1}{2}$ &
$P(\delta x_t=-1 | x_t = 1) = \frac{1}{2}$ &
$P(\delta x_t=-1 | x_t = 2) = 0$ \\
\end{tabular}
\end{center}
Thus,
\begin{eqnarray}
\langle \rho_{x\delta x} \rangle &=& \frac{1}{9}\left[\left(1-\frac{1}{2}\right)+\left(0-1\right)+\left(0-\frac{1}{2}\right)\right.\\
& &+\left(0-\frac{3}{2}\right)+\left(\frac{1}{2}-\frac{1}{2}\right)+\left(\frac{1}{2}-0\right)\\
& &\left.+\left(\frac{1}{2}-1\right)+\left(\frac{1}{2}-\frac{1}{2}\right)+\left(0-\frac{1}{2}\right)\right] \\
&=& \frac{1}{9}\left[\frac{1}{2}-1-\frac{1}{2}-\frac{3}{2}+\frac{1}{2}-\frac{1}{2}-\frac{1}{2}\right] \\
&=& -\frac{1}{3}
\end{eqnarray}
and
\begin{eqnarray}
\langle \rho_{\delta x x} \rangle &=& \frac{1}{9}\left[\left(\frac{1}{2}-0\right)+\left(0-\frac{3}{2}\right)+\left(\frac{1}{2}-\frac{1}{2}\right)\right.\\
& &+\left(0-\frac{1}{2}\right)+\left(\frac{1}{2}-1\right)+\left(\frac{1}{2}-\frac{1}{2}\right)\\
& &\left.+\left(0-\frac{1}{2}\right)+\left(1-\frac{1}{2}\right)+\left(0-1\right)\right] \\
&=& \frac{1}{9}\left[\frac{1}{2}-\frac{3}{2}-\frac{1}{2}-\frac{1}{2}-\frac{1}{2}+\frac{1}{2}-1\right] \\
&=& -\frac{1}{3}\;\; .
\end{eqnarray}

Consider the penchants for $(\mathbf{\delta x},\mathbf{y})$.  The conditional probability table is
\begin{center}
\begin{tabular}{c|c|c}
$P(\delta x_t=0 | y_t = 0) = \frac{1}{2}$ &
$P(\delta x_t=0 | y_t = 1) = 0$ &
$P(\delta x_t=0 | y_t = 2) = 0$ \\

$P(\delta x_t=1 | y_t = 0) = \frac{1}{2}$ &
$P(\delta x_t=1 | y_t = 1) = \frac{1}{2}$ &
$P(\delta x_t=1 | y_t = 2) = 0$ \\

$P(\delta x_t=-1 | y_t = 0) = 0$ &
$P(\delta x_t=-1 | y_t = 1) = \frac{1}{2}$ &
$P(\delta x_t=-1 | y_t = 2) = 1$ \\
\hline \\
$P(y_t=0 | \delta x_t = 0) = 1$ &
$P(y_t=0 | \delta x_t = 1) = \frac{1}{2}$ &
$P(y_t=0 | \delta x_t = -1) = 0$ \\

$P(y_t=1 | \delta x_t = 0) = 0$ &
$P(y_t=1 | \delta x_t = 1) = \frac{1}{2}$ &
$P(y_t=1 | \delta x_t = -1) = \frac{1}{2}$ \\

$P(y_t=2 | \delta x_t = 0) = 0$ &
$P(y_t=2 | \delta x_t = 1) = 0$ &
$P(y_t=2 | \delta x_t = -1) = \frac{1}{2}$ \\
\end{tabular}
\end{center}
Thus,
\begin{eqnarray}
\langle \rho_{\delta x y} \rangle &=& \frac{1}{9}\left[\left(\frac{1}{2}-0\right)+\left(\frac{1}{2}-\frac{1}{2}\right)+\left(0-\frac{3}{2}\right)\right.\\
& &+\left(0-\frac{1}{2}\right)+\left(\frac{1}{2}-\frac{1}{2}\right)+\left(\frac{1}{2}-1\right)\\
& &\left.+\left(0-\frac{1}{2}\right)+\left(0-1\right)+\left(1-\frac{1}{2}\right)\right] \\
&=& \frac{1}{9}\left[\frac{1}{2}-\frac{3}{2}-\frac{1}{2}-\frac{1}{2}-\frac{1}{2}-1+\frac{1}{2}\right] \\
&=& -\frac{2}{9}
\end{eqnarray}
and
\begin{eqnarray}
\langle \rho_{y \delta x} \rangle &=& \frac{1}{9}\left[\left(1-\frac{1}{2}\right)+\left(0-1\right)+\left(0-\frac{1}{2}\right)\right.\\
& &+\left(\frac{1}{2}-1\right)+\left(\frac{1}{2}-\frac{1}{2}\right)+\left(0-\frac{1}{2}\right)\\
& &\left.+\left(0-\frac{3}{2}\right)+\left(\frac{1}{2}-\frac{1}{2}\right)+\left(\frac{1}{2}-0\right)\right] \\
&=& \frac{1}{9}\left[-\frac{1}{2}-1-\frac{1}{2}-\frac{1}{2}-\frac{1}{2}-\frac{3}{2}+\frac{1}{2}\right] \\
&=& -\frac{4}{9} \;\;.
\end{eqnarray}

Consider the penchants for $(\mathbf{\delta x},\mathbf{\delta y})$.  The conditional probability table is
\begin{center}
\begin{tabular}{c|c|c}
$P(\delta x_t=0 | \delta y_t = 0) = \frac{1}{2}$ &
$P(\delta x_t=0 | \delta y_t = 1) = 0$ &
$P(\delta x_t=0 | \delta y_t = -1) = 0$ \\

$P(\delta x_t=1 | \delta y_t = 0) = \frac{1}{2}$ &
$P(\delta x_t=1 | \delta y_t = 1) = \frac{1}{2}$ &
$P(\delta x_t=1 | \delta y_t = -1) = 0$ \\

$P(\delta x_t=-1 | \delta y_t = 0) = 0$ &
$P(\delta x_t=-1 | \delta y_t = 1) = \frac{1}{2}$ &
$P(\delta x_t=-1 | \delta y_t = -1) = 1$ \\
\hline \\
$P(\delta y_t=0 | \delta x_t = 0) = 1$ &
$P(\delta y_t=0 | \delta x_t = 1) = \frac{1}{2}$ &
$P(\delta y_t=0 | \delta x_t = -1) = 0$ \\

$P(\delta y_t=1 | \delta x_t = 0) = 0$ &
$P(\delta y_t=1 | \delta x_t = 1) = \frac{1}{2}$ &
$P(\delta y_t=1 | \delta x_t = -1) = \frac{1}{2}$ \\

$P(\delta y_t=-1 | \delta x_t = 0) = 0$ &
$P(\delta y_t=-1 | \delta x_t = 1) = 0$ &
$P(\delta y_t=-1 | \delta x_t = -1) = \frac{1}{2}$ \\
\end{tabular}
\end{center}
Thus,
\begin{eqnarray}
\langle \rho_{\delta x \delta y} \rangle &=& \frac{1}{9}\left[\left(\frac{1}{2}-0\right)+\left(\frac{1}{2}-\frac{1}{2}\right)+\left(0-\frac{3}{2}\right)\right.\\
& &+\left(0-\frac{1}{2}\right)+\left(\frac{1}{2}-\frac{1}{2}\right)+\left(\frac{1}{2}-1\right)\\
& &\left.+\left(0-\frac{1}{2}\right)+\left(0-1\right)+\left(1-\frac{1}{2}\right)\right] \\
&=& \frac{1}{9}\left[\frac{1}{2}-\frac{3}{2}-\frac{1}{2}-\frac{1}{2}-\frac{1}{2}-1+\frac{1}{2}\right] \\
&=& -\frac{1}{3}
\end{eqnarray}
and
\begin{eqnarray}
\langle \rho_{\delta y \delta x} \rangle &=& \frac{1}{9}\left[\left(1-\frac{1}{2}\right)+\left(0-1\right)+\left(0-\frac{1}{2}\right)\right.\\
& &+\left(\frac{1}{2}-1\right)+\left(\frac{1}{2}-\frac{1}{2}\right)+\left(0-\frac{1}{2}\right)\\
& &\left.+\left(0-\frac{3}{2}\right)+\left(\frac{1}{2}-\frac{1}{2}\right)+\left(\frac{1}{2}-0\right)\right] \\
&=& \frac{1}{9}\left[\frac{1}{2}-1-\frac{1}{2}-\frac{1}{2}-\frac{1}{2}-\frac{3}{2}+\frac{1}{2}\right] \\
&=& -\frac{1}{3} \;\;.
\end{eqnarray}

Consider the penchants for $(\mathbf{ x},\mathbf{\delta y})$.  The conditional probability table is
\begin{center}
\begin{tabular}{c|c|c}
$P(x_t=0 | \delta y_t = 0) = \frac{1}{2}$ &
$P(x_t=0 | \delta y_t = 1) = 0$ &
$P(x_t=0 | \delta y_t = -1) = 1$ \\

$P(x_t=1 | \delta y_t = 0) = \frac{1}{2}$ &
$P(x_t=1 | \delta y_t = 1) = \frac{1}{2}$ &
$P(x_t=1 | \delta y_t = -1) = 0$ \\

$P(x_t=2 | \delta y_t = 0) = 0$ &
$P(x_t=2 | \delta y_t = 1) = \frac{1}{2}$ &
$P(x_t=2 | \delta y_t = -1) = 0$ \\
\hline \\
$P(\delta y_t=0 | x_t = 0) = \frac{1}{2}$ &
$P(\delta y_t=0 | x_t = 1) = \frac{1}{2}$ &
$P(\delta y_t=0 | x_t = 2) = 0$ \\

$P(\delta y_t=1 | x_t = 0) = 0$ &
$P(\delta y_t=1 | x_t = 1) = \frac{1}{2}$ &
$P(\delta y_t=1 | x_t = 2) = 1$ \\

$P(\delta y_t=-1 | x_t = 0) = \frac{1}{2}$ &
$P(\delta y_t=-1 | x_t = 1) = 0$ &
$P(\delta y_t=-1 | x_t = 2) = 0$ \\
\end{tabular}
\end{center}
Thus,
\begin{eqnarray}
\langle \rho_{x \delta y} \rangle &=& \frac{1}{9}\left[\left(\frac{1}{2}-1\right)+\left(\frac{1}{2}-\frac{1}{2}\right)+\left(0-\frac{1}{2}\right)\right.\\
& &+\left(0-\frac{3}{2}\right)+\left(\frac{1}{2}-\frac{1}{2}\right)+\left(\frac{1}{2}-0\right)\\
& &\left.+\left(1-\frac{1}{2}\right)+\left(0-1\right)+\left(0-\frac{1}{2}\right)\right] \\
&=& \frac{1}{9}\left[-\frac{1}{2}-\frac{1}{2}-\frac{3}{2}+\frac{1}{2}+\frac{1}{2}-1-\frac{1}{2}\right] \\
&=& -\frac{1}{3}
\end{eqnarray}
and
\begin{eqnarray}
\langle \rho_{\delta y x} \rangle &=& \frac{1}{9}\left[\left(\frac{1}{2}-\frac{1}{2}\right)+\left(0-\frac{3}{2}\right)+\left(\frac{1}{2}-0\right)\right.\\
& &+\left(\frac{1}{2}-\frac{1}{2}\right)+\left(\frac{1}{2}-1\right)+\left(0-\frac{1}{2}\right)\\
& &\left.+\left(0-1\right)+\left(1-\frac{1}{2}\right)+\left(0-\frac{1}{2}\right)\right] \\
&=& \frac{1}{9}\left[-\frac{3}{2}+\frac{1}{2}-\frac{1}{2}-\frac{1}{2}-1+\frac{1}{2}-\frac{1}{2}\right] \\
&=& -\frac{1}{3} \;\;.
\end{eqnarray}


\end{document}
